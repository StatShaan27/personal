<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title> Measure-Theoretic Probability | Shaan</title>
  <link rel="stylesheet" href="../style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display&family=Inter:wght@300;500&display=swap" rel="stylesheet">
  <style>
    body {
      background-color: #121212;
      color: #e0e0e0;
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
    }

    h1, h2, h3, h4 {
      font-family: 'Playfair Display', serif;
      color: #6effe0;
    }

    header {
      text-align: center;
      padding: 2rem 1rem 1rem;
      background: #1e1e1e;
      border-bottom: 2px solid #333;
    }

    header nav ul {
      list-style: none;
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 1rem;
      padding: 0;
    }

    header nav ul li {
      display: inline;
    }

    header nav ul li a {
      text-decoration: none;
      color: #6effe0;
      font-weight: 500;
      border: 1px solid transparent;
      padding: 0.4rem 0.8rem;
      border-radius: 6px;
      transition: all 0.3s ease;
    }

    header nav ul li a:hover {
      border-color: #6effe0;
      background-color: #6effe0;
      color: #000;
    }

    .notes-wrapper {
      max-width: 900px;
      margin: 3rem auto;
      padding: 2rem;
      animation: fadeIn 1.2s ease;
    }

    .info-block {
      margin-bottom: 2rem;
      text-align: left;
      line-height: 1.6;
    }

    .info-block strong {
      color: #6effe0;
    }

    .toc {
      background-color: #1e1e1e;
      border-left: 4px solid #6effe0;
      padding: 1rem 1.5rem;
      margin-bottom: 2rem;
      border-radius: 8px;
    }

    .toc ul {
      padding-left: 1rem;
    }

    .toc ul li {
      padding: 0.3rem 0;
    }

    .toc a {
      color: #c9d1d9;
      text-decoration: none;
      border-bottom: 1px dashed #6effe0;
    }

    .toc a:hover {
      color: #6effe0;
    }

    .glow-box {
      border: 2px solid #6effe0;
      box-shadow: 0 0 20px rgba(110, 255, 224, 0.2);
      border-radius: 16px;
      padding: 2rem;
      margin-bottom: 2rem;
      background-color: #1e1e1e;
      animation: pulse 3s infinite ease-in-out;
    }

    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 20px rgba(110, 255, 224, 0.2); }
      50% { box-shadow: 0 0 35px rgba(110, 255, 224, 0.4); }
    }

    ul {
      line-height: 1.6;
      padding-left: 1.2rem;
    }

    code, pre {
      background-color: #222;
      padding: 0.5rem 1rem;
      border-radius: 8px;
      font-family: monospace;
      color: #6effe0;
      display: block;
      overflow-x: auto;
    }

    .next-btn {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background-color: #6effe0;
      color: #000;
      padding: 0.6rem 1.2rem;
      border: none;
      border-radius: 8px;
      font-weight: bold;
      cursor: pointer;
      box-shadow: 0 0 10px #6effe0;
      transition: background-color 0.3s ease;
      z-index: 100;
    }

    .next-btn:hover {
      background-color: #00ffcc;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 2rem 1rem;
    }
  </style>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</head>
<body>

<header>
  <h1>Measure-Theoretic Probability</h1>
  <p>AY 2025‚Äì26</p>
  <nav>
    <ul>
      <li><a href="../../index.html">Home</a></li>
      <li><a href="../../assets/Shaan_CV.pdf">CV</a></li>
      <li><a href="../../sections/projects.html">Projects</a></li>
      <li><a href="../../sections/education.html">Education</a></li>
      <li><a href="../../sections/misc.html">Misc</a></li>
    </ul>
  </nav>
</header>

<section class="notes-wrapper">

 <div class="info-block">
    <p><strong>Instructor:</strong> Independent Study </p>
    <p><strong>Office / Department:</strong> - </p>
    <p><strong>Email:</strong> mdsworld2006@gmail.com</p>
    
    </p>
  </div>

  <div class="toc">
    <h3>Contents</h3>
    <ul>
      <li><a href="#1"> Overview: Why Measure Theory? </a></li>
      <li><a href="#2"> The Cost and Benefit of Rigor </a></li>
      <li><a href="#3"> </a></li>
      <li><a href="#4"> </a></li>
      <li><a href="#0"></a></li>
      <!-- Add more as you progress -->
    </ul>
  </div>

  <!-- Notes section boxes -->
  <div class="glow-box" id="1">
   <section>

  <h2>üî∑ Overview: Why Measure Theory?</h2>

  <blockquote>
    <p><em>"Following Kolmogorov (1933)... it has been widely accepted that probabilities should be studied as special sorts of measures."</em></p>
  </blockquote>

  <h3>üîë Key Idea:</h3>
  <p>Measure theory gives probability a <strong>rigorous mathematical foundation</strong>.</p>

  <h3>‚öñÔ∏è Visual Analogy:</h3>
  <p>Think of <strong>probability</strong> as measuring <strong>areas</strong> under curves or <strong>sizes</strong> of sets‚Äîjust like how length, area, and volume are measured. Kolmogorov formalized this by saying:</p>

  <blockquote>
    <p>Probabilities <strong>are</strong> measures, just defined over a special space called a <strong>probability space</strong>.</p>
  </blockquote>

  <h3>üìå Minimal Setup Needed:</h3>
  <ul>
    <li>A <strong>\(\sigma\)</strong>-field (collection of measurable events)</li>
    <li>A <strong>measure</strong> (assigns values to events)</li>
    <li>An <strong>integral</strong> (generalized sum)</li>
  </ul>

  <h3>‚úÖ Why This Matters:</h3>
  <p>Most undergraduate texts don‚Äôt teach this and rely on <strong>hand-wavy, case-specific arguments</strong>. That becomes limiting very fast.</p>

  <hr>

  <h2>üî∑ Section 1: Independence</h2>

  <blockquote>
    <p><em>"There are various elementary definitions of independence for random variables..."</em></p>
  </blockquote>

  <h3>üîë Classical Definition:</h3>
  <p>Independence via <strong>distribution function factorization</strong>:</p>

  \[
  P(X \le x, Y \le y) = P(X \le x) \cdot P(Y \le y)
  \]

  <p>‚úÖ Works for simple cases.</p>

  <h3>‚ùå Problem:</h3>
  <p>Suppose you define:</p>

  \[
  Y = X_1 X_2 \left[ \log \left( \frac{X_1^2 + X_2^2}{|X_1| + |X_2|} \right) + \frac{|X_1|^3 + X_2^3}{X_1^4 + X_2^4} \right]
  \]
  \[
  Z = \sin \left[ X_3 + X_3^2 + X_3 X_4 + X_4^2 + \sqrt{X_3^4 + X_4^4} \right]
  \]

  <p>Even if \(X_1, X_2, X_3, X_4\) are independent, showing \(Y \perp Z\) by checking joint CDFs or densities is nearly impossible.</p>

  <ul>
    <li>The distributions of \(Y\) and \(Z\) are complex and likely <strong>don‚Äôt have nice density forms</strong>.</li>
    <li>Calculating \(P(Y \le y, Z \le z)\) is infeasible.</li>
    <li><strong>Jacobians?</strong> Ugh. Very messy.</li>
  </ul>

  <h3>‚úÖ Measure Theoretic View:</h3>
  <p>You define <strong>independence via product measures</strong>:</p>

  <blockquote>
    <p>Two random variables \(X\) and \(Y\) are independent if their joint distribution measure is the <strong>product</strong> of their individual distribution measures.</p>
  </blockquote>

  <h4>üìå This avoids:</h4>
  <ul>
    <li>Calculating densities</li>
    <li>Checking joint CDFs</li>
    <li>Tedious transformations</li>
  </ul>

  <hr>

  <h2>üî∑ Section 2: Discrete vs Continuous</h2>

  <blockquote>
    <p><em>"Proofs of Tchebychev‚Äôs inequality... split into discrete and continuous cases."</em></p>
  </blockquote>

  <h3>üîë Observation:</h3>
  <ul>
    <li>One proof for <strong>discrete</strong> \(X\): use finite sums.</li>
    <li>One for <strong>continuous</strong> \(X\): use integrals.</li>
  </ul>

  <p>Example:</p>

  \[
  P(|X - \mu| \ge \epsilon) \le \frac{\text{Var}(X)}{\epsilon^2}
  \]

  <h3>‚ùå Problem:</h3>
  <p>Why duplicate effort for each case?</p>

  <h3>‚úÖ Measure Theory Fix:</h3>
  <p>Just define <strong>expected value</strong> as:</p>

  \[
  E[X] = \int_\Omega X \, dP
  \]

  <p>No need to distinguish between:</p>
  <ul>
    <li>Discrete (integral reduces to sum)</li>
    <li>Continuous (integral over density)</li>
  </ul>

  <h4>üìå Key Insight:</h4>
  <p>Measure theory <strong>unifies</strong> the two worlds: discrete and continuous are just <strong>special cases</strong> of measurable functions.</p>

  <hr>

  <h2>üî∑ Section 3: Univariate vs Multivariate</h2>

  <blockquote>
    <p><em>"Once you learn single-variable formulae, you're told to relearn everything for multiple variables."</em></p>
  </blockquote>

  <h3>üîë Typical Scenario:</h3>
  <ul>
    <li>First: Study \(X\), a single random variable.</li>
    <li>Then: Study \((X, Y)\): joint density, CDFs, Jacobians, etc.</li>
  </ul>

  <h3>‚ùå Problem:</h3>
  <p>Redundant work and repetition. Jacobians in transformations are often complex and unnecessary.</p>

  <h3>‚úÖ Measure Theory Fix:</h3>
  <ul>
    <li>A <strong>distribution</strong> is an <strong>image measure</strong> of the probability measure under a function \(X : \Omega \to \mathbb{R}\).</li>
    <li>A <strong>joint distribution</strong> is the image measure under a vector-valued function \((X_1, X_2, \dots) : \Omega \to \mathbb{R}^d\).</li>
  </ul>

  <h4>üìå Key Insight:</h4>
  <p>No need for two separate theories.</p>

  <blockquote>
    <p>‚ÄúImage measure‚Äù = the push-forward of measure via random variable mapping.</p>
  </blockquote>

  <hr>

  <h2>üî∑ Section 4: Approximation of Distributions</h2>

  <blockquote>
    <p><em>"How can a discrete distribution be approximated by a continuous one?"</em></p>
  </blockquote>

  <h3>üîë Central Limit Theorem (CLT):</h3>
  <p>If \(\xi_1, \dots, \xi_n\) are i.i.d. random variables with mean 0, variance summing to 1:</p>

  \[
  S_n = \xi_1 + \dots + \xi_n \xrightarrow{d} N(0, 1)
  \]

  <h3>‚ùå Problem with Classical Approach:</h3>
  <ul>
    <li>You‚Äôre told convergence in distribution means \( F_n(x) \to F(x) \) pointwise.</li>
    <li>But real CLT proofs use moment generating functions, characteristic functions, or Laplace transforms.</li>
  </ul>

  <p>There‚Äôs a <strong>gap in rigor</strong>‚Äîequivalence of convergence types is often <strong>assumed</strong>, not proved.</p>

  <h3>‚úÖ Measure Theory Fix:</h3>

  <blockquote>
    <p>Define convergence in distribution as:</p>
  </blockquote>

  \[
  X_n \xrightarrow{d} X \iff E[f(X_n)] \to E[f(X)] \quad \text{for all bounded, continuous } f
  \]

  <p>This covers:</p>
  <ul>
    <li>Real-valued random variables</li>
    <li>Random vectors</li>
    <li>Random elements in metric spaces like function spaces</li>
  </ul>

  <h4>üìå Visual:</h4>
  <p><em>A discrete sum (e.g., binomial) getting closer to the bell curve (normal). Measure theory allows us to rigorously define this convergence.</em></p>

  
  <hr>

  <h2>üî∑ Final Takeaway</h2>

  <blockquote>
    <p><em>"In the long run, the measure theoretic approach will save you much work..."</em></p>
  </blockquote>

  <h3>‚úÖ Summary of Benefits:</h3>

  <table>
    <thead>
      <tr>
        <th>Classical Approach</th>
        <th>Measure-Theoretic Approach</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Separate cases: discrete vs continuous</td>
        <td>Unified via integration w.r.t. measures</td>
      </tr>
      <tr>
        <td>Messy independence proofs</td>
        <td>Simple via product measures</td>
      </tr>
      <tr>
        <td>Repetition in univariate/multivariate</td>
        <td>Unified via image measures and product spaces</td>
      </tr>
      <tr>
        <td>Vague convergence arguments</td>
        <td>Rigorous convergence via expectations</td>
      </tr>
      <tr>
        <td>Case-by-case formulas</td>
        <td>General theorems that cover all cases</td>
      </tr>
    </tbody>
  </table>

  <hr>

  <h2>üîö Conclusion</h2>

  <p>Measure theory isn‚Äôt just abstract formality. It <strong>streamlines</strong>, <strong>generalizes</strong>, and <strong>deepens</strong> your understanding of probability:</p>

  <ul>
    <li>Avoids tedious calculations</li>
    <li>Handles complex problems cleanly</li>
    <li>Prepares you for modern probability, statistics, and machine learning</li>
  </ul>

  <p>And as the author hints ‚Äî once you learn this framework, you won‚Äôt go back.</p>

</section>

  </div>





























  <div class="glow-box" id="2">
    <h2>üîπ 1. The Cost and Benefit of Rigor</h2>
<blockquote><p>"Probabilities are numbers in [0,1] attached to subsets of a sample space Œ©..."</p></blockquote>
<h3>‚úÖ Classical Rules:</h3>
<ul>
  <li>P(‚àÖ) = 0, P(Œ©) = 1 ‚Äî impossible vs. certain events</li>
  <li>For disjoint sets A<sub>i</sub>:<br>
    <p>P(‚à™ A<sub>i</sub>) = ‚àë P(A<sub>i</sub>)</p>
    <ul>
      <li>This is countable additivity.</li>
    </ul>
  </li>
</ul>
<h4>üîç Visual Analogy:</h4>
<p>Think of sets A<sub>i</sub> like non-overlapping tiles. Total area = sum of individual areas.</p>

<h2>üîπ 2. The Subtlety of Countable Additivity</h2>
<blockquote><p>‚ÄúMathematically, this extension is far from obvious...‚Äù</p></blockquote>
<h3>‚ö†Ô∏è Key Insight:</h3>
<p>Infinite sums of disjoint probabilities aren't trivial. Some pathological subsets can break countable additivity.</p>

<h3>üéì Lebesgue‚Äôs Contribution (1902):</h3>
<ul>
  <li>Defined measure on complicated sets in ‚Ñù.</li>
  <li>Proved: No measure on all subsets of ‚Ñù can satisfy countable additivity, translation invariance, and match interval lengths.</li>
  <li>Leads to paradoxes (e.g., Vitali sets).</li>
</ul>

<h4>üí° Enter Sigma-Fields</h4>
<p>Define probabilities only for sets in a œÉ-field: a structured, manageable collection.</p>

<h2>üîπ 3. Sigma-Fields: A Solution with Benefits</h2>
<h3>‚úÖ Definition:</h3>
<ul>
  <li>Œ© ‚àà ·µä</li>
  <li>If A ‚àà ·µä, then A<sup>c</sup> ‚àà ·µä</li>
  <li>If A<sub>i</sub> ‚àà ·µä, then ‚à™ A<sub>i</sub> ‚àà ·µä</li>
</ul>
<h4>üéØ Benefit:</h4>
<p>Restricts our universe, but enables general, paradox-free reasoning. Many key theorems become easier to prove.</p>

<h2>üîπ 4. Anatomy of a Measure-Theoretic Proof</h2>
<h3>üìò Example Goal:</h3>
<p>Prove a property ‚Ñì(A) holds for all A ‚àà ·µä.</p>
<h3>üìå Steps:</h3>
<ul>
  <li>(a) Prove for basic class ‚ÑÇ (e.g., intervals)</li>
  <li>(b) Show ·µä = œÉ(‚ÑÇ)</li>
  <li>(c) Prove property is preserved under set operations</li>
  <li>(d) Define ·µç := {A ‚àà ·µä : ‚Ñì(A) holds}</li>
  <li>(e) Show ·µç is a œÉ-field containing ‚ÑÇ ‚ûî ·µç = ·µä</li>
</ul>
<h4>üí° Example:</h4>
<p>Prove P(A<sup>c</sup>) = 1 - P(A) for all A:</p>
<ul>
  <li>Start with intervals</li>
  <li>Show preservation under complement</li>
  <li>Define B = {A : P(A<sup>c</sup>) = 1 - P(A)}</li>
  <li>Show B is a œÉ-field containing intervals</li>
</ul>

<h2>üîπ 5. Historical Context: From Lebesgue to Kolmogorov</h2>
<h3>üß† Lebesgue:</h3>
<ul>
  <li>Extended integration theory beyond Riemann.</li>
  <li>Enabled rigorous probability via measure theory.</li>
</ul>
<h3>üß† Kolmogorov (1933):</h3>
<ul>
  <li>Formalized probability as a measure space (Œ©, ·µä, P).</li>
</ul>
<table>
  <thead>
    <tr><th>Probability Concept</th><th>Measure-Theoretic Concept</th></tr>
  </thead>
  <tbody>
    <tr><td>Event</td><td>Measurable set</td></tr>
    <tr><td>Probability</td><td>Measure</td></tr>
    <tr><td>Random variable</td><td>Measurable function</td></tr>
    <tr><td>Expected value</td><td>Integral w.r.t. measure</td></tr>
    <tr><td>Independence</td><td>Product measure</td></tr>
  </tbody>
</table>

<h2>üîπ 6. Redefining Random Variables and Expectation</h2>
<blockquote><p>"Kolmogorov identified random variables with measurable functions..."</p></blockquote>
<h3>‚úÖ Random Variables:</h3>
<p>Defined as measurable functions: X: Œ© ‚Üí ‚Ñù</p>
<p>For each x, the set { œâ : X(œâ) ‚â§ x } is in ·µä.</p>

<h3>‚úÖ Expectation:</h3>
<p>Defined as Lebesgue integral:</p>
<p>E[X] = ‚à´ X dP</p>

<h4>üîπ Familiar Properties:</h4>
<ul>
  <li>Linearity: E(c<sub>1</sub>X<sub>1</sub> + c<sub>2</sub>X<sub>2</sub>) = c<sub>1</sub>E(X<sub>1</sub>) + c<sub>2</sub>E(X<sub>2</sub>)</li>
  <li>Monotonicity: If X ‚â• Y, then E(X) ‚â• E(Y)</li>
</ul>

<h4>üîπ New Benefits:</h4>
<ul>
  <li>Continuity under limits</li>
  <li>Powerful convergence theorems (MCT, DCT, etc.)</li>
</ul>


<h2>üî∫ Final Takeaways</h2>
<h3>üéØ Why Use Measure Theory in Probability?</h3>
<table>
  <thead>
    <tr><th>Classical Probability</th><th>Measure-Theoretic Probability</th></tr>
  </thead>
  <tbody>
    <tr><td>Vague or redundant definitions</td><td>Clean, general definitions</td></tr>
    <tr><td>Struggles with infinite sample spaces</td><td>Handles countable additivity</td></tr>
    <tr><td>Events = subsets</td><td>Events = measurable sets</td></tr>
    <tr><td>Random variables = rules</td><td>Random variables = measurable functions</td></tr>
    <tr><td>Expectation = sum/integral</td><td>Expectation = Lebesgue integral</td></tr>
    <tr><td>Case-by-case reasoning</td><td>Unified framework</td></tr>
  </tbody>
</table>

  </div>




























<div class="glow-box" id="0">
  
</div>




























<div class="glow-box" id="0">
  
</div>





























<div class="glow-box" id="0">
  
</div>





























<div class="glow-box" id="0">
  
</div>





























<div class="glow-box" id="0">
  
</div>





























<div class="glow-box" id="0">
  
</div>





























<div class="glow-box" id="0">
  
</div>





























<div class="glow-box" id="0">
  
</div>





























<div class="glow-box" id="0">
  
</div>


</section>

<button class="next-btn" onclick="location.href='wk2.html'">Next Section ‚Üí</button>

<footer>
  <p>&copy; 2025 Shaan | Built with purpose</p>
</footer>

</body>
</html>


