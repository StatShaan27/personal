<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Topic-1 | Shaan</title>
  <link rel="stylesheet" href="../style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display&family=Inter:wght@300;500&display=swap" rel="stylesheet">
  <style>
    body {
      background-color: #121212;
      color: #e0e0e0;
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
    }

    h1, h2, h3, h4 {
      font-family: 'Playfair Display', serif;
      color: #6effe0;
    }

    header {
      text-align: center;
      padding: 2rem 1rem 1rem;
      background: #1e1e1e;
      border-bottom: 2px solid #333;
    }

    header nav ul {
      list-style: none;
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 1rem;
      padding: 0;
    }

    header nav ul li {
      display: inline;
    }

    header nav ul li a {
      text-decoration: none;
      color: #6effe0;
      font-weight: 500;
      border: 1px solid transparent;
      padding: 0.4rem 0.8rem;
      border-radius: 6px;
      transition: all 0.3s ease;
    }

    header nav ul li a:hover {
      border-color: #6effe0;
      background-color: #6effe0;
      color: #000;
    }

    .notes-wrapper {
      max-width: 900px;
      margin: 3rem auto;
      padding: 2rem;
      animation: fadeIn 1.2s ease;
    }

    .info-block {
      margin-bottom: 2rem;
      text-align: left;
      line-height: 1.6;
    }

    .info-block strong {
      color: #6effe0;
    }

    .toc {
      background-color: #1e1e1e;
      border-left: 4px solid #6effe0;
      padding: 1rem 1.5rem;
      margin-bottom: 2rem;
      border-radius: 8px;
    }

    .toc ul {
      padding-left: 1rem;
    }

    .toc ul li {
      padding: 0.3rem 0;
    }

    .toc a {
      color: #c9d1d9;
      text-decoration: none;
      border-bottom: 1px dashed #6effe0;
    }

    .toc a:hover {
      color: #6effe0;
    }

    .glow-box {
      border: 2px solid #6effe0;
      box-shadow: 0 0 20px rgba(110, 255, 224, 0.2);
      border-radius: 16px;
      padding: 2rem;
      margin-bottom: 2rem;
      background-color: #1e1e1e;
      animation: pulse 3s infinite ease-in-out;
    }

    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 20px rgba(110, 255, 224, 0.2); }
      50% { box-shadow: 0 0 35px rgba(110, 255, 224, 0.4); }
    }

    ul {
      line-height: 1.6;
      padding-left: 1.2rem;
    }

    code, pre {
      background-color: #222;
      padding: 0.5rem 1rem;
      border-radius: 8px;
      font-family: monospace;
      color: #6effe0;
      display: block;
      overflow-x: auto;
    }

    .next-btn {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background-color: #6effe0;
      color: #000;
      padding: 0.6rem 1.2rem;
      border: none;
      border-radius: 8px;
      font-weight: bold;
      cursor: pointer;
      box-shadow: 0 0 10px #6effe0;
      transition: background-color 0.3s ease;
      z-index: 100;
    }

    .next-btn:hover {
      background-color: #00ffcc;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 2rem 1rem;
    }
  </style>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</head>
<body>

<header>
  <h1>Statistical Methods IV: Median</h1>
  <p>AY 2025‚Äì26</p>
  <nav>
    <ul>
      <li><a href="../../index.html">Home</a></li>
      <li><a href="../../assets/Shaan_CV.pdf">CV</a></li>
      <li><a href="../../sections/projects.html">Projects</a></li>
      <li><a href="../../sections/education.html">Education</a></li>
      <li><a href="../../sections/misc.html">Misc</a></li>
    </ul>
  </nav>
</header>

<section class="notes-wrapper">

  <div class="info-block">
    <p><strong>Instructor:</strong> Shyamal K De </p>
    <p><strong>Office / Department:</strong> ASU</p>
    <p><strong>Email:</strong> skd.isical@gmail.com</p>
    <p><strong>Marking Scheme:</strong><br>
      Class Test: 10% | Project: 15: | Midterm Test: 25% | End Semester: 50%
    </p>
  </div>

 <div class="glow-box" id="pdf">
<section>

<h2>PDF Version of the Notes</h2>

<p>For offline reading, printing, or long-form study, a compiled <strong>PDF version</strong> of these contents is provided.</p>

<p>The PDF mirrors the structure and notation of the webpage and is suitable for academic reference.</p>

<h3>Download</h3>

<p>
  <a href="../../assets/Median.pdf" target="_blank" style="color:#6effe0; text-decoration:none; font-weight:500;">
    üìÑ Download the PDF version of these notes
  </a>
</p>

<h3>Remarks</h3>

<ul>
  <li>The PDF is generated directly from the same source material as this webpage</li>
  <li>Mathematical notation and figures are preserved</li>
  <li>Page numbering makes it convenient for citation and discussion</li>
</ul>

</section>
</div>


  <!-- Notes section boxes -->
 <!-- INDEX_TOPIC -->

<div class="glow-box" id="2.1">
<section>

<h2>1. Computation: what is <em>the</em> sample median, really?</h2>

<p>Up to now, the sample median (<span>\(\hat\mu = T(F_n)\)</span>) was defined abstractly via several equivalent population characterizations. But when you plug in the <strong>empirical CDF</strong> (<span>\(F_n\)</span>):</p>

<ul>
  <li>Those definitions <strong>need not give a unique solution</strong></li>
  <li>So a <strong>convention</strong> is imposed</li>
</ul>

<h3>Procedure</h3>

<ol>
  <li>
    <p>Order the data:</p>
    <p>
      \[
      x_{(1)} \le x_{(2)} \le \cdots \le x_{(n)}
      \]
    </p>
  </li>
  <li>
    <p>Define:</p>
    <p>
      \[
      \hat\mu = \frac{x_{[(n+1)/2]} + x_{[(n+2)/2]}}{2}
      \]
    </p>
  </li>
</ol>

<h3>Interpretation</h3>

<ul>
  <li>If <span>\(n\)</span> is odd: both indices coincide ‚Üí usual middle order statistic</li>
  <li>If <span>\(n\)</span> is even: average of the two central observations</li>
</ul>

<h3>Why this matters</h3>

<ul>
  <li>This choice ensures symmetry and equivariance</li>
  <li>It matches the estimating-equation view (<span>\(\hat R(\hat\mu)=0\)</span>)</li>
</ul>

<h3>Important remark</h3>

<blockquote>
  <p>In the multivariate case there is no natural ordering.</p>
</blockquote>

<p>This sentence is a warning shot: everything above relies on order. Once you lose order, the median becomes genuinely geometric.</p>

</section>
</div>

<div class="glow-box" id="2.2">
<section>

<h2>2. Robustness: why statisticians love the median</h2>

<p>Two core robustness concepts appear here.</p>

<h3>(a) Breakdown point = <span>\(\tfrac12\)</span></h3>

<p>The <strong>asymptotic breakdown point</strong> is the smallest fraction of contamination that can drive the estimator arbitrarily far.</p>

<p>For the median:</p>

<p>
\[
\varepsilon^* = \frac12
\]
</p>

<h4>Meaning</h4>

<ul>
  <li>You can replace almost half the data with arbitrarily bad outliers</li>
  <li>The median still does not explode</li>
</ul>

<p>No location estimator can do better. This is maximal robustness.</p>

<h3>(b) Influence function is bounded</h3>

<p>The influence function is:</p>

<p>
\[
\mathrm{IF}(x;T,F) = \delta^{-1} S(x - T(F)), \quad \delta = 2f(\mu)
\]
</p>

<h4>Key features</h4>

<ul>
  <li>Takes only three values: <span>\((\pm \delta^{-1}, 0)\)</span></li>
  <li>Does <strong>not grow</strong> as <span>\(|x|\to\infty\)</span></li>
</ul>

<h4>Interpretation</h4>

<ul>
  <li>A single extreme outlier has <em>limited effect</em></li>
  <li>Contrast with the mean, whose influence function is linear and unbounded</li>
</ul>

<p>This formally explains why the median resists outliers.</p>

</section>
</div>

<div class="glow-box" id="2.3">
<section>

<h2>3. Asymptotic efficiency: price of robustness</h2>

<p>Now the comparison with the sample mean begins.</p>

<h3>Assume</h3>

<ul>
  <li><span>\(F\)</span> has finite variance <span>\(\sigma^2\)</span></li>
</ul>

<p>Then the sample mean satisfies:</p>

<p>
\[
\sqrt{n}(\bar x - \mu) \;\to\; N(0,\sigma^2)
\]
</p>

<p>The median satisfies:</p>

<p>
\[
\sqrt{n}(\hat\mu - \mu) \;\to\; N\!\left(0,\frac{1}{4f(\mu)^2}\right)
\]
</p>

<h3>Asymptotic Relative Efficiency (ARE)</h3>

<p>Defined as:</p>

<p>
\[
\mathrm{ARE}(\text{median},\text{mean})
= \frac{\text{Var(mean)}}{\text{Var(median)}}
= 4 f(\mu)^2 \sigma^2
\]
</p>

<h4>Interpretation</h4>

<ul>
  <li>ARE &lt; 1 ‚Üí median less efficient</li>
  <li>ARE &gt; 1 ‚Üí median more efficient</li>
</ul>

<h4>Examples</h4>

<ul>
  <li>
    Normal <span>\(N(\mu,\sigma^2)\)</span>:
    <span>\(f(\mu)=\frac{1}{\sqrt{2\pi}\sigma}\)</span> ‚Üí ARE ‚âà <strong>0.64</strong>
  </li>
  <li>
    Heavy-tailed distributions:
    <ul>
      <li><span>\(t_3\)</span>: ARE ‚âà <strong>1.62</strong></li>
      <li>Laplace: ARE = <strong>2</strong></li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Under Gaussian noise, the mean wins. Under heavy tails, the median dominates.</p>
</blockquote>

<p>This is the classic <strong>robustness‚Äìefficiency tradeoff</strong>.</p>

</section>
</div>

<div class="glow-box" id="2.4">
<section>

<h2>4. Estimating the variance: a practical headache</h2>

<p>To build confidence intervals for <span>\(\mu\)</span> using the asymptotic normality of <span>\(\hat\mu\)</span>, you need:</p>

<p>
\[
\delta = 2f(\mu)
\]
</p>

<h3>Problem</h3>

<ul>
  <li><span>\(f(\mu)\)</span> is unknown</li>
  <li>Density estimation at a point is unstable</li>
</ul>

<blockquote>
  <p>Estimation of <span>\(\delta\)</span> from the data is difficult.</p>
</blockquote>

</section>
</div>

<div class="glow-box" id="2.5">
<section>

<h2>5. Exact, distribution-free confidence intervals (the clever trick)</h2>

<p>Instead of estimating <span>\(\delta\)</span>, invert the <strong>sign test</strong>.</p>

<p>For a continuous <span>\(F\)</span>:</p>

<p>
\[
P(x_{(i)} &lt; \mu &lt; x_{(n+1-i)})
= P\!\left(
i \le \frac{n\hat R(\mu)+1}{2} \le n-i
\right)
\]
</p>

<p>Since:</p>

<p>
\[
\frac{n\hat R(\mu)+1}{2} \sim \mathrm{Bin}(n,\tfrac12)
\]
</p>

<p>We get:</p>

<p>
\[
P(x_{(i)} &lt; \mu &lt; x_{(n+1-i)})
= \sum_{j=i}^{n-i} \binom{n}{j}2^{-n}
\]
</p>

<h3>Interpretation</h3>

<ul>
  <li>Confidence interval = <strong>order-statistic interval</strong></li>
  <li>Coverage probability = exact binomial tail</li>
  <li>No density estimation</li>
  <li>Fully distribution-free</li>
</ul>

<p>This is one of the deepest practical advantages of the median.</p>

</section>
</div>

<div class="glow-box" id="2.6">
<section>

<h2>6. Equivariance: how the median behaves under transformations</h2>

<p>For a location functional <span>\(T\)</span>, we want:</p>

<p>
\[
T(F_{aX+b}) = aT(F_X) + b
\]
</p>

<h3>Meaning</h3>

<ul>
  <li>Shift data ‚Üí estimator shifts</li>
  <li>Rescale data ‚Üí estimator rescales</li>
</ul>

<p>The text states:</p>

<ul>
  <li>This holds for the median when <span>\(f\)</span> is smooth near <span>\(\mu\)</span></li>
  <li>In fact, something stronger is true</li>
</ul>

<p>If <span>\(g\)</span> is <strong>strictly monotone</strong>, then:</p>

<p>
\[
T(F_{g(X)}) = g(T(F_X))
\]
</p>

<h3>Interpretation</h3>

<ul>
  <li>Apply log, exp, power transforms</li>
  <li>Median transforms <em>exactly</em> as the data do</li>
</ul>

<p>The mean does <strong>not</strong> have this property.</p>

</section>
</div>

<!-- INDEX_TOPIC -->

<div class="glow-box" id="2.7">
<section>

<h2>Influence Function: why the median resists outliers</h2>

<h3>1. What is the influence function, conceptually?</h3>

<p>The <strong>influence function (IF)</strong> answers one precise question:</p>

<blockquote>
  <p>If I contaminate the distribution <span>\(F\)</span> by an infinitesimal amount of mass at the point <span>\(x\)</span>, how much does my estimator move?</p>
</blockquote>

<p>Formally:</p>

<p>
\[
\mathrm{IF}(x;T,F)
=
\left.
\frac{d}{d\varepsilon}
T\big((1-\varepsilon)F + \varepsilon \Delta_x\big)
\right|_{\varepsilon=0}
\]
</p>

<p>where:</p>

<ul>
  <li><span>\(T\)</span> = estimator viewed as a functional (here: the median)</li>
  <li><span>\(\Delta_x\)</span> = point mass at <span>\(x\)</span></li>
</ul>

<p>Think of the influence function as the <strong>first-order sensitivity</strong> of the estimator to a single outlier at <span>\(x\)</span>.</p>

<hr>

<h3>2. Why the sign function appears for the median</h3>

<p>The population median <span>\(\mu = T(F)\)</span> satisfies the estimating equation:</p>

<p>
\[
\mathbb{E}_F[S(X-\mu)] = 0
\]
</p>

<p>This equation <em>defines</em> the median.</p>

<p>Now contaminate the distribution slightly:</p>

<p>
\[
F_\varepsilon = (1-\varepsilon)F + \varepsilon \Delta_x
\]
</p>

<p>The new median <span>\(T(F_\varepsilon)\)</span> must still satisfy:</p>

<p>
\[
\mathbb{E}_{F_\varepsilon}
\big[S(X - T(F_\varepsilon))\big] = 0
\]
</p>

<p>Linearizing this equation around <span>\(\varepsilon=0\)</span> is standard <strong>M-estimator calculus</strong>. The structure of the estimating equation forces the <strong>sign function</strong> to appear.</p>

<hr>

<h3>3. Where the constant <span>\(\delta = 2f(\mu)\)</span> comes from</h3>

<p>Differentiate the estimating equation with respect to <span>\(t\)</span>:</p>

<p>
\[
\frac{d}{dt}\mathbb{E}[S(X-t)]\Big|_{t=\mu}
= -2f(\mu)
\]
</p>

<h4>Explanation</h4>

<ul>
  <li><span>\(S(X-t)\)</span> jumps at <span>\(X=t\)</span></li>
  <li>The derivative picks up mass from the density at <span>\(t=\mu\)</span></li>
  <li>The slope is exactly <span>\(-2f(\mu)\)</span></li>
</ul>

<p>Conclusion: the median reacts <strong>more strongly</strong> when the density at the median is small (a flat region).</p>

<hr>

<h3>4. The final influence function for the median</h3>

<p>Putting everything together:</p>

<p>
\[
\boxed{
\mathrm{IF}(x;T,F)
=
\frac{1}{2f(\mu)}\, S(x-\mu)
}
\]
</p>

<p>Explicitly:</p>

<p>
\[
\mathrm{IF}(x;T,F)
=
\begin{cases}
+\dfrac{1}{2f(\mu)} & x > \mu \\
0 & x = \mu \\
-\dfrac{1}{2f(\mu)} & x < \mu
\end{cases}
\]
</p>

<hr>

<h3>5. Why this is a <em>big deal</em></h3>

<h4>(a) The influence function is bounded</h4>

<p>No matter how large <span>\(x\)</span> is:</p>

<p>
\[
|\mathrm{IF}(x;T,F)| \le \frac{1}{2f(\mu)}
\]
</p>

<p>This is the <strong>formal reason</strong> the median is robust.</p>

<p>Compare with the mean:</p>

<p>
\[
\mathrm{IF}_{\text{mean}}(x) = x - \mu
\]
</p>

<p>which diverges as <span>\(|x|\to\infty\)</span>.</p>

<h4>(b) Interpretation in plain language</h4>

<ul>
  <li>An observation far above the median pushes it up by a <em>fixed amount</em></li>
  <li>An observation far below the median pushes it down by the <em>same fixed amount</em></li>
  <li>Extreme values are <strong>automatically clipped</strong></li>
</ul>

<p>The median does not care <em>how far</em> the outlier is ‚Äî only <strong>which side</strong> it lies on.</p>

<hr>

<h3>6. Connection to breakdown point</h3>

<p>Because the influence function is bounded:</p>

<ul>
  <li>A single outlier cannot destroy the estimator</li>
  <li>You need <strong>half the data</strong> on one side to move the median arbitrarily</li>
</ul>

<p>This is why the breakdown point equals <strong>1/2</strong>.</p>

<hr>

<h3>7. Mental picture (very important)</h3>

<p>Visualize the median as a <strong>balance point</strong>:</p>

<ul>
  <li>Points to the right push right</li>
  <li>Points to the left push left</li>
  <li>Push strength is constant</li>
  <li>Only the <em>number</em> of points matters, not their magnitude</li>
</ul>

<p>This geometric intuition is exactly encoded by the <strong>sign function</strong> in the influence function.</p>

<hr>

<h3>8. Why <span>\(f(\mu)\)</span> matters</h3>

<ul>
  <li>If <span>\(f(\mu)\)</span> is small:
    <ul>
      <li>The distribution is flat near the median</li>
      <li>A small perturbation shifts the median a lot</li>
    </ul>
  </li>
  <li>If <span>\(f(\mu)\)</span> is large:
    <ul>
      <li>The median is well-anchored</li>
    </ul>
  </li>
</ul>

<p>This also explains the variance formula:</p>

<p>
\[
\operatorname{Var}(\hat\mu) \approx \frac{1}{4n f(\mu)^2}
\]
</p>

</section>
</div>



<!-- INDEX_TOPIC -->

<div class="glow-box" id="2.8">
<section>

<h2>Deriving the influence function by linearization</h2>

<h3>1. The equation we want to linearize</h3>

<p>The median is defined by the <strong>estimating equation</strong>:</p>

<p>
\[
\Psi(t, F) := \mathbb{E}_F\big[S(X - t)\big] = 0
\]
</p>

<p>The true median <span>\(\mu\)</span> satisfies:</p>

<p>
\[
\Psi(\mu, F) = 0.
\]
</p>

<p>Now contaminate the distribution:</p>

<p>
\[
F_\varepsilon = (1-\varepsilon)F + \varepsilon \Delta_x,
\]
</p>

<p>and let the corresponding median be:</p>

<p>
\[
t_\varepsilon := T(F_\varepsilon).
\]
</p>

<p>By definition, <span>\(t_\varepsilon\)</span> satisfies:</p>

<p>
\[
\Psi(t_\varepsilon, F_\varepsilon) = 0.
\]
</p>

<p>This is the equation we will <strong>linearize around</strong> <span>\((\mu,0)\)</span>.</p>

<hr>

<h3>2. What ‚Äúlinearize‚Äù means here</h3>

<p>‚ÄúLinearize‚Äù means:</p>

<blockquote>
  <p>Take a first-order Taylor expansion of <span>\(\Psi(t_\varepsilon, F_\varepsilon)\)</span> around <span>\(t=\mu\)</span> and <span>\(\varepsilon=0\)</span>.</p>
</blockquote>

<p>Nothing more than first-order calculus.</p>

<hr>

<h3>3. Expansion with respect to both arguments</h3>

<p>Write:</p>

<p>
\[
0
=
\Psi(t_\varepsilon, F_\varepsilon)
\approx
\Psi(\mu, F)
+
\frac{\partial \Psi}{\partial t}\Big|_{\mu,F}
(t_\varepsilon-\mu)
+
\frac{\partial \Psi}{\partial \varepsilon}\Big|_{\mu,0}\varepsilon
\]
</p>

<p>Since <span>\(\Psi(\mu,F)=0\)</span>, this reduces to:</p>

<p>
\[
0
\approx
\frac{\partial \Psi}{\partial t}(\mu,F)(t_\varepsilon-\mu)
+
\frac{\partial \Psi}{\partial \varepsilon}(\mu,0)\varepsilon.
\]
</p>

<p>We now compute both derivatives explicitly.</p>

<hr>

<h3>4. Derivative with respect to <span>\(t\)</span></h3>

<p>Recall:</p>

<p>
\[
\Psi(t,F)=\mathbb{E}_F[S(X-t)].
\]
</p>

<p>As <span>\(t\)</span> increases, the sign function flips at <span>\(X=t\)</span>. The derivative comes from this jump:</p>

<p>
\[
\frac{\partial}{\partial t}\Psi(t,F)
=
-2f(t).
\]
</p>

<p>Evaluated at <span>\(t=\mu\)</span>:</p>

<p>
\[
\frac{\partial \Psi}{\partial t}(\mu,F)
=
-2f(\mu)
=
-\delta.
\]
</p>

<p>This is exactly where the constant <span>\(\delta=2f(\mu)\)</span> comes from.</p>

<hr>

<h3>5. Derivative with respect to <span>\(\varepsilon\)</span></h3>

<p>Using the contaminated distribution:</p>

<p>
\[
\Psi(t,F_\varepsilon)
=
(1-\varepsilon)\mathbb{E}_F[S(X-t)]
+
\varepsilon S(x-t).
\]
</p>

<p>Differentiate with respect to <span>\(\varepsilon\)</span>:</p>

<p>
\[
\frac{\partial \Psi}{\partial \varepsilon}
=
-\mathbb{E}_F[S(X-t)] + S(x-t).
\]
</p>

<p>At <span>\(t=\mu\)</span>, since <span>\(\mathbb{E}_F[S(X-\mu)]=0\)</span>:</p>

<p>
\[
\frac{\partial \Psi}{\partial \varepsilon}(\mu,0)
=
S(x-\mu).
\]
</p>

<hr>

<h3>6. Putting the pieces together</h3>

<p>Insert both derivatives into the linearized equation:</p>

<p>
\[
0
\approx
(-2f(\mu))(t_\varepsilon-\mu)
+
\varepsilon S(x-\mu).
\]
</p>

<p>Solving for <span>\(t_\varepsilon-\mu\)</span>:</p>

<p>
\[
t_\varepsilon-\mu
\approx
\frac{\varepsilon}{2f(\mu)}S(x-\mu).
\]
</p>

<hr>

<h3>7. Definition of the influence function</h3>

<p>By definition:</p>

<p>
\[
\mathrm{IF}(x;T,F)
=
\left.
\frac{d}{d\varepsilon}t_\varepsilon
\right|_{\varepsilon=0}.
\]
</p>

<p>From the expansion above:</p>

<p>
\[
\boxed{
\mathrm{IF}(x;T,F)
=
\frac{1}{2f(\mu)}S(x-\mu)
}
\]
</p>

<hr>

<h3>8. What ‚Äústandard M-estimator calculus‚Äù really means</h3>

<p>Whenever an estimator <span>\(T(F)\)</span> is defined by:</p>

<p>
\[
\mathbb{E}_F[\psi(X,T(F))]=0,
\]
</p>

<p>the influence function is:</p>

<p>
\[
\mathrm{IF}(x)
=
\left(
\mathbb{E}\Big[\frac{\partial}{\partial t}\psi(X,t)\Big|_{t=\mu}\Big]
\right)^{-1}
\psi(x,\mu).
\]
</p>

<p>For the median:</p>

<ul>
  <li><span>\(\psi(x,t)=S(x-t)\)</span></li>
  <li>The derivative equals <span>\(-2f(\mu)\)</span></li>
</ul>

<p>Everything above is a concrete instance of this general rule.</p>

<hr>

<h3>9. Intuition check</h3>

<blockquote>
  <p>A tiny contamination moves the median proportionally to the sign of the contaminating point, scaled by how steep the CDF is at the median.</p>
</blockquote>

<p>No higher-order effects matter at first order. That is the entire point of linearization.</p>

</section>
</div>

<!-- INDEX_TOPIC -->

<div class="glow-box" id="2.9">
<section>

<h2>What is ARE (Asymptotic Relative Efficiency)?</h2>

<p><strong>ARE ‚Äî Asymptotic Relative Efficiency</strong> ‚Äî is a precise, asymptotic way to compare two estimators <em>when the sample size is large</em>. It answers one sharp question:</p>

<blockquote>
  <p>How many samples does estimator A need to match the precision of estimator B?</p>
</blockquote>

<hr>

<h3>The formal definition</h3>

<p>Suppose two estimators <span>\(\hat\theta_1\)</span> and <span>\(\hat\theta_2\)</span> estimate the same parameter <span>\(\theta\)</span>, and both are <span>\(\sqrt{n}\)</span>-consistent and asymptotically normal:</p>

<p>
\[
\sqrt{n}(\hat\theta_i-\theta)\xrightarrow{d}N(0,V_i),\quad i=1,2.
\]
</p>

<p>Then the <strong>ARE of <span>\(\hat\theta_1\)</span> relative to <span>\(\hat\theta_2\)</span></strong> is defined as:</p>

<p>
\[
\boxed{
\mathrm{ARE}(\hat\theta_1,\hat\theta_2)=\frac{V_2}{V_1}
}
\]
</p>

<h4>Interpretation</h4>

<ul>
  <li>ARE = 1: equally efficient</li>
  <li>ARE &lt; 1: <span>\(\hat\theta_1\)</span> is less efficient</li>
  <li>ARE &gt; 1: <span>\(\hat\theta_1\)</span> is more efficient</li>
</ul>

<hr>

<h3>Why it‚Äôs called <em>relative</em></h3>

<p>If <span>\(\mathrm{ARE}=0.64\)</span>, estimator 1 needs about:</p>

<p>
\[
\frac{1}{0.64}\approx1.56
\]
</p>

<p>times <strong>more data</strong> to achieve the same asymptotic accuracy as estimator 2.</p>

<p>So ARE is literally a <strong>sample-size exchange rate</strong>.</p>

<hr>

<h3>ARE for median vs mean</h3>

<p>For a symmetric distribution <span>\(F\)</span>:</p>

<ul>
  <li>
    Mean:
    <p>
    \[
    \sqrt{n}(\bar X-\mu)\to N(0,\sigma^2)
    \]
    </p>
  </li>
  <li>
    Median:
    <p>
    \[
    \sqrt{n}(\hat\mu-\mu)\to N\!\left(0,\frac{1}{4f(\mu)^2}\right)
    \]
    </p>
  </li>
</ul>

<p>Therefore:</p>

<p>
\[
\boxed{
\mathrm{ARE}(\text{median},\text{mean})
=
\frac{\sigma^2}{\frac{1}{4f(\mu)^2}}
=
4f(\mu)^2\sigma^2
}
\]
</p>

<hr>

<h3>Concrete examples</h3>

<ul>
  <li>
    <strong>Normal distribution</strong> <span>\(N(\mu,\sigma^2)\)</span><br>
    <span>\(f(\mu)=\frac{1}{\sqrt{2\pi}\sigma}\)</span>
    <p>
    \[
    \mathrm{ARE}=\frac{2}{\pi}\approx0.64
    \]
    </p>
    <p>The median loses efficiency under Gaussian noise.</p>
  </li>

  <li>
    <strong>Heavy-tailed distributions</strong>
    <ul>
      <li><span>\(t_3\)</span>: ARE ‚âà <strong>1.62</strong></li>
      <li>Laplace: ARE = <strong>2</strong></li>
    </ul>
    <p>The median wins decisively.</p>
  </li>
</ul>

<hr>

<h3>What ARE is <em>not</em></h3>

<ul>
  <li>Not a finite-sample guarantee</li>
  <li>Not a robustness measure</li>
  <li>Not about bias</li>
</ul>

<p>ARE is <strong>purely about asymptotic variance</strong>.</p>

<hr>

<h3>Takeaway</h3>

<p>ARE quantifies the classic tradeoff:</p>

<blockquote>
  <p>The mean is optimal under Gaussian noise.<br>
     The median sacrifices efficiency to gain robustness ‚Äî and under heavy tails, that sacrifice becomes a win.</p>
</blockquote>

</section>
</div>

<!-- INDEX_TOPIC -->

<div class="glow-box" id="3.1">
<section>

<h2>Affine Equivariance of the Estimate; Transformation‚ÄìRetransformation (TR) Median</h2>

<h3>1. Formal setup and definitions</h3>

<p>Let <span>\(X \in \mathbb{R}^p\)</span> be a random vector with distribution <span>\(F_X\)</span>.</p>

<h4>1.1 Multivariate location functional</h4>

<p>A <strong>multivariate location functional</strong> is a mapping</p>

<p>
\[
T : \mathcal{F}_p \to \mathbb{R}^p,
\]
</p>

<p>where <span>\(\mathcal{F}_p\)</span> is a suitable class of distributions on <span>\(\mathbb{R}^p\)</span>.</p>

<p>Examples:</p>

<ul>
  <li>Mean vector: <span>\(T(F_X) = \mathbb{E}[X]\)</span></li>
  <li>Vector of marginal medians:
    <p>
    \[
    T(F_X) = (\operatorname{med}(X_1), \ldots, \operatorname{med}(X_p))^\top
    \]
    </p>
  </li>
</ul>

<h4>1.2 Affine transformations of distributions</h4>

<p>For a full-rank matrix <span>\(A \in \mathbb{R}^{p\times p}\)</span> and vector <span>\(b \in \mathbb{R}^p\)</span>, define</p>

<p>
\[
Y = AX + b.
\]
</p>

<p>The distribution of <span>\(Y\)</span> is denoted by <span>\(F_{AX+b}\)</span>.</p>

<h4>1.3 Affine equivariance (formal definition)</h4>

<p>A location functional <span>\(T\)</span> is <strong>affine equivariant</strong> if</p>

<p>
\[
\boxed{
T(F_{AX+b}) = A\,T(F_X) + b
}
\]
</p>

<p>for all full-rank <span>\(A\)</span> and all <span>\(b\)</span>.</p>

<p>This is a <strong>structural requirement</strong>, not a statistical convenience.</p>

<hr>

<h3>2. Why affine equivariance is expected</h3>

<h4>Formal meaning</h4>

<p>Affine transformations include:</p>

<ul>
  <li>translations</li>
  <li>rescalings</li>
  <li>rotations</li>
  <li>shears</li>
  <li>any composition of the above</li>
</ul>

<p>Affine equivariance ensures that the estimator <strong>respects the geometry of the data</strong>.</p>

<h4>Intuition</h4>

<p>If you rotate your coordinate system, your notion of ‚Äúcenter‚Äù should rotate with it.  
If you stretch the axes, the center should stretch accordingly.</p>

<p>If this fails, the estimator is coordinate-dependent and geometrically inconsistent.</p>

<hr>

<h3>3. The vector of marginal medians is <em>not</em> affine equivariant</h3>

<h4>3.1 Definition of the marginal median vector</h4>

<p>
\[
T(F_X) =
\begin{pmatrix}
\operatorname{med}(X_1) \\
\vdots \\
\operatorname{med}(X_p)
\end{pmatrix}.
\]
</p>

<p>Each component is computed <strong>independently</strong>, ignoring dependence among coordinates.</p>

<h4>3.2 Failure of affine equivariance (formal argument)</h4>

<p>Consider an affine transformation:</p>

<p>
\[
Y = AX + b,
\quad A = (a_{ij})_{i,j=1}^p.
\]
</p>

<p>Then</p>

<p>
\[
Y_i = \sum_{j=1}^p a_{ij} X_j + b_i.
\]
</p>

<p>The median of <span>\(Y_i\)</span> satisfies</p>

<p>
\[
\operatorname{med}(Y_i)
=
\operatorname{med}\!\left(\sum_{j=1}^p a_{ij} X_j\right) + b_i.
\]
</p>

<p>In general,</p>

<p>
\[
\operatorname{med}\!\left(\sum_{j=1}^p a_{ij} X_j\right)
\neq
\sum_{j=1}^p a_{ij} \operatorname{med}(X_j),
\]
</p>

<p>unless <strong>only one term is present</strong>.</p>

<p>Hence,</p>

<p>
\[
T(F_{AX+b}) \neq A T(F_X) + b
\]
</p>

<p>for general <span>\(A\)</span>.</p>

<h4>3.3 Special case where it works</h4>

<p>If <span>\(A\)</span> is diagonal with nonzero entries,</p>

<p>
\[
A = \mathrm{diag}(a_1,\dots,a_p),
\]
</p>

<p>then</p>

<p>
\[
Y_i = a_i X_i + b_i,
\]
</p>

<p>and since univariate medians are affine equivariant,</p>

<p>
\[
\operatorname{med}(Y_i) = a_i \operatorname{med}(X_i) + b_i.
\]
</p>

<p>Thus affine equivariance holds <strong>only for diagonal</strong> <span>\(A\)</span>.</p>

<h4>Intuition</h4>

<p>Marginal medians treat each axis as sacred.  
The moment you mix coordinates (rotation, shear), the estimator refuses to cooperate.</p>

<p>This is not a bug‚Äîit is a <strong>structural limitation</strong> of marginal thinking in multivariate space.</p>

<hr>

<h3>4. Invariant Coordinate System (ICS) functional</h3>

<p>To repair this, we introduce a <strong>coordinate-aware transformation</strong>.</p>

<h4>4.1 Definition (formal)</h4>

<p>A matrix-valued functional</p>

<p>
\[
G : \mathcal{F}_p \to \mathbb{R}^{p\times p}
\]
</p>

<p>is called an <strong>Invariant Coordinate System (ICS)</strong> functional if</p>

<p>
\[
\boxed{
G(F_{AX+b}) = G(F_X) A^{-1}
}
\]
</p>

<p>for all full-rank <span>\(A\)</span> and <span>\(b\)</span>.</p>

<h4>4.2 Interpretation</h4>

<ul>
  <li><span>\(G(F_X)\)</span> chooses a <strong>data-dependent coordinate system</strong></li>
  <li>Under affine transformation, the coordinate system adapts <strong>contragrediently</strong></li>
</ul>

<h4>Intuition</h4>

<p>Think of <span>\(G(F_X)\)</span> as saying:</p>

<blockquote>
  <p>‚ÄúBefore doing anything, I‚Äôll rotate and scale the data into a canonical position.‚Äù</p>
</blockquote>

<p>If the data are transformed, the canonicalizer compensates exactly.</p>

<hr>

<h3>5. Transformation‚ÄìRetransformation (TR) median</h3>

<h4>5.1 Definition (formal)</h4>

<p>Let <span>\(T\)</span> be the <strong>vector of marginal medians</strong>.</p>

<p>Define the <strong>TR median functional</strong>:</p>

<p>
\[
\boxed{
T_{\mathrm{TR}}(F_X)
=
G(F_X)^{-1}
\,
T\!\left(F_{\,G(F_X)X}\right)
}
\]
</p>

<p>This is a three-step procedure:</p>

<ol>
  <li>Transform the data using <span>\(G(F_X)\)</span></li>
  <li>Compute marginal medians</li>
  <li>Retransform back</li>
</ol>

<h4>5.2 Proof of affine equivariance</h4>

<p>Let <span>\(Y = AX + b\)</span>. Then:</p>

<p>
\[
\begin{aligned}
T_{\mathrm{TR}}(F_Y)
&= G(F_Y)^{-1} \, T(F_{G(F_Y)Y}) \\
&= (G(F_X)A^{-1})^{-1}
\, T(F_{G(F_X)A^{-1}(AX+b)}) \\
&= A G(F_X)^{-1}
\, T(F_{G(F_X)X + G(F_X)A^{-1}b})
\end{aligned}
\]
</p>

<p>Using translation equivariance of marginal medians,</p>

<p>
\[
T(F_{Z+c}) = T(F_Z) + c,
\]
</p>

<p>we obtain</p>

<p>
\[
T_{\mathrm{TR}}(F_Y)
= A T_{\mathrm{TR}}(F_X) + b.
\]
</p>

<p>Thus <span>\(T_{\mathrm{TR}}\)</span> is <strong>affine equivariant</strong>.</p>

<h4>Intuition</h4>

<p>This is ‚Äúmedian, but done in the right coordinates.‚Äù</p>

<p>Instead of forcing medians to understand geometry,  
we <strong>teach geometry first</strong>, then compute medians, then come back.</p>

<hr>

<h3>6. Common pitfalls</h3>

<ul>
  <li>Believing marginal robustness implies multivariate robustness</li>
  <li>Ignoring coordinate dependence</li>
  <li>Assuming medians behave like means under linear mixing</li>
</ul>

<p>The TR construction fixes all three.</p>

</section>
</div>

<!-- INDEX_TOPIC -->

<div class="glow-box" id="3.2">
<section>

<h2>Why the Transformation‚ÄìRetransformation (TR) concept matters</h2>

<p>The <strong>Transformation‚ÄìRetransformation (TR)</strong> idea is not cosmetic. It fixes a <em>structural flaw</em> in na√Øve multivariate robust estimators and does so in a way that is mathematically principled, geometrically honest, and practically useful.</p>

<hr>

<h3>1. What problem does TR actually solve?</h3>

<h4>The blunt truth</h4>

<p>Most ‚Äúsimple‚Äù multivariate robust estimators (such as the vector of marginal medians) are <strong>coordinate artifacts</strong>. Their output depends on how you choose your axes, not on the intrinsic geometry of the data cloud.</p>

<p>This is unacceptable when data live in <span>\(\mathbb{R}^p\)</span> as geometry, not as <span>\(p\)</span> unrelated columns.</p>

<h4>What affine equivariance really buys you</h4>

<blockquote>
  <p>Two analysts using different linear coordinate systems will report the same center, up to the same transformation.</p>
</blockquote>

<p>This is not philosophical. It is <strong>statistical reproducibility under reparameterization</strong>.</p>

<p>Without affine equivariance:</p>

<ul>
  <li>Rotating the data changes the estimator</li>
  <li>Mixing variables changes the estimator</li>
  <li>Scientific conclusions depend on arbitrary preprocessing</li>
</ul>

<p>TR restores this invariance <strong>without sacrificing robustness</strong>.</p>

<hr>

<h3>2. What TR is doing conceptually</h3>

<p>TR is a <strong>change-of-coordinates strategy</strong>:</p>

<ol>
  <li><strong>Find the geometry of the data</strong><br>
      Use an ICS functional <span>\(G(F)\)</span> to identify directions, scales, and shape.</li>
  <li><strong>Move to canonical coordinates</strong><br>
      Transform the data so the cloud is standardized (often spherical or axis-aligned).</li>
  <li><strong>Apply a simple robust estimator</strong><br>
      Compute marginal medians where they actually make sense.</li>
  <li><strong>Transform back</strong><br>
      Return to the original space.</li>
</ol>

<p>The estimator stays simple. The <em>space</em> is made intelligent.</p>

<hr>

<h4>Mental picture</h4>

<p>Imagine a <strong>tilted elliptical cloud</strong> of points in 2D:</p>

<ul>
  <li>The true center is the center of the ellipse</li>
  <li>The cloud is rotated by, say, 30¬∞</li>
</ul>

<p>Now compute:</p>

<ul>
  <li>The median of <span>\(X_1\)</span></li>
  <li>The median of <span>\(X_2\)</span></li>
</ul>

<p>These medians are taken <strong>along the coordinate axes</strong>, not along the geometry of the cloud.</p>

<h4>What goes wrong</h4>

<ul>
  <li>Correlation is ignored</li>
  <li>Rotating the cloud changes the reported ‚Äúcenter‚Äù</li>
  <li>The estimator is not intrinsic to the data</li>
</ul>

<p>This is the concrete failure of affine equivariance.</p>

<hr>

<h4>Step-by-step visual logic</h4>

<ol>
  <li><strong>Before TR</strong><br>
      The data cloud is elongated and rotated.</li>
  <li><strong>Apply <span>\(G(F)\)</span></strong><br>
      Rotation and scaling are undone; the ellipse becomes roughly spherical.</li>
  <li><strong>Compute marginal medians</strong><br>
      Axes now align with structure; medians behave sensibly.</li>
  <li><strong>Retransform</strong><br>
      The center is mapped back to the original geometry.</li>
</ol>

<p>The final point:</p>

<ul>
  <li>Sits at the geometric center</li>
  <li>Moves correctly under affine transformations</li>
  <li>Remains robust to outliers</li>
</ul>

<hr>

<h3>3. Why this matters statistically</h3>

<h4>Robustness plus equivariance is rare</h4>

<ul>
  <li><strong>Mean</strong>: affine equivariant, not robust</li>
  <li><strong>Marginal medians</strong>: robust, not affine equivariant</li>
</ul>

<p>TR delivers <strong>both</strong>, provided the ICS functional is well chosen.</p>

<h4>In practice, TR enables</h4>

<ul>
  <li>Meaningful multivariate medians</li>
  <li>Robust PCA-like constructions</li>
  <li>Coordinate-free comparison across studies</li>
  <li>Geometrically honest inference</li>
</ul>

<p>This is why TR ideas appear repeatedly in robust multivariate analysis, outlier detection, and high-dimensional statistics.</p>

<hr>

<h3>4. A clean intuition to remember</h3>

<blockquote>
  <p><strong>TR does not fix the estimator.<br>
     It fixes the coordinate system in which the estimator is allowed to act.</strong></p>
</blockquote>

<p>Once you see it this way, the idea stops looking clever and starts looking inevitable.</p>

<p>The deeper lesson is simple:</p>

<p><strong>Robust statistics fails when it ignores geometry.  
TR is geometry-aware robustness.</strong></p>

</section>
</div>

  <!-- INDEX_TOPIC -->

<div class="glow-box" id="4.1">
<section>

<h2>The spatial median: geometry-aware robustness in <span>\(\mathbb{R}^p\)</span></h2>

<h3>1. What the spatial median is‚Äîand why it exists</h3>

<h4>Formal definition (sample version)</h4>

<p>Given data points <span>\(x_1,\dots,x_n \in \mathbb{R}^p\)</span>, define</p>

<p>
\[
D_n(t)
=
\frac{1}{n}\sum_{i=1}^n \bigl(|x_i - t| - |x_i|\bigr),
\quad t \in \mathbb{R}^p,
\]
</p>

<p>where <span>\(|\cdot|\)</span> denotes the Euclidean norm.</p>

<p>The <strong>spatial median</strong> <span>\(\hat\mu\)</span> is any minimizer of <span>\(D_n(t)\)</span>.</p>

<p>The subtraction of <span>\(|x_i|\)</span> is constant in <span>\(t\)</span> and does <strong>not</strong> affect the minimizer. It is included solely to ensure finiteness of expectations in the population version.</p>

<h4>Population version</h4>

<p>Let <span>\(X\sim F_X\)</span>. Define</p>

<p>
\[
D(t) = \mathbb{E}\bigl[|X-t| - |X|\bigr].
\]
</p>

<p>The <strong>spatial median functional</strong></p>

<p>
\[
\mu = T(F_X)
\]
</p>

<p>is the unique minimizer of <span>\(D(t)\)</span>, under the stated assumptions.</p>

<h4>Intuition</h4>

<p>The spatial median minimizes the <strong>average distance</strong> to the data cloud.</p>

<ul>
  <li>The mean minimizes average <em>squared</em> distance</li>
  <li>The spatial median minimizes average <em>distance</em></li>
</ul>

<p>This single change replaces sensitivity to magnitude by sensitivity to geometry.</p>

<hr>

<h3>2. Assumptions and why they matter</h3>

<h4>Assumption 1: uniqueness</h4>

<blockquote>
  <p>The minimizer <span>\(\mu\)</span> of <span>\(D(t)\)</span> is unique.</p>
</blockquote>

<p><strong>Why needed:</strong> Without uniqueness, asymptotic expansions and limiting distributions are not well-defined.</p>

<p><strong>Geometric intuition:</strong> In dimension <span>\(\ge 2\)</span>, surrounding geometry pins the center down; on a line, it can slide.</p>

<h4>Assumption 2: smoothness of the density</h4>

<blockquote>
  <p><span>\(F_X\)</span> has a bounded and continuous density at <span>\(\mu\)</span>.</p>
</blockquote>

<p><strong>Why needed:</strong> Ensures Taylor expansion of <span>\(D(t)\)</span> and finiteness of expectations involving <span>\(|X-\mu|^{-1}\)</span>.</p>

<hr>

<h3>3. Local quadratic expansion of the objective</h3>

<h4>Formal expansion</h4>

<p>Under the assumptions,</p>

<p>
\[
D(t)
=
D(\mu)
+
\frac{1}{2}(t-\mu)^\top \Gamma (t-\mu)
+
o(|t-\mu|^2),
\]
</p>

<p>where</p>

<p>
\[
\Gamma
=
\mathbb{E}\left[
\frac{1}{|X-\mu|}
\left(
I_p
-
\frac{(X-\mu)(X-\mu)^\top}{|X-\mu|^2}
\right)
\right].
\]
</p>

<h4>Why this matrix appears</h4>

<p>The gradient of <span>\(|x-t|\)</span> is</p>

<p>
\[
\nabla_t |x-t| = -\frac{x-t}{|x-t|},
\]
</p>

<p>and the Hessian introduces the projection matrix</p>

<p>
\[
I_p - \frac{(x-\mu)(x-\mu)^\top}{|x-\mu|^2},
\]
</p>

<p>which projects orthogonally to the direction <span>\(x-\mu\)</span>.</p>

<h4>Intuition</h4>

<p>Near the spatial median, the objective is quadratic‚Äîbut curvature depends on direction. The bowl is anisotropic and reflects how points surround the center.</p>

<hr>

<h3>4. Spatial sign and centered rank</h3>

<h4>Spatial sign</h4>

<p>
\[
S(t)
=
\begin{cases}
t/|t|, & t\neq 0,\\
0, & t=0.
\end{cases}
\]
</p>

<p>This is a <strong>direction-only</strong> object.</p>

<h4>Centered rank</h4>

<p>
\[
\hat R(t)
=
\frac{1}{n}\sum_{i=1}^n S(t - x_i).
\]
</p>

<h4>Key properties</h4>

<ul>
  <li>Lies inside the unit <span>\(p\)</span>-ball</li>
  <li>Ignores magnitude completely</li>
  <li>Retains geometric direction</li>
</ul>

<p>Each observation pulls with <strong>unit force</strong>, no matter how far away it is.</p>

<hr>

<h3>5. Spatial median as a zero of the rank function</h3>

<p>The spatial median satisfies</p>

<p>
\[
\hat R(\hat\mu) = 0.
\]
</p>

<p>This is the multivariate analogue of balancing signs to the left and right in one dimension.</p>

<hr>

<h3>6. Asymptotic distribution</h3>

<h4>Central limit theorem for ranks</h4>

<p>
\[
\sqrt{n}\,\hat R(\mu)
\xrightarrow{d}
N_p(0,\Omega),
\quad
\Omega
=
\mathbb{E}\left[
\frac{(X-\mu)(X-\mu)^\top}{|X-\mu|^2}
\right].
\]
</p>

<h4>Asymptotic normality of the estimator</h4>

<p>
\[
\hat\mu
=
\mu
+
\Gamma^{-1}\hat R(\mu)
+
o_P(n^{-1/2}),
\]
</p>

<p>hence</p>

<p>
\[
\sqrt{n}(\hat\mu-\mu)
\xrightarrow{d}
N_p\bigl(0,\Gamma^{-1}\Omega\Gamma^{-1}\bigr).
\]
</p>

<h4>Intuition</h4>

<ul>
  <li><span>\(\Omega\)</span> measures directional variability</li>
  <li><span>\(\Gamma^{-1}\)</span> converts force imbalance into displacement</li>
</ul>

<p>Think: <strong>force ‚Üí motion</strong>, governed by curvature.</p>

<hr>


<h4>What to see</h4>

<ul>
  <li>The spatial median sits where directional pulls cancel</li>
  <li>Distant outliers barely move it</li>
  <li>The center reflects shape, not extremes</li>
</ul>

<hr>

<h3>7. Computation: Weiszfeld algorithm</h3>

<h4>Iteration step</h4>

<p>
\[
\mu \leftarrow
\mu
+
\left(
\sum_{i=1}^n \frac{1}{|x_i-\mu|}
\right)^{-1}
\hat R(\mu).
\]
</p>

<p>This is a fixed-point iteration derived from the estimating equation.</p>

<h4>Practical note</h4>

<p>The classical algorithm may fail if <span>\(\mu\)</span> coincides with a data point. Modified variants guarantee monotone convergence.</p>

<hr>

<h3>8. Robustness properties</h3>

<h4>Breakdown point</h4>

<p>The spatial median has asymptotic breakdown point <span>\(1/2\)</span>. No location estimator can do better.</p>

<h4>Influence function</h4>

<p>
\[
\mathrm{IF}(x;T,F)
=
-\Gamma^{-1}S(x-\mu).
\]
</p>

<p>This influence function is <strong>bounded</strong>.</p>

<h4>Intuition</h4>

<p>Magnitude is ignored; only direction matters. One bad point cannot dominate.</p>

<hr>

<h3>9. Efficiency tradeoff</h3>

<p>If the covariance matrix <span>\(\Sigma\)</span> exists, the ARE relative to the mean is</p>

<p>
\[
\mathrm{ARE}
=
\frac{1}{p}
\frac{|\Sigma|}
{|\Gamma^{-1}\Omega\Gamma^{-1}|}.
\]
</p>

<h4>Meaning</h4>

<ul>
  <li>Some efficiency is lost under perfect Gaussianity</li>
  <li>Massive stability is gained under contamination</li>
</ul>

<p>This is not a defect‚Äîit is a deliberate design choice.</p>

</section>
</div>

<!-- INDEX_TOPIC -->

<div class="glow-box" id="4.2">
<section>

<h2>Estimation of the Covariance Matrix; Affine Equivariance; TR Spatial Median</h2>

<h3>1. Asymptotic covariance of the spatial median</h3>

<h4>Formal result recalled</h4>

<p>From earlier theory, the spatial median <span>\(\hat\mu\)</span> satisfies</p>

<p>
\[
\sqrt{n}(\hat\mu-\mu)
\xrightarrow{d}
N_p\!\left(0,\; \Gamma^{-1}\Omega\Gamma^{-1}\right),
\]
</p>

<p>where</p>

<p>
\[
\Gamma
=
\mathbb{E}\!\left[
\frac{1}{|X-\mu|}
\Bigl(
I_p - \frac{(X-\mu)(X-\mu)^\top}{|X-\mu|^2}
\Bigr)
\right],
\qquad
\Omega
=
\mathbb{E}\!\left[
\frac{(X-\mu)(X-\mu)^\top}{|X-\mu|^2}
\right].
\]
</p>

<p>Therefore, an <strong>approximate covariance matrix</strong> for <span>\(\hat\mu\)</span> is</p>

<p>
\[
\boxed{
\operatorname{Cov}(\hat\mu)
\approx
\frac{1}{n}\,\Gamma^{-1}\Omega\Gamma^{-1}.
}
\]
</p>

<hr>

<h3>2. Sample estimators <span>\(\hat\Gamma\)</span> and <span>\(\hat\Omega\)</span></h3>

<h4>Plug-in principle</h4>

<p>Since <span>\(\Gamma\)</span> and <span>\(\Omega\)</span> are expectations under <span>\(F_X\)</span>, we estimate them by empirical averages, replacing</p>

<ul>
  <li><span>\(\mu\)</span> with <span>\(\hat\mu\)</span></li>
  <li>expectations with sample means</li>
</ul>

<p>Thus,</p>

<p>
\[
\boxed{
\hat\Gamma
=
\frac{1}{n}
\sum_{i=1}^n
\frac{1}{|x_i-\hat\mu|}
\left[
I_p
-
\frac{(x_i-\hat\mu)(x_i-\hat\mu)^\top}{|x_i-\hat\mu|^2}
\right]
}
\]
</p>

<p>and</p>

<p>
\[
\boxed{
\hat\Omega
=
\frac{1}{n}
\sum_{i=1}^n
\frac{(x_i-\hat\mu)(x_i-\hat\mu)^\top}{|x_i-\hat\mu|^2}
}
\]
</p>

<p>The resulting covariance estimator is</p>

<p>
\[
\boxed{
\widehat{\operatorname{Cov}}(\hat\mu)
=
\frac{1}{n}\,\hat\Gamma^{-1}\hat\Omega\hat\Gamma^{-1}.
}
\]
</p>

<h4>Why these formulas are correct</h4>

<ul>
  <li>Each summand is bounded whenever <span>\(x_i\neq\hat\mu\)</span></li>
  <li>Expectations exist under the stated density assumptions</li>
  <li>The law of large numbers ensures consistency</li>
  <li>The estimator mirrors the asymptotic variance exactly</li>
</ul>

<p>This is direct M-estimation theory, not heuristic adjustment.</p>

<h4>Intuition: what <span>\(\Gamma\)</span> and <span>\(\Omega\)</span> mean</h4>

<ul>
  <li><span>\(\Omega\)</span>: <strong>directional scatter</strong> ‚Äî only directions matter, distances are normalized</li>
  <li><span>\(\Gamma\)</span>: <strong>local curvature</strong> ‚Äî resistance of the objective to movement</li>
</ul>

<p>Think of <span>\(\Omega\)</span> as random force and <span>\(\Gamma^{-1}\)</span> as mechanical compliance.</p>

<hr>

<h3>3. Why the spatial median is <em>not</em> affine equivariant</h3>

<h4>Formal statement</h4>

<p>The spatial median satisfies</p>

<p>
\[
T(F_{AX+b}) = A T(F_X) + b
\]
</p>

<p><strong>only if</strong> <span>\(A\)</span> is orthogonal, i.e.</p>

<p>
\[
A^\top A = I_p.
\]
</p>

<h4>Why this fails in general</h4>

<p>The spatial median minimizes</p>

<p>
\[
\mathbb{E}|X-t|,
\]
</p>

<p>which depends explicitly on the <strong>Euclidean norm</strong>.</p>

<p>If <span>\(A\)</span> is not orthogonal,</p>

<p>
\[
|AX| \neq |X|,
\]
</p>

<p>so the objective itself changes shape under general affine maps.</p>

<h4>Intuition</h4>

<p>The spatial median is</p>

<ul>
  <li>rotation invariant</li>
  <li>reflection invariant</li>
  <li>translation invariant</li>
</ul>

<p>but <strong>not</strong> scale- or shear-invariant. It respects angles, not full linear geometry.</p>

<hr>

<h3>4. Transformation‚ÄìRetransformation (TR) spatial median</h3>

<h4>Motivation</h4>

<p>Fix the geometry <em>before</em> computing the median.</p>

<hr>

<h3>5. Construction of the TR spatial median</h3>

<h4>Step 1: choose a scatter functional</h4>

<p>Let <span>\(S(F)\)</span> be a <strong>scatter functional</strong> (e.g. Tyler‚Äôs shape matrix). Define</p>

<p>
\[
G(F) = S(F)^{-1/2}.
\]
</p>

<p>This standardizes the data.</p>

<h4>Step 2: normalization property</h4>

<p>By construction,</p>

<p>
\[
\boxed{
G(F)\,S(F)\,G(F)^\top = I_p.
}
\]
</p>

<p>The transformed data are therefore spherical.</p>

<h4>Step 3: define the TR estimator</h4>

<p>
\[
\boxed{
T_{\mathrm{TR}}(F_X)
=
G(F_X)^{-1}
\,
T\!\left(F_{\,G(F_X)X}\right).
}
\]
</p>

<p>Compute the spatial median after standardization, then map it back.</p>

<hr>

<h3>6. Why TR restores affine equivariance</h3>

<h4>Formal logic</h4>

<ul>
  <li><span>\(S(F_{AX+b}) = A S(F_X) A^\top\)</span></li>
  <li><span>\(G(F_{AX+b}) = G(F_X)A^{-1}\)</span></li>
  <li>The standardized geometry is invariant</li>
  <li>Retransformation restores the affine structure</li>
</ul>

<p>Hence,</p>

<p>
\[
\boxed{
T_{\mathrm{TR}}(F_{AX+b}) = A T_{\mathrm{TR}}(F_X) + b.
}
\]
</p>

<h4>Intuition</h4>

<p>The spatial median fails because it trusts raw coordinates.</p>

<blockquote>
  <p>Coordinates are negotiable. Geometry is not.</p>
</blockquote>

<hr>

<h3>7. Relationship to known estimators</h3>

<p>The <strong>Hettmansperger‚ÄìRandles median</strong> combines</p>

<ul>
  <li>the spatial median (robust location)</li>
  <li>Tyler‚Äôs scatter (robust shape)</li>
  <li>TR geometry correction</li>
</ul>

<p>This is not ad hoc. It is the canonical way to unite robustness with affine invariance.</p>

</section>
</div>

<!-- INDEX_TOPIC -->

<div class="glow-box" id="5.1">
<section>

<h2>The Oja Median: a geometric notion of multivariate location</h2>

<h3>Restatement of the problem context</h3>

<p>We are in <strong>multivariate statistics</strong>, studying a notion of multivariate location called the <strong>Oja median</strong>. The construction is geometric: it measures how a candidate point <span>\(t \in \mathbb{R}^p\)</span> relates to the data cloud via volumes of simplices.</p>

<p>The goals are:</p>

<ul>
  <li>Define the <strong>sample Oja median</strong> via a minimization problem</li>
  <li>Define the corresponding <strong>population (functional) version</strong></li>
  <li>Introduce the <strong>multivariate sign and rank functions</strong> needed for asymptotic analysis</li>
</ul>

<p>The development proceeds from geometry to probability, step by step.</p>

<hr>

<h3>1. Data, distribution, and notation</h3>

<h4>Formal setup</h4>

<p>Let</p>

<p>
\[
X = (x_1, x_2, \dots, x_n)' \quad \text{with } x_i \in \mathbb{R}^p
\]
</p>

<p>be a random sample from a <span>\(p\)</span>-variate distribution with cumulative distribution function</p>

<p>
\[
F : \mathbb{R}^p \to [0,1].
\]
</p>

<p>Here <span>\(p\)</span> is the dimension and <span>\(n\)</span> the sample size.</p>

<h4>Intuition</h4>

<p>Think of <span>\(x_1,\dots,x_n\)</span> as points in <span>\(\mathbb{R}^p\)</span>. We seek a notion of ‚Äúcenter‚Äù that is robust and intrinsic to their geometry.</p>

<hr>

<h3>2. Volume of a <span>\(p\)</span>-simplex</h3>

<h4>Definition</h4>

<p>Given <span>\(p+1\)</span> points</p>

<p>
\[
t_1, t_2, \dots, t_{p+1} \in \mathbb{R}^p,
\]
</p>

<p>the volume of the simplex they determine is</p>

<p>
\[
V(t_1, \dots, t_{p+1})
=
\frac{1}{p!}
\left|
\det
\begin{pmatrix}
1 & \cdots & 1 \\
t_1 & \cdots & t_{p+1}
\end{pmatrix}
\right|.
\]
</p>

<h4>Why this formula is valid</h4>

<ul>
  <li>The determinant computes signed volume of a parallelepiped</li>
  <li>The row of ones converts points to an affine representation</li>
  <li>Division by <span>\(p!\)</span> converts parallelepiped volume to simplex volume</li>
  <li>The absolute value removes orientation</li>
</ul>

<h4>Low-dimensional intuition</h4>

<ul>
  <li><strong><span>\(p=1\)</span></strong>: two points define an interval, and
    <p>
    \[
    V(t_1,t_2)=|t_2-t_1|.
    \]
    </p>
  </li>
  <li><strong><span>\(p=2\)</span></strong>: three points form a triangle, and <span>\(V\)</span> is its area</li>
</ul>

<p>This confirms the formula generalizes length and area.</p>

<hr>

<h3>3. Sample Oja objective function</h3>

<h4>Definition</h4>

<p>For a candidate location <span>\(t \in \mathbb{R}^p\)</span>, define</p>

<p>
\[
D_n(t)
=
\binom{n}{p}^{-1}
\sum_{1 \le i_1 < \cdots < i_p \le n}
V(x_{i_1}, \dots, x_{i_p}, t).
\]
</p>

<h4>Explanation of components</h4>

<ul>
  <li>The sum runs over all <span>\(p\)</span>-subsets of the sample</li>
  <li>Each term is the volume of the simplex formed by those points and <span>\(t\)</span></li>
  <li>The binomial factor normalizes the sum to an average</li>
</ul>

<h4>Intuition</h4>

<p>If <span>\(t\)</span> is central, simplices formed with the data tend to have small volume on average.</p>

<hr>

<h3>4. Sample Oja median</h3>

<h4>Definition</h4>

<p>The <strong>Oja median</strong> <span>\(T(X)\)</span> is any minimizer of</p>

<p>
\[
D_n(t),
\quad\text{i.e.}\quad
T(X) \in \arg\min_{t \in \mathbb{R}^p} D_n(t).
\]
</p>

<h4>Intuition</h4>

<ul>
  <li>In one dimension, this reduces to the usual median</li>
  <li>In higher dimensions, it minimizes average simplex volume</li>
</ul>

<hr>

<h3>5. Population (functional) version</h3>

<h4>Definition</h4>

<p>Define the population objective</p>

<p>
\[
D(t) = E_F\big[V(X_1,\dots,X_p,t)\big],
\]
</p>

<p>where <span>\(X_1,\dots,X_p\)</span> are i.i.d. with distribution <span>\(F\)</span>.</p>

<p>The <strong>Oja functional</strong> <span>\(T(F)\)</span> is any minimizer of <span>\(D(t)\)</span>.</p>

<h4>Existence of expectations</h4>

<ul>
  <li><span>\(V(\cdot)\)</span> grows at most linearly</li>
  <li>Finite first moments of <span>\(F\)</span> suffice for finiteness</li>
</ul>

<hr>

<h3>6. Assumptions for asymptotic theory</h3>

<ul>
  <li><strong>Uniqueness</strong>: the minimizer <span>\(\mu=T(F)\)</span> is unique</li>
  <li><strong>Second moments</strong>: <span>\(E|X|^2&lt;\infty\)</span></li>
</ul>

<p>These guarantee differentiability and quadratic approximation of <span>\(D(t)\)</span>.</p>

<hr>

<h3>7. Quadratic expansion of <span>\(D(t)\)</span></h3>

<h4>Formal statement</h4>

<p>Near <span>\(\mu\)</span>,</p>

<p>
\[
D(t)
=
D(\mu)
+
\frac{1}{2}(t-\mu)' \Delta (t-\mu)
+
o(|t-\mu|^2),
\]
</p>

<p>where</p>

<p>
\[
\Delta
=
\left.
\frac{\partial^2}{\partial t\,\partial t'}
D(t)
\right|_{t=\mu}.
\]
</p>

<h4>Interpretation</h4>

<p><span>\(\Delta\)</span> is the Hessian of the objective; locally, <span>\(D(t)\)</span> behaves like a quadratic bowl.</p>

<hr>

<h3>8. Indexing subsets</h3>

<p>Define</p>

<p>
\[
Q = \{(i_1,\dots,i_{p-1}) : 1\le i_1&lt;\cdots&lt;i_{p-1}\le n\},
\quad
P = \{(i_1,\dots,i_p) : 1\le i_1&lt;\cdots&lt;i_p\le n\}.
\]
</p>

<p>These index simplices of different orders.</p>

<hr>

<h3>9. Determinant decompositions</h3>

<h4>Definition</h4>

<p>For <span>\(q\in Q\)</span>, define <span>\(e_q\in\mathbb{R}^p\)</span> by</p>

<p>
\[
\det(x_{i_1},\dots,x_{i_{p-1}},x)=e_q' x.
\]
</p>

<p>Similarly, for <span>\(p\in P\)</span>,</p>

<p>
\[
\det
\begin{pmatrix}
1 & \cdots & 1 & 1 \\
x_{i_1} & \cdots & x_{i_p} & x
\end{pmatrix}
=
d_{0p}+d_p' x.
\]
</p>

<h4>Interpretation</h4>

<p>Linearity of the determinant reduces volume calculations to linear forms.</p>

<hr>

<h3>10. Sample sign and rank functions</h3>

<h4>Definitions</h4>

<p>
\[
\hat S(t)
=
\binom{n}{p}^{-1}
\sum_{q\in Q}
\operatorname{sign}(e_q' t)\, e_q,
\]
</p>

<p>
\[
\hat R(t)
=
\binom{n}{p}^{-1}
\sum_{p\in P}
\operatorname{sign}(d_{0p}+d_p' t)\, d_p.
\]
</p>

<h4>Sign function</h4>

<p>
\[
\operatorname{sign}(u)=
\begin{cases}
+1,&u&gt;0,\\
0,&u=0,\\
-1,&u&lt;0.
\end{cases}
\]
</p>

<h4>Role</h4>

<p>These act as generalized gradients; the Oja median satisfies <span>\(\hat R(t)\approx0\)</span>.</p>

<hr>

<h3>11. Population versions</h3>

<p>
\[
S(t)=E[\operatorname{sign}(e_q' t)e_q],
\quad
R(t)=E[\operatorname{sign}(d_{0p}+d_p' t)d_p].
\]
</p>

<p>These are theoretical objects used in asymptotic analysis.</p>

<hr>

<h3>Big picture intuition</h3>

<p>The Oja median asks:</p>

<blockquote>
  <p>How small are the simplices formed when this point is included?</p>
</blockquote>

<p>The sign and rank functions translate geometry into algebra suitable for probability theory.</p>

<p>This is geometry wearing a statistics hat‚Äîand doing it properly.</p>

</section>
</div>

























</section>

<button class="next-btn" onclick="location.href='t2.html'">Next Section ‚Üí</button>

<footer>
  <p>&copy; 2026 Shaan | Built with purpose</p>
</footer>
<script>
MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]']]
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

</body>
</html>
