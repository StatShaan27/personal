<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Topic-1 | Shaan</title>
  <link rel="stylesheet" href="../style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display&family=Inter:wght@300;500&display=swap" rel="stylesheet">
  <style>
    body {
      background-color: #121212;
      color: #e0e0e0;
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
    }

    h1, h2, h3, h4 {
      font-family: 'Playfair Display', serif;
      color: #6effe0;
    }

    header {
      text-align: center;
      padding: 2rem 1rem 1rem;
      background: #1e1e1e;
      border-bottom: 2px solid #333;
    }

    header nav ul {
      list-style: none;
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 1rem;
      padding: 0;
    }

    header nav ul li {
      display: inline;
    }

    header nav ul li a {
      text-decoration: none;
      color: #6effe0;
      font-weight: 500;
      border: 1px solid transparent;
      padding: 0.4rem 0.8rem;
      border-radius: 6px;
      transition: all 0.3s ease;
    }

    header nav ul li a:hover {
      border-color: #6effe0;
      background-color: #6effe0;
      color: #000;
    }

    .notes-wrapper {
      max-width: 900px;
      margin: 3rem auto;
      padding: 2rem;
      animation: fadeIn 1.2s ease;
    }

    .info-block {
      margin-bottom: 2rem;
      text-align: left;
      line-height: 1.6;
    }

    .info-block strong {
      color: #6effe0;
    }

    .toc {
      background-color: #1e1e1e;
      border-left: 4px solid #6effe0;
      padding: 1rem 1.5rem;
      margin-bottom: 2rem;
      border-radius: 8px;
    }

    .toc ul {
      padding-left: 1rem;
    }

    .toc ul li {
      padding: 0.3rem 0;
    }

    .toc a {
      color: #c9d1d9;
      text-decoration: none;
      border-bottom: 1px dashed #6effe0;
    }

    .toc a:hover {
      color: #6effe0;
    }

    .glow-box {
      border: 2px solid #6effe0;
      box-shadow: 0 0 20px rgba(110, 255, 224, 0.2);
      border-radius: 16px;
      padding: 2rem;
      margin-bottom: 2rem;
      background-color: #1e1e1e;
      animation: pulse 3s infinite ease-in-out;
    }

    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 20px rgba(110, 255, 224, 0.2); }
      50% { box-shadow: 0 0 35px rgba(110, 255, 224, 0.4); }
    }

    ul {
      line-height: 1.6;
      padding-left: 1.2rem;
    }

    code, pre {
      background-color: #222;
      padding: 0.5rem 1rem;
      border-radius: 8px;
      font-family: monospace;
      color: #6effe0;
      display: block;
      overflow-x: auto;
    }

    .next-btn {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background-color: #6effe0;
      color: #000;
      padding: 0.6rem 1.2rem;
      border: none;
      border-radius: 8px;
      font-weight: bold;
      cursor: pointer;
      box-shadow: 0 0 10px #6effe0;
      transition: background-color 0.3s ease;
      z-index: 100;
    }

    .next-btn:hover {
      background-color: #00ffcc;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 2rem 1rem;
    }
  </style>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</head>
<body>

<header>
  <h1>Statistical Methods IV: Median</h1>
  <p>AY 2025–26</p>
  <nav>
    <ul>
      <li><a href="../../index.html">Home</a></li>
      <li><a href="../../assets/Shaan_CV.pdf">CV</a></li>
      <li><a href="../../sections/projects.html">Projects</a></li>
      <li><a href="../../sections/education.html">Education</a></li>
      <li><a href="../../sections/misc.html">Misc</a></li>
    </ul>
  </nav>
</header>

<section class="notes-wrapper">

  <div class="info-block">
    <p><strong>Instructor:</strong> Shyamal K De </p>
    <p><strong>Office / Department:</strong> ASU</p>
    <p><strong>Email:</strong> skd.isical@gmail.com</p>
    <p><strong>Marking Scheme:</strong><br>
      Class Test: 10% | Project: 15: | Midterm Test: 25% | End Semester: 50%
    </p>
  </div>

  <div class="toc">
    <h3>Contents</h3>
    <ul>
      <li><a href="#1.1"> Topic 1</a></li>
      <li><a href="#1.2"> ...</a></li>
      
      <!-- Add more as you progress -->
    </ul>
  </div>

  <!-- Notes section boxes -->
 <!-- INDEX_TOPIC -->

<div class="glow-box" id="2.1">
<section>

<h2>1. Computation: what is <em>the</em> sample median, really?</h2>

<p>Up to now, the sample median (<span>\(\hat\mu = T(F_n)\)</span>) was defined abstractly via several equivalent population characterizations. But when you plug in the <strong>empirical CDF</strong> (<span>\(F_n\)</span>):</p>

<ul>
  <li>Those definitions <strong>need not give a unique solution</strong></li>
  <li>So a <strong>convention</strong> is imposed</li>
</ul>

<h3>Procedure</h3>

<ol>
  <li>
    <p>Order the data:</p>
    <p>
      \[
      x_{(1)} \le x_{(2)} \le \cdots \le x_{(n)}
      \]
    </p>
  </li>
  <li>
    <p>Define:</p>
    <p>
      \[
      \hat\mu = \frac{x_{[(n+1)/2]} + x_{[(n+2)/2]}}{2}
      \]
    </p>
  </li>
</ol>

<h3>Interpretation</h3>

<ul>
  <li>If <span>\(n\)</span> is odd: both indices coincide → usual middle order statistic</li>
  <li>If <span>\(n\)</span> is even: average of the two central observations</li>
</ul>

<h3>Why this matters</h3>

<ul>
  <li>This choice ensures symmetry and equivariance</li>
  <li>It matches the estimating-equation view (<span>\(\hat R(\hat\mu)=0\)</span>)</li>
</ul>

<h3>Important remark</h3>

<blockquote>
  <p>In the multivariate case there is no natural ordering.</p>
</blockquote>

<p>This sentence is a warning shot: everything above relies on order. Once you lose order, the median becomes genuinely geometric.</p>

</section>
</div>

<div class="glow-box" id="2.2">
<section>

<h2>2. Robustness: why statisticians love the median</h2>

<p>Two core robustness concepts appear here.</p>

<h3>(a) Breakdown point = <span>\(\tfrac12\)</span></h3>

<p>The <strong>asymptotic breakdown point</strong> is the smallest fraction of contamination that can drive the estimator arbitrarily far.</p>

<p>For the median:</p>

<p>
\[
\varepsilon^* = \frac12
\]
</p>

<h4>Meaning</h4>

<ul>
  <li>You can replace almost half the data with arbitrarily bad outliers</li>
  <li>The median still does not explode</li>
</ul>

<p>No location estimator can do better. This is maximal robustness.</p>

<h3>(b) Influence function is bounded</h3>

<p>The influence function is:</p>

<p>
\[
\mathrm{IF}(x;T,F) = \delta^{-1} S(x - T(F)), \quad \delta = 2f(\mu)
\]
</p>

<h4>Key features</h4>

<ul>
  <li>Takes only three values: <span>\((\pm \delta^{-1}, 0)\)</span></li>
  <li>Does <strong>not grow</strong> as <span>\(|x|\to\infty\)</span></li>
</ul>

<h4>Interpretation</h4>

<ul>
  <li>A single extreme outlier has <em>limited effect</em></li>
  <li>Contrast with the mean, whose influence function is linear and unbounded</li>
</ul>

<p>This formally explains why the median resists outliers.</p>

</section>
</div>

<div class="glow-box" id="2.3">
<section>

<h2>3. Asymptotic efficiency: price of robustness</h2>

<p>Now the comparison with the sample mean begins.</p>

<h3>Assume</h3>

<ul>
  <li><span>\(F\)</span> has finite variance <span>\(\sigma^2\)</span></li>
</ul>

<p>Then the sample mean satisfies:</p>

<p>
\[
\sqrt{n}(\bar x - \mu) \;\to\; N(0,\sigma^2)
\]
</p>

<p>The median satisfies:</p>

<p>
\[
\sqrt{n}(\hat\mu - \mu) \;\to\; N\!\left(0,\frac{1}{4f(\mu)^2}\right)
\]
</p>

<h3>Asymptotic Relative Efficiency (ARE)</h3>

<p>Defined as:</p>

<p>
\[
\mathrm{ARE}(\text{median},\text{mean})
= \frac{\text{Var(mean)}}{\text{Var(median)}}
= 4 f(\mu)^2 \sigma^2
\]
</p>

<h4>Interpretation</h4>

<ul>
  <li>ARE &lt; 1 → median less efficient</li>
  <li>ARE &gt; 1 → median more efficient</li>
</ul>

<h4>Examples</h4>

<ul>
  <li>
    Normal <span>\(N(\mu,\sigma^2)\)</span>:
    <span>\(f(\mu)=\frac{1}{\sqrt{2\pi}\sigma}\)</span> → ARE ≈ <strong>0.64</strong>
  </li>
  <li>
    Heavy-tailed distributions:
    <ul>
      <li><span>\(t_3\)</span>: ARE ≈ <strong>1.62</strong></li>
      <li>Laplace: ARE = <strong>2</strong></li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Under Gaussian noise, the mean wins. Under heavy tails, the median dominates.</p>
</blockquote>

<p>This is the classic <strong>robustness–efficiency tradeoff</strong>.</p>

</section>
</div>

<div class="glow-box" id="2.4">
<section>

<h2>4. Estimating the variance: a practical headache</h2>

<p>To build confidence intervals for <span>\(\mu\)</span> using the asymptotic normality of <span>\(\hat\mu\)</span>, you need:</p>

<p>
\[
\delta = 2f(\mu)
\]
</p>

<h3>Problem</h3>

<ul>
  <li><span>\(f(\mu)\)</span> is unknown</li>
  <li>Density estimation at a point is unstable</li>
</ul>

<blockquote>
  <p>Estimation of <span>\(\delta\)</span> from the data is difficult.</p>
</blockquote>

</section>
</div>

<div class="glow-box" id="2.5">
<section>

<h2>5. Exact, distribution-free confidence intervals (the clever trick)</h2>

<p>Instead of estimating <span>\(\delta\)</span>, invert the <strong>sign test</strong>.</p>

<p>For a continuous <span>\(F\)</span>:</p>

<p>
\[
P(x_{(i)} &lt; \mu &lt; x_{(n+1-i)})
= P\!\left(
i \le \frac{n\hat R(\mu)+1}{2} \le n-i
\right)
\]
</p>

<p>Since:</p>

<p>
\[
\frac{n\hat R(\mu)+1}{2} \sim \mathrm{Bin}(n,\tfrac12)
\]
</p>

<p>We get:</p>

<p>
\[
P(x_{(i)} &lt; \mu &lt; x_{(n+1-i)})
= \sum_{j=i}^{n-i} \binom{n}{j}2^{-n}
\]
</p>

<h3>Interpretation</h3>

<ul>
  <li>Confidence interval = <strong>order-statistic interval</strong></li>
  <li>Coverage probability = exact binomial tail</li>
  <li>No density estimation</li>
  <li>Fully distribution-free</li>
</ul>

<p>This is one of the deepest practical advantages of the median.</p>

</section>
</div>

<div class="glow-box" id="2.6">
<section>

<h2>6. Equivariance: how the median behaves under transformations</h2>

<p>For a location functional <span>\(T\)</span>, we want:</p>

<p>
\[
T(F_{aX+b}) = aT(F_X) + b
\]
</p>

<h3>Meaning</h3>

<ul>
  <li>Shift data → estimator shifts</li>
  <li>Rescale data → estimator rescales</li>
</ul>

<p>The text states:</p>

<ul>
  <li>This holds for the median when <span>\(f\)</span> is smooth near <span>\(\mu\)</span></li>
  <li>In fact, something stronger is true</li>
</ul>

<p>If <span>\(g\)</span> is <strong>strictly monotone</strong>, then:</p>

<p>
\[
T(F_{g(X)}) = g(T(F_X))
\]
</p>

<h3>Interpretation</h3>

<ul>
  <li>Apply log, exp, power transforms</li>
  <li>Median transforms <em>exactly</em> as the data do</li>
</ul>

<p>The mean does <strong>not</strong> have this property.</p>

</section>
</div>

<!-- INDEX_TOPIC -->

<div class="glow-box" id="2.7">
<section>

<h2>Influence Function: why the median resists outliers</h2>

<h3>1. What is the influence function, conceptually?</h3>

<p>The <strong>influence function (IF)</strong> answers one precise question:</p>

<blockquote>
  <p>If I contaminate the distribution <span>\(F\)</span> by an infinitesimal amount of mass at the point <span>\(x\)</span>, how much does my estimator move?</p>
</blockquote>

<p>Formally:</p>

<p>
\[
\mathrm{IF}(x;T,F)
=
\left.
\frac{d}{d\varepsilon}
T\big((1-\varepsilon)F + \varepsilon \Delta_x\big)
\right|_{\varepsilon=0}
\]
</p>

<p>where:</p>

<ul>
  <li><span>\(T\)</span> = estimator viewed as a functional (here: the median)</li>
  <li><span>\(\Delta_x\)</span> = point mass at <span>\(x\)</span></li>
</ul>

<p>Think of the influence function as the <strong>first-order sensitivity</strong> of the estimator to a single outlier at <span>\(x\)</span>.</p>

<hr>

<h3>2. Why the sign function appears for the median</h3>

<p>The population median <span>\(\mu = T(F)\)</span> satisfies the estimating equation:</p>

<p>
\[
\mathbb{E}_F[S(X-\mu)] = 0
\]
</p>

<p>This equation <em>defines</em> the median.</p>

<p>Now contaminate the distribution slightly:</p>

<p>
\[
F_\varepsilon = (1-\varepsilon)F + \varepsilon \Delta_x
\]
</p>

<p>The new median <span>\(T(F_\varepsilon)\)</span> must still satisfy:</p>

<p>
\[
\mathbb{E}_{F_\varepsilon}
\big[S(X - T(F_\varepsilon))\big] = 0
\]
</p>

<p>Linearizing this equation around <span>\(\varepsilon=0\)</span> is standard <strong>M-estimator calculus</strong>. The structure of the estimating equation forces the <strong>sign function</strong> to appear.</p>

<hr>

<h3>3. Where the constant <span>\(\delta = 2f(\mu)\)</span> comes from</h3>

<p>Differentiate the estimating equation with respect to <span>\(t\)</span>:</p>

<p>
\[
\frac{d}{dt}\mathbb{E}[S(X-t)]\Big|_{t=\mu}
= -2f(\mu)
\]
</p>

<h4>Explanation</h4>

<ul>
  <li><span>\(S(X-t)\)</span> jumps at <span>\(X=t\)</span></li>
  <li>The derivative picks up mass from the density at <span>\(t=\mu\)</span></li>
  <li>The slope is exactly <span>\(-2f(\mu)\)</span></li>
</ul>

<p>Conclusion: the median reacts <strong>more strongly</strong> when the density at the median is small (a flat region).</p>

<hr>

<h3>4. The final influence function for the median</h3>

<p>Putting everything together:</p>

<p>
\[
\boxed{
\mathrm{IF}(x;T,F)
=
\frac{1}{2f(\mu)}\, S(x-\mu)
}
\]
</p>

<p>Explicitly:</p>

<p>
\[
\mathrm{IF}(x;T,F)
=
\begin{cases}
+\dfrac{1}{2f(\mu)} & x > \mu \\
0 & x = \mu \\
-\dfrac{1}{2f(\mu)} & x < \mu
\end{cases}
\]
</p>

<hr>

<h3>5. Why this is a <em>big deal</em></h3>

<h4>(a) The influence function is bounded</h4>

<p>No matter how large <span>\(x\)</span> is:</p>

<p>
\[
|\mathrm{IF}(x;T,F)| \le \frac{1}{2f(\mu)}
\]
</p>

<p>This is the <strong>formal reason</strong> the median is robust.</p>

<p>Compare with the mean:</p>

<p>
\[
\mathrm{IF}_{\text{mean}}(x) = x - \mu
\]
</p>

<p>which diverges as <span>\(|x|\to\infty\)</span>.</p>

<h4>(b) Interpretation in plain language</h4>

<ul>
  <li>An observation far above the median pushes it up by a <em>fixed amount</em></li>
  <li>An observation far below the median pushes it down by the <em>same fixed amount</em></li>
  <li>Extreme values are <strong>automatically clipped</strong></li>
</ul>

<p>The median does not care <em>how far</em> the outlier is — only <strong>which side</strong> it lies on.</p>

<hr>

<h3>6. Connection to breakdown point</h3>

<p>Because the influence function is bounded:</p>

<ul>
  <li>A single outlier cannot destroy the estimator</li>
  <li>You need <strong>half the data</strong> on one side to move the median arbitrarily</li>
</ul>

<p>This is why the breakdown point equals <strong>1/2</strong>.</p>

<hr>

<h3>7. Mental picture (very important)</h3>

<p>Visualize the median as a <strong>balance point</strong>:</p>

<ul>
  <li>Points to the right push right</li>
  <li>Points to the left push left</li>
  <li>Push strength is constant</li>
  <li>Only the <em>number</em> of points matters, not their magnitude</li>
</ul>

<p>This geometric intuition is exactly encoded by the <strong>sign function</strong> in the influence function.</p>

<hr>

<h3>8. Why <span>\(f(\mu)\)</span> matters</h3>

<ul>
  <li>If <span>\(f(\mu)\)</span> is small:
    <ul>
      <li>The distribution is flat near the median</li>
      <li>A small perturbation shifts the median a lot</li>
    </ul>
  </li>
  <li>If <span>\(f(\mu)\)</span> is large:
    <ul>
      <li>The median is well-anchored</li>
    </ul>
  </li>
</ul>

<p>This also explains the variance formula:</p>

<p>
\[
\operatorname{Var}(\hat\mu) \approx \frac{1}{4n f(\mu)^2}
\]
</p>

</section>
</div>



<!-- INDEX_TOPIC -->

<div class="glow-box" id="2.8">
<section>

<h2>Deriving the influence function by linearization</h2>

<h3>1. The equation we want to linearize</h3>

<p>The median is defined by the <strong>estimating equation</strong>:</p>

<p>
\[
\Psi(t, F) := \mathbb{E}_F\big[S(X - t)\big] = 0
\]
</p>

<p>The true median <span>\(\mu\)</span> satisfies:</p>

<p>
\[
\Psi(\mu, F) = 0.
\]
</p>

<p>Now contaminate the distribution:</p>

<p>
\[
F_\varepsilon = (1-\varepsilon)F + \varepsilon \Delta_x,
\]
</p>

<p>and let the corresponding median be:</p>

<p>
\[
t_\varepsilon := T(F_\varepsilon).
\]
</p>

<p>By definition, <span>\(t_\varepsilon\)</span> satisfies:</p>

<p>
\[
\Psi(t_\varepsilon, F_\varepsilon) = 0.
\]
</p>

<p>This is the equation we will <strong>linearize around</strong> <span>\((\mu,0)\)</span>.</p>

<hr>

<h3>2. What “linearize” means here</h3>

<p>“Linearize” means:</p>

<blockquote>
  <p>Take a first-order Taylor expansion of <span>\(\Psi(t_\varepsilon, F_\varepsilon)\)</span> around <span>\(t=\mu\)</span> and <span>\(\varepsilon=0\)</span>.</p>
</blockquote>

<p>Nothing more than first-order calculus.</p>

<hr>

<h3>3. Expansion with respect to both arguments</h3>

<p>Write:</p>

<p>
\[
0
=
\Psi(t_\varepsilon, F_\varepsilon)
\approx
\Psi(\mu, F)
+
\frac{\partial \Psi}{\partial t}\Big|_{\mu,F}
(t_\varepsilon-\mu)
+
\frac{\partial \Psi}{\partial \varepsilon}\Big|_{\mu,0}\varepsilon
\]
</p>

<p>Since <span>\(\Psi(\mu,F)=0\)</span>, this reduces to:</p>

<p>
\[
0
\approx
\frac{\partial \Psi}{\partial t}(\mu,F)(t_\varepsilon-\mu)
+
\frac{\partial \Psi}{\partial \varepsilon}(\mu,0)\varepsilon.
\]
</p>

<p>We now compute both derivatives explicitly.</p>

<hr>

<h3>4. Derivative with respect to <span>\(t\)</span></h3>

<p>Recall:</p>

<p>
\[
\Psi(t,F)=\mathbb{E}_F[S(X-t)].
\]
</p>

<p>As <span>\(t\)</span> increases, the sign function flips at <span>\(X=t\)</span>. The derivative comes from this jump:</p>

<p>
\[
\frac{\partial}{\partial t}\Psi(t,F)
=
-2f(t).
\]
</p>

<p>Evaluated at <span>\(t=\mu\)</span>:</p>

<p>
\[
\frac{\partial \Psi}{\partial t}(\mu,F)
=
-2f(\mu)
=
-\delta.
\]
</p>

<p>This is exactly where the constant <span>\(\delta=2f(\mu)\)</span> comes from.</p>

<hr>

<h3>5. Derivative with respect to <span>\(\varepsilon\)</span></h3>

<p>Using the contaminated distribution:</p>

<p>
\[
\Psi(t,F_\varepsilon)
=
(1-\varepsilon)\mathbb{E}_F[S(X-t)]
+
\varepsilon S(x-t).
\]
</p>

<p>Differentiate with respect to <span>\(\varepsilon\)</span>:</p>

<p>
\[
\frac{\partial \Psi}{\partial \varepsilon}
=
-\mathbb{E}_F[S(X-t)] + S(x-t).
\]
</p>

<p>At <span>\(t=\mu\)</span>, since <span>\(\mathbb{E}_F[S(X-\mu)]=0\)</span>:</p>

<p>
\[
\frac{\partial \Psi}{\partial \varepsilon}(\mu,0)
=
S(x-\mu).
\]
</p>

<hr>

<h3>6. Putting the pieces together</h3>

<p>Insert both derivatives into the linearized equation:</p>

<p>
\[
0
\approx
(-2f(\mu))(t_\varepsilon-\mu)
+
\varepsilon S(x-\mu).
\]
</p>

<p>Solving for <span>\(t_\varepsilon-\mu\)</span>:</p>

<p>
\[
t_\varepsilon-\mu
\approx
\frac{\varepsilon}{2f(\mu)}S(x-\mu).
\]
</p>

<hr>

<h3>7. Definition of the influence function</h3>

<p>By definition:</p>

<p>
\[
\mathrm{IF}(x;T,F)
=
\left.
\frac{d}{d\varepsilon}t_\varepsilon
\right|_{\varepsilon=0}.
\]
</p>

<p>From the expansion above:</p>

<p>
\[
\boxed{
\mathrm{IF}(x;T,F)
=
\frac{1}{2f(\mu)}S(x-\mu)
}
\]
</p>

<hr>

<h3>8. What “standard M-estimator calculus” really means</h3>

<p>Whenever an estimator <span>\(T(F)\)</span> is defined by:</p>

<p>
\[
\mathbb{E}_F[\psi(X,T(F))]=0,
\]
</p>

<p>the influence function is:</p>

<p>
\[
\mathrm{IF}(x)
=
\left(
\mathbb{E}\Big[\frac{\partial}{\partial t}\psi(X,t)\Big|_{t=\mu}\Big]
\right)^{-1}
\psi(x,\mu).
\]
</p>

<p>For the median:</p>

<ul>
  <li><span>\(\psi(x,t)=S(x-t)\)</span></li>
  <li>The derivative equals <span>\(-2f(\mu)\)</span></li>
</ul>

<p>Everything above is a concrete instance of this general rule.</p>

<hr>

<h3>9. Intuition check</h3>

<blockquote>
  <p>A tiny contamination moves the median proportionally to the sign of the contaminating point, scaled by how steep the CDF is at the median.</p>
</blockquote>

<p>No higher-order effects matter at first order. That is the entire point of linearization.</p>

</section>
</div>























</section>

<button class="next-btn" onclick="location.href='t2.html'">Next Section →</button>

<footer>
  <p>&copy; 2025 Shaan | Built with purpose</p>
</footer>
<script>
MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]']]
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

</body>
</html>
