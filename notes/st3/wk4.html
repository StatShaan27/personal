<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Week-4 | Shaan</title>
  <link rel="stylesheet" href="../style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display&family=Inter:wght@300;500&display=swap" rel="stylesheet">
  <style>
    body {
      background-color: #121212;
      color: #e0e0e0;
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
    }

    h1, h2, h3, h4 {
      font-family: 'Playfair Display', serif;
      color: #6effe0;
    }

    header {
      text-align: center;
      padding: 2rem 1rem 1rem;
      background: #1e1e1e;
      border-bottom: 2px solid #333;
    }

    header nav ul {
      list-style: none;
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 1rem;
      padding: 0;
    }

    header nav ul li {
      display: inline;
    }

    header nav ul li a {
      text-decoration: none;
      color: #6effe0;
      font-weight: 500;
      border: 1px solid transparent;
      padding: 0.4rem 0.8rem;
      border-radius: 6px;
      transition: all 0.3s ease;
    }

    header nav ul li a:hover {
      border-color: #6effe0;
      background-color: #6effe0;
      color: #000;
    }

    .notes-wrapper {
      max-width: 900px;
      margin: 3rem auto;
      padding: 2rem;
      animation: fadeIn 1.2s ease;
    }

    .info-block {
      margin-bottom: 2rem;
      text-align: left;
      line-height: 1.6;
    }

    .info-block strong {
      color: #6effe0;
    }

    .toc {
      background-color: #1e1e1e;
      border-left: 4px solid #6effe0;
      padding: 1rem 1.5rem;
      margin-bottom: 2rem;
      border-radius: 8px;
    }

    .toc ul {
      padding-left: 1rem;
    }

    .toc ul li {
      padding: 0.3rem 0;
    }

    .toc a {
      color: #c9d1d9;
      text-decoration: none;
      border-bottom: 1px dashed #6effe0;
    }

    .toc a:hover {
      color: #6effe0;
    }

    .glow-box {
      border: 2px solid #6effe0;
      box-shadow: 0 0 20px rgba(110, 255, 224, 0.2);
      border-radius: 16px;
      padding: 2rem;
      margin-bottom: 2rem;
      background-color: #1e1e1e;
      animation: pulse 3s infinite ease-in-out;
    }

    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 20px rgba(110, 255, 224, 0.2); }
      50% { box-shadow: 0 0 35px rgba(110, 255, 224, 0.4); }
    }

    ul {
      line-height: 1.6;
      padding-left: 1.2rem;
    }

    code, pre {
      background-color: #222;
      padding: 0.5rem 1rem;
      border-radius: 8px;
      font-family: monospace;
      color: #6effe0;
      display: block;
      overflow-x: auto;
    }

    .next-btn {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background-color: #6effe0;
      color: #000;
      padding: 0.6rem 1.2rem;
      border: none;
      border-radius: 8px;
      font-weight: bold;
      cursor: pointer;
      box-shadow: 0 0 10px #6effe0;
      transition: background-color 0.3s ease;
      z-index: 100;
    }


    .prev-btn {
    position: fixed;
    bottom: 20px;
    left: 20px;
    background-color: #6effe0;
    color: #000;
    border: none;
    padding: 0.6rem 1.2rem;
    border-radius: 8px;
    font-weight: bold
    cursor: pointer;
    box-shadow: 0 0 10px #6effe0;
    transition: background-color 0.3s ease;
    z-index: 100;
    }

    .next-btn:hover {
      background-color: #00ffcc;
    }

    .prev-btn:hover {
      background-color: #00ffcc;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 2rem 1rem;
    }
  </style>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</head>
<body>

<header>
  <h1>Statistical Methods III: Week-4</h1>
  <p>AY 2025–26</p>
  <nav>
    <ul>
      <li><a href="../../index.html">Home</a></li>
      <li><a href="../../assets/Shaan_CV.pdf">CV</a></li>
      <li><a href="../../sections/projects.html">Projects</a></li>
      <li><a href="../../sections/education.html">Education</a></li>
      <li><a href="../../sections/misc.html">Misc</a></li>
    </ul>
  </nav>
</header>

<section class="notes-wrapper">

  <div class="info-block">
    <p><strong>Instructor:</strong> Debasis Sengupta </p>
    <p><strong>Office / Department:</strong> ASU</p>
    <p><strong>Email:</strong> sdebasis@isical.ac.in</p>
    <p><strong>Marking Scheme:</strong><br>
      Assignments: 20% | Midterm Test: 30% | End Semester: 50%
    </p>
  </div>

  <div class="toc">
    <h3>Contents</h3>
    <ul>
      <li><a href="#4.1"> General EM Framework</a></li>
      <li><a href="#3.2"> Numerical Optimization Algorithms</a></li>
      <li><a href="#3.3"> Expectation–Maximization with Incomplete Data</a></li>
      
      
    </ul>
  </div>

  <!-- Notes section boxes -->
  <div class="glow-box" id="4.1">
    <section>
  <h2>General EM Framework</h2>
  <ul>
    <li>Introduce a “fantasy complete dataset” \(\vec{x}\) so optimization is easier if we had it.</li>
    <li>Observed data: \(\vec{y}\).</li>
    <li>Complete data: \((\vec{y},\vec{x})\).</li>
    <li>Parameters: \(\vec{\theta}\).</li>
    <li>Target: maximize observed log-likelihood \(L(\vec{\theta}\mid\vec{y})=\ln g(\vec{y}\mid\vec{\theta})\), which is hard directly.</li>
    <li>Strategy:
      <ul>
        <li>Express the problem via the complete-data log-likelihood.</li>
        <li>Take expectation over missing parts given current parameter guess.</li>
        <li>Alternate between estimating missing parts (E-step) and maximizing w.r.t. parameters (M-step).</li>
      </ul>
    </li>
  </ul>

  <h2>E-step and M-step (Formal)</h2>
  <ul>
    <li><strong>E-step:</strong> compute the expected complete-data log-likelihood under the current estimate \(\hat\theta^{(n)}\):
      \[
      Q(\theta\mid\hat\theta^{(n)})= \mathbb{E}_{\vec{x}\mid\vec{y},\hat\theta^{(n)}}\big[L(\theta\mid\vec{y},\vec{x})\big].
      \]
    </li>
    <li><strong>M-step:</strong> update by maximizing that expectation:
      \[
      \hat\theta^{(n+1)}=\arg\max_{\theta} Q(\theta\mid\hat\theta^{(n)}).
      \]
    </li>
    <li>Interpretation: E-step fills missing information probabilistically; M-step treats that fill-in as data for optimization.</li>
  </ul>

  <h2>Proof of Monotone Convergence (Sketch)</h2>
  <ul>
    <li><strong>Claim:</strong> \(L(\hat\theta^{(n+1)}\mid\vec y)\ge L(\hat\theta^{(n)}\mid\vec y)\) — EM never decreases the observed log-likelihood.</li>

```
<li><strong>Step 1 — Decomposition:</strong>
  <ul>
    <li>For any \(\theta\):
      \[
      L(\theta\mid\vec y)=\mathbb{E}_{\vec x\mid\vec y,\hat\theta^{(n)}}\big[L(\theta\mid\vec y,\vec x)\big]
      -\mathbb{E}_{\vec x\mid\vec y,\hat\theta^{(n)}}\big[\ln h(\vec x\mid\vec y,\theta)\big].
      \]
    </li>
    <li>First term = expected complete-data log-likelihood; second term = penalty involving the conditional distribution of missing data.</li>
  </ul>
</li>

<li><strong>Step 2 — M-step increases the first term:</strong>
  <ul>
    <li>\(\hat\theta^{(n+1)}\) is chosen to maximize the expectation, hence
      \[
      \mathbb{E}\big[L(\hat\theta^{(n+1)}\mid\vec y,\vec x)\big]\ge
      \mathbb{E}\big[L(\hat\theta^{(n)}\mid\vec y,\vec x)\big].
      \]
    </li>
  </ul>
</li>

<li><strong>Step 3 — Penalty term does not increase:</strong>
  <ul>
    <li>Need
      \[
      \mathbb{E}\big[\ln h(\vec x\mid\vec y,\hat\theta^{(n+1)})\big]\le
      \mathbb{E}\big[\ln h(\vec x\mid\vec y,\hat\theta^{(n)})\big].
      \]
    </li>
    <li>Follows from concavity of \(\ln\) and KL-divergence arguments: the expected log-ratio induces a non-negative KL term, preventing the penalty from offsetting the gain.</li>
  </ul>
</li>

<li><strong>Step 4 — Combine:</strong>
  <ul>
    <li>Subtract penalty terms from expected complete-data logs to get
      \[
      L(\hat\theta^{(n+1)}\mid\vec y)-L(\hat\theta^{(n)}\mid\vec y)\ge0,
      \]
      proving ascent.</li>
  </ul>
</li>
```

  </ul>

  <h2>Why This Works — Intuition</h2>
  <ul>
    <li>EM turns a hard marginal optimization into repeated easier optimizations of an expected complete-data objective.</li>
    <li>In exponential families, the E-step reduces to replacing missing sufficient statistics by their conditional expectations.</li>
    <li>Each M-step optimizes a surrogate that lower-bounds (or tangentially touches) the observed log-likelihood at the current iterate, guaranteeing non-decrease.</li>
  </ul>

  <h2>Visual Intuition</h2>
  <ul>
    <li>Likelihood surface = fogged mountain; complete-data likelihood = clear map; observed-data likelihood = fogged projection.</li>
    <li>E-step: clear the fog locally by estimating missing pieces; M-step: climb uphill on the clearer map.</li>
    <li>Iteration trace: monotone ascent of \(L(\hat\theta^{(n)}\mid\vec y)\) vs iteration \(n\), flattening near convergence.</li>
    <li>Suggested sketch: EM schematic (E → M → E …) + plot of likelihood vs iterations showing steady ascent and plateau; optional image: EM intuition diagram (illustrative).</li>
  </ul>

  <h2>Caveats & Practical Notes</h2>
  <ul>
    <li><strong>Local maxima:</strong> EM may converge to a local peak; initialization matters.</li>
    <li><strong>Slow near optimum:</strong> EM generally has linear convergence; Newton/Fisher can be quadratically faster but less stable.</li>
    <li><strong>Missingness mechanism:</strong> EM assumes ignorable missingness (MCAR/MAR); otherwise model the missingness process explicitly.</li>
    <li><strong>Diagnostics:</strong> monitor observed log-likelihood, relative parameter changes, and run multiple starts to check for global optimum.</li>
    <li><strong>Extensions:</strong> stochastic EM, ECM, AECM and other variants can speed up or stabilize convergence in complex models.</li>
  </ul>

  <h2>Summary / Insight</h2>
  <p>EM alternates expectation (probabilistic imputation of missing data) and maximization (optimization of a tractable surrogate). The ascent property follows from decomposing the observed log-likelihood into expected complete-data terms minus a penalty; M-step raises the expected term while Jensen/KL ensures the penalty cannot overturn the gain. EM is robust and stable, but can be slow and sensitive to starting values—so use sensible initialization and diagnostics.</p>
</section>

  </div>












    <div class="glow-box" id="3.2">
      
    </div>
















    

  









<div class="glow-box" id="3.3">
  
    </div>



























<div class="glow-box" id="3.0">
  
    </div>

  





































  <div class="glow-box" id="3.0">
  
    </div>














































  <div class="glow-box" id="3.0">
  
    </div>





















































  
</section>
<button class="prev-btn" onclick="location.href='wk3.html'"> ← Previous Section </button>

<button class="next-btn" onclick="location.href='wk5.html'">Next Section →</button>

<footer>
  <p>&copy; 2025 Shaan | Built with purpose</p>
</footer>
<script>
MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]']]
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

</body>
</html>
