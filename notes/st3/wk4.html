<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Week-4 | Shaan</title>
  <link rel="stylesheet" href="../style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display&family=Inter:wght@300;500&display=swap" rel="stylesheet">
  <style>
    body {
      background-color: #121212;
      color: #e0e0e0;
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
    }

    h1, h2, h3, h4 {
      font-family: 'Playfair Display', serif;
      color: #6effe0;
    }

    header {
      text-align: center;
      padding: 2rem 1rem 1rem;
      background: #1e1e1e;
      border-bottom: 2px solid #333;
    }

    header nav ul {
      list-style: none;
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 1rem;
      padding: 0;
    }

    header nav ul li {
      display: inline;
    }

    header nav ul li a {
      text-decoration: none;
      color: #6effe0;
      font-weight: 500;
      border: 1px solid transparent;
      padding: 0.4rem 0.8rem;
      border-radius: 6px;
      transition: all 0.3s ease;
    }

    header nav ul li a:hover {
      border-color: #6effe0;
      background-color: #6effe0;
      color: #000;
    }

    .notes-wrapper {
      max-width: 900px;
      margin: 3rem auto;
      padding: 2rem;
      animation: fadeIn 1.2s ease;
    }

    .info-block {
      margin-bottom: 2rem;
      text-align: left;
      line-height: 1.6;
    }

    .info-block strong {
      color: #6effe0;
    }

    .toc {
      background-color: #1e1e1e;
      border-left: 4px solid #6effe0;
      padding: 1rem 1.5rem;
      margin-bottom: 2rem;
      border-radius: 8px;
    }

    .toc ul {
      padding-left: 1rem;
    }

    .toc ul li {
      padding: 0.3rem 0;
    }

    .toc a {
      color: #c9d1d9;
      text-decoration: none;
      border-bottom: 1px dashed #6effe0;
    }

    .toc a:hover {
      color: #6effe0;
    }

    .glow-box {
      border: 2px solid #6effe0;
      box-shadow: 0 0 20px rgba(110, 255, 224, 0.2);
      border-radius: 16px;
      padding: 2rem;
      margin-bottom: 2rem;
      background-color: #1e1e1e;
      animation: pulse 3s infinite ease-in-out;
    }

    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 20px rgba(110, 255, 224, 0.2); }
      50% { box-shadow: 0 0 35px rgba(110, 255, 224, 0.4); }
    }

    ul {
      line-height: 1.6;
      padding-left: 1.2rem;
    }

    code, pre {
      background-color: #222;
      padding: 0.5rem 1rem;
      border-radius: 8px;
      font-family: monospace;
      color: #6effe0;
      display: block;
      overflow-x: auto;
    }

    .next-btn {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background-color: #6effe0;
      color: #000;
      padding: 0.6rem 1.2rem;
      border: none;
      border-radius: 8px;
      font-weight: bold;
      cursor: pointer;
      box-shadow: 0 0 10px #6effe0;
      transition: background-color 0.3s ease;
      z-index: 100;
    }


    .prev-btn {
    position: fixed;
    bottom: 20px;
    left: 20px;
    background-color: #6effe0;
    color: #000;
    border: none;
    padding: 0.6rem 1.2rem;
    border-radius: 8px;
    font-weight: bold
    cursor: pointer;
    box-shadow: 0 0 10px #6effe0;
    transition: background-color 0.3s ease;
    z-index: 100;
    }

    .next-btn:hover {
      background-color: #00ffcc;
    }

    .prev-btn:hover {
      background-color: #00ffcc;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 2rem 1rem;
    }
  </style>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</head>
<body>

<header>
  <h1>Statistical Methods III: Week-4</h1>
  <p>AY 2025–26</p>
  <nav>
    <ul>
      <li><a href="../../index.html">Home</a></li>
      <li><a href="../../assets/Shaan_CV.pdf">CV</a></li>
      <li><a href="../../sections/projects.html">Projects</a></li>
      <li><a href="../../sections/education.html">Education</a></li>
      <li><a href="../../sections/misc.html">Misc</a></li>
    </ul>
  </nav>
</header>

<section class="notes-wrapper">

  <div class="info-block">
    <p><strong>Instructor:</strong> Debasis Sengupta </p>
    <p><strong>Office / Department:</strong> ASU</p>
    <p><strong>Email:</strong> sdebasis@isical.ac.in</p>
    <p><strong>Marking Scheme:</strong><br>
      Assignments: 20% | Midterm Test: 30% | End Semester: 50%
    </p>
  </div>

  <div class="toc">
    <h3>Contents</h3>
    <ul>
      <li><a href="#4.1"> General EM Framework</a></li>
      <li><a href="#4.2"> Genetics Example of EM (Dempster–Laird–Rubin, 1977) </a></li>
      <li><a href="#4.3"> Report on the EM Algorithm in Genetics, Genomics, and Public Health </a></li>
      
      
    </ul>
  </div>

  <!-- Notes section boxes -->
  <div class="glow-box" id="4.1">
    <section>
  <h2>General EM Framework</h2>
  <ul>
    <li>Introduce a “fantasy complete dataset” \(\vec{x}\) so optimization is easier if we had it.</li>
    <li>Observed data: \(\vec{y}\).</li>
    <li>Complete data: \((\vec{y},\vec{x})\).</li>
    <li>Parameters: \(\vec{\theta}\).</li>
    <li>Target: maximize observed log-likelihood \(L(\vec{\theta}\mid\vec{y})=\ln g(\vec{y}\mid\vec{\theta})\), which is hard directly.</li>
    <li>Strategy:
      <ul>
        <li>Express the problem via the complete-data log-likelihood.</li>
        <li>Take expectation over missing parts given current parameter guess.</li>
        <li>Alternate between estimating missing parts (E-step) and maximizing w.r.t. parameters (M-step).</li>
      </ul>
    </li>
  </ul>

  <h2>E-step and M-step (Formal)</h2>
  <ul>
    <li><strong>E-step:</strong> compute the expected complete-data log-likelihood under the current estimate \(\hat\theta^{(n)}\):
      \[
      Q(\theta\mid\hat\theta^{(n)})= \mathbb{E}_{\vec{x}\mid\vec{y},\hat\theta^{(n)}}\big[L(\theta\mid\vec{y},\vec{x})\big].
      \]
    </li>
    <li><strong>M-step:</strong> update by maximizing that expectation:
      \[
      \hat\theta^{(n+1)}=\arg\max_{\theta} Q(\theta\mid\hat\theta^{(n)}).
      \]
    </li>
    <li>Interpretation: E-step fills missing information probabilistically; M-step treats that fill-in as data for optimization.</li>
  </ul>

  <h2>Proof of Monotone Convergence (Sketch)</h2>
  <ul>
    <li><strong>Claim:</strong> \(L(\hat\theta^{(n+1)}\mid\vec y)\ge L(\hat\theta^{(n)}\mid\vec y)\) — EM never decreases the observed log-likelihood.</li>

```
<li><strong>Step 1 — Decomposition:</strong>
  <ul>
    <li>For any \(\theta\):
      \[
      L(\theta\mid\vec y)=\mathbb{E}_{\vec x\mid\vec y,\hat\theta^{(n)}}\big[L(\theta\mid\vec y,\vec x)\big]
      -\mathbb{E}_{\vec x\mid\vec y,\hat\theta^{(n)}}\big[\ln h(\vec x\mid\vec y,\theta)\big].
      \]
    </li>
    <li>First term = expected complete-data log-likelihood; second term = penalty involving the conditional distribution of missing data.</li>
  </ul>
</li>

<li><strong>Step 2 — M-step increases the first term:</strong>
  <ul>
    <li>\(\hat\theta^{(n+1)}\) is chosen to maximize the expectation, hence
      \[
      \mathbb{E}\big[L(\hat\theta^{(n+1)}\mid\vec y,\vec x)\big]\ge
      \mathbb{E}\big[L(\hat\theta^{(n)}\mid\vec y,\vec x)\big].
      \]
    </li>
  </ul>
</li>

<li><strong>Step 3 — Penalty term does not increase:</strong>
  <ul>
    <li>Need
      \[
      \mathbb{E}\big[\ln h(\vec x\mid\vec y,\hat\theta^{(n+1)})\big]\le
      \mathbb{E}\big[\ln h(\vec x\mid\vec y,\hat\theta^{(n)})\big].
      \]
    </li>
    <li>Follows from concavity of \(\ln\) and KL-divergence arguments: the expected log-ratio induces a non-negative KL term, preventing the penalty from offsetting the gain.</li>
  </ul>
</li>

<li><strong>Step 4 — Combine:</strong>
  <ul>
    <li>Subtract penalty terms from expected complete-data logs to get
      \[
      L(\hat\theta^{(n+1)}\mid\vec y)-L(\hat\theta^{(n)}\mid\vec y)\ge0,
      \]
      proving ascent.</li>
  </ul>
</li>
```

  </ul>

  <h2>Why This Works — Intuition</h2>
  <ul>
    <li>EM turns a hard marginal optimization into repeated easier optimizations of an expected complete-data objective.</li>
    <li>In exponential families, the E-step reduces to replacing missing sufficient statistics by their conditional expectations.</li>
    <li>Each M-step optimizes a surrogate that lower-bounds (or tangentially touches) the observed log-likelihood at the current iterate, guaranteeing non-decrease.</li>
  </ul>

  <h2>Visual Intuition</h2>
  <ul>
    <li>Likelihood surface = fogged mountain; complete-data likelihood = clear map; observed-data likelihood = fogged projection.</li>
    <li>E-step: clear the fog locally by estimating missing pieces; M-step: climb uphill on the clearer map.</li>
    <li>Iteration trace: monotone ascent of \(L(\hat\theta^{(n)}\mid\vec y)\) vs iteration \(n\), flattening near convergence.</li>
    <li>Suggested sketch: EM schematic (E → M → E …) + plot of likelihood vs iterations showing steady ascent and plateau; optional image: EM intuition diagram (illustrative).</li>
  </ul>

  <h2>Caveats & Practical Notes</h2>
  <ul>
    <li><strong>Local maxima:</strong> EM may converge to a local peak; initialization matters.</li>
    <li><strong>Slow near optimum:</strong> EM generally has linear convergence; Newton/Fisher can be quadratically faster but less stable.</li>
    <li><strong>Missingness mechanism:</strong> EM assumes ignorable missingness (MCAR/MAR); otherwise model the missingness process explicitly.</li>
    <li><strong>Diagnostics:</strong> monitor observed log-likelihood, relative parameter changes, and run multiple starts to check for global optimum.</li>
    <li><strong>Extensions:</strong> stochastic EM, ECM, AECM and other variants can speed up or stabilize convergence in complex models.</li>
  </ul>

  <h2>Summary / Insight</h2>
  <p>EM alternates expectation (probabilistic imputation of missing data) and maximization (optimization of a tractable surrogate). The ascent property follows from decomposing the observed log-likelihood into expected complete-data terms minus a penalty; M-step raises the expected term while Jensen/KL ensures the penalty cannot overturn the gain. EM is robust and stable, but can be slow and sensitive to starting values—so use sensible initialization and diagnostics.</p>
</section>

  </div>












    <div class="glow-box" id="4.2">
      <section>
  <h2>Genetics Example of EM (Dempster–Laird–Rubin, 1977)</h2>
  <p>We start with observed data counts. The EM trick is to imagine hidden complete data, so the optimization becomes much simpler.</p>

  <h3>Observed Data Setup</h3>
  <p>We have 197 animals split into 4 categories:</p>
  <table border="1" cellpadding="5" cellspacing="0">
    <tr>
      <th>Group</th>
      <th>Count (y<sub>i</sub>)</th>
      <th>Probability</th>
    </tr>
    <tr>
      <td>y<sub>1</sub></td>
      <td>125</td>
      <td>1/2 + π/4</td>
    </tr>
    <tr>
      <td>y<sub>2</sub></td>
      <td>18</td>
      <td>(1-π)/4</td>
    </tr>
    <tr>
      <td>y<sub>3</sub></td>
      <td>20</td>
      <td>(1-π)/4</td>
    </tr>
    <tr>
      <td>y<sub>4</sub></td>
      <td>34</td>
      <td>π/4</td>
    </tr>
  </table>
  <p>So the multinomial probabilities depend on one parameter π. But the first probability y<sub>1</sub> is a sum of two probabilities: 1/2 and π/4. That’s the clue for EM.</p>

  <h3>Complete Data Idea</h3>
  <ul>
    <li>Pretend that y<sub>1</sub> was really two hidden groups:
      <ul>
        <li>x<sub>1</sub> animals came from prob 1/2</li>
        <li>x<sub>2</sub> animals came from prob π/4</li>
      </ul>
    </li>
    <li>So: x<sub>1</sub>+x<sub>2</sub>=y<sub>1</sub>, x<sub>3</sub>=y<sub>2</sub>, x<sub>4</sub>=y<sub>3</sub>, x<sub>5</sub>=y<sub>4</sub>.</li>
  </ul>

  <table border="1" cellpadding="5" cellspacing="0">
    <tr>
      <th>Complete Data</th>
      <th>Probabilities</th>
    </tr>
    <tr>
      <td>x<sub>1</sub></td>
      <td>1/2</td>
    </tr>
    <tr>
      <td>x<sub>2</sub></td>
      <td>π/4</td>
    </tr>
    <tr>
      <td>x<sub>3</sub></td>
      <td>(1-π)/4</td>
    </tr>
    <tr>
      <td>x<sub>4</sub></td>
      <td>(1-π)/4</td>
    </tr>
    <tr>
      <td>x<sub>5</sub></td>
      <td>π/4</td>
    </tr>
  </table>

  <h3>Complete-Data Log-Likelihood</h3>
  <p>
    L(π) = x<sub>1</sub> ln(1/2) + x<sub>2</sub> ln(π/4) + (x<sub>3</sub>+x<sub>4</sub>) ln((1-π)/4) + x<sub>5</sub> ln(π/4)
  </p>
  <p>This is simple to maximize if we had x<sub>2</sub>. But we don’t. That’s where the E-step comes in.</p>

  <h3>E-Step</h3>
  <ul>
    <li>We compute the expected value of x<sub>2</sub>, given y<sub>1</sub> and current π<sup>(n)</sup>.</li>
    <li>x<sub>2</sub> | y<sub>1</sub> ~ Binomial(y<sub>1</sub>, (π/4)/(1/2 + π/4))</li>
    <li>So expectation:
      <p>x<sub>2</sub><sup>(n)</sup> = y<sub>1</sub> · (π<sup>(n)</sup>/4) / (1/2 + π<sup>(n)</sup>/4)</p>
    </li>
  </ul>

  <h3>M-Step</h3>
  <ul>
    <li>Maximize expected log-likelihood. Differentiate, set to zero, solve for π:</li>
  </ul>
  <p>
    π<sup>(n+1)</sup> = (x<sub>2</sub><sup>(n)</sup> + x<sub>5</sub>) / (x<sub>2</sub><sup>(n)</sup> + x<sub>3</sub> + x<sub>4</sub> + x<sub>5</sub>)
  </p>
  <p>Substitute observed counts:</p>
  <p>
    π<sup>(n+1)</sup> = (x<sub>2</sub><sup>(n)</sup> + y<sub>4</sub>) / (x<sub>2</sub><sup>(n)</sup> + y<sub>2</sub> + y<sub>3</sub> + y<sub>4</sub>)
  </p>
  <p>And iterate until convergence.</p>

  <h2>Side Concepts</h2>

  <h3>Conditional Multinomial Property</h3>
  <p>If you lump two multinomial cells, then condition on their sum, one component is Binomial. This is why x<sub>2</sub>|(x<sub>1</sub>+x<sub>2</sub>) became Binomial.</p>

  <h3>Hierarchical Model / Linear Regression</h3>
  <p>
    <ul>
      <li>x<sub>i</sub> ~ g(x|θ)</li>
      <li>Y<sub>i</sub> | X<sub>i</sub> ~ N(α+βX<sub>i</sub>, σ²)</li>
    </ul>
    Joint likelihood:
    L(α,β,σ²,θ) = Σ ln( (1/σ) φ((y<sub>i</sub>-(α+βx<sub>i</sub>))/σ) g(x<sub>i</sub>|θ) )
  </p>

  <h3>Laplace Distribution (Robust Regression)</h3>
  <p>
    If Y<sub>i</sub>|X<sub>i</sub> ~ (1/2) λ exp(-λ |y<sub>i</sub>-(α+βx<sub>i</sub>)|),  
    this leads to Least Absolute Deviations (LAD) regression.
  </p>
  <ul>
    <li>Normal errors → minimize squared error (sensitive to outliers).</li>
    <li>Laplace errors → minimize absolute deviations (robust to outliers).</li>
  </ul>

  <h2>Visual Summary</h2>
  <ul>
    <li>Observed categories: y<sub>1</sub>…y<sub>4</sub></li>
    <li>Hidden split: y<sub>1</sub>=x<sub>1</sub>+x<sub>2</sub></li>
    <li>E-step: estimate x<sub>2</sub></li>
    <li>M-step: update π</li>
    <li>Iterate until stable</li>
  </ul>

  <h2>Summary Insight</h2>
  <p>This genetics example is a template: split complicated observed categories into hidden simple ones, use conditional distributions to impute missing parts, optimize on the completed dataset, and repeat. EM alternates estimation and maximization, guaranteeing monotone ascent in likelihood.</p>
</section>

    </div>
















    

  









<div class="glow-box" id="4.3">
  <h2>Report on the EM Algorithm in Genetics, Genomics, and Public Health</h2>
<p>
These notes are based on the report by <strong>Nan M. Laird</strong>.  
The full document can be accessed here:
</p>
<ul>
  <li><a href="wk4.pdf" target="_blank">Download: EM Algorithm in Genetics, Genomics, and Public Health</a></li>
</ul>

</div>


















































  
</section>
<button class="prev-btn" onclick="location.href='wk3.html'"> ← Previous Section </button>

<button class="next-btn" onclick="location.href='wk5.html'">Next Section →</button>

<footer>
  <p>&copy; 2025 Shaan | Built with purpose</p>
</footer>
<script>
MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]']]
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

</body>
</html>
