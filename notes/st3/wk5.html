<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Week-5 | Shaan</title>
  <link rel="stylesheet" href="../style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display&family=Inter:wght@300;500&display=swap" rel="stylesheet">
  <style>
    body {
      background-color: #121212;
      color: #e0e0e0;
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
    }

    h1, h2, h3, h4 {
      font-family: 'Playfair Display', serif;
      color: #6effe0;
    }

    header {
      text-align: center;
      padding: 2rem 1rem 1rem;
      background: #1e1e1e;
      border-bottom: 2px solid #333;
    }

    header nav ul {
      list-style: none;
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 1rem;
      padding: 0;
    }

    header nav ul li {
      display: inline;
    }

    header nav ul li a {
      text-decoration: none;
      color: #6effe0;
      font-weight: 500;
      border: 1px solid transparent;
      padding: 0.4rem 0.8rem;
      border-radius: 6px;
      transition: all 0.3s ease;
    }

    header nav ul li a:hover {
      border-color: #6effe0;
      background-color: #6effe0;
      color: #000;
    }

    .notes-wrapper {
      max-width: 900px;
      margin: 3rem auto;
      padding: 2rem;
      animation: fadeIn 1.2s ease;
    }

    .info-block {
      margin-bottom: 2rem;
      text-align: left;
      line-height: 1.6;
    }

    .info-block strong {
      color: #6effe0;
    }

    .toc {
      background-color: #1e1e1e;
      border-left: 4px solid #6effe0;
      padding: 1rem 1.5rem;
      margin-bottom: 2rem;
      border-radius: 8px;
    }

    .toc ul {
      padding-left: 1rem;
    }

    .toc ul li {
      padding: 0.3rem 0;
    }

    .toc a {
      color: #c9d1d9;
      text-decoration: none;
      border-bottom: 1px dashed #6effe0;
    }

    .toc a:hover {
      color: #6effe0;
    }

    .glow-box {
      border: 2px solid #6effe0;
      box-shadow: 0 0 20px rgba(110, 255, 224, 0.2);
      border-radius: 16px;
      padding: 2rem;
      margin-bottom: 2rem;
      background-color: #1e1e1e;
      animation: pulse 3s infinite ease-in-out;
    }

    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 20px rgba(110, 255, 224, 0.2); }
      50% { box-shadow: 0 0 35px rgba(110, 255, 224, 0.4); }
    }

    ul {
      line-height: 1.6;
      padding-left: 1.2rem;
    }

    code, pre {
      background-color: #222;
      padding: 0.5rem 1rem;
      border-radius: 8px;
      font-family: monospace;
      color: #6effe0;
      display: block;
      overflow-x: auto;
    }

    .next-btn {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background-color: #6effe0;
      color: #000;
      padding: 0.6rem 1.2rem;
      border: none;
      border-radius: 8px;
      font-weight: bold;
      cursor: pointer;
      box-shadow: 0 0 10px #6effe0;
      transition: background-color 0.3s ease;
      z-index: 100;
    }


    .prev-btn {
    position: fixed;
    bottom: 20px;
    left: 20px;
    background-color: #6effe0;
    color: #000;
    border: none;
    padding: 0.6rem 1.2rem;
    border-radius: 8px;
    font-weight: bold
    cursor: pointer;
    box-shadow: 0 0 10px #6effe0;
    transition: background-color 0.3s ease;
    z-index: 100;
    }

    .next-btn:hover {
      background-color: #00ffcc;
    }

    .prev-btn:hover {
      background-color: #00ffcc;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 2rem 1rem;
    }
  </style>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</head>
<body>

<header>
  <h1>Statistical Methods III: Week-5</h1>
  <p>AY 2025–26</p>
  <nav>
    <ul>
      <li><a href="../../index.html">Home</a></li>
      <li><a href="../../assets/Shaan_CV.pdf">CV</a></li>
      <li><a href="../../sections/projects.html">Projects</a></li>
      <li><a href="../../sections/education.html">Education</a></li>
      <li><a href="../../sections/misc.html">Misc</a></li>
    </ul>
  </nav>
</header>

<section class="notes-wrapper">

  <div class="info-block">
    <p><strong>Instructor:</strong> Debasis Sengupta </p>
    <p><strong>Office / Department:</strong> ASU</p>
    <p><strong>Email:</strong> sdebasis@isical.ac.in</p>
    <p><strong>Marking Scheme:</strong><br>
      Assignments: 20% | Midterm Test: 30% | End Semester: 50%
    </p>
  </div>

  <div class="toc">
    <h3>Contents</h3>
    <ul>
      <li><a href="#5.1"> Motivation for LAD: Why look beyond OLS?</a></li>
      <li><a href="#5.2">Huber's Work and IRLS </a></li>
      <li><a href="#4.3"> Report on the EM Algorithm in Genetics, Genomics, and Public Health </a></li>
      
      
    </ul>
  </div>

  <!-- Notes section boxes -->
  <div class="glow-box" id="5.1">
    <section>

<h2>1. Motivation: Why look beyond OLS?</h2>
<ul>
  <li><b>OLS (Ordinary Least Squares)</b> is the standard regression method. It minimizes the sum of squared residuals.</li>
  <li>But squaring makes large errors (outliers) extremely influential. A single extreme data point can drag the fitted line towards itself.</li>
</ul>
<p><b>Visualization:</b> Imagine a scatter plot with one point far above the main cluster. The OLS line bends upward to reduce the squared error of that outlier.</p>

<h2>2. LAD (Least Absolute Deviation) regression</h2>
<ul>
  <li>LAD minimizes the sum of absolute residuals, instead of squared residuals.</li>
  <li>The formula becomes: “Choose \( \beta_0, \beta_1 \) to minimize \(\sum |y_i - \beta_0 - \beta_1 x_i|\).”</li>
  <li>Geometrically, this means we are fitting a line that makes the <i>absolute vertical distances</i> as small as possible.</li>
</ul>
<p><b>Visualization:</b> On the same scatter plot, the LAD line passes closer to the cluster and ignores the influence of the outlier.</p>

<h2>3. Comparing OLS and LAD</h2>
<ul>
  <li><b>OLS loss function</b> is quadratic (\(x^2\)), a smooth parabola.</li>
  <li><b>LAD loss function</b> is linear (\(|x|\)), a sharp V-shape.</li>
  <li>This small difference changes everything:
    <ul>
      <li>OLS → sensitive to outliers, mean-based.</li>
      <li>LAD → robust to outliers, median-based.</li>
    </ul>
  </li>
</ul>
<p><b>Visualization:</b> Plot two curves against \(x\):</p>
<ul>
  <li>Parabola \(x^2\), showing steep growth for large deviations.</li>
  <li>V-shape \(|x|\), showing slower, linear growth.</li>
</ul>

<h2>4. Practical meaning</h2>
<ul>
  <li>In OLS, a huge error from one observation dominates the objective function.</li>
  <li>In LAD, all errors contribute more equally — so the largest deviation doesn’t overwhelm the model.</li>
</ul>
<p>This robustness is why LAD is often called a “robust regression method.”</p>

<h2>5. Estimation procedure</h2>
<ul>
  <li>The general regression problem is written as: “Minimize \(\sum \rho(y_i - \beta_0 - \beta_1 x_i)\), where \(\rho\) is a chosen loss function.”</li>
  <li>Special cases:
    <ul>
      <li>\(\rho(x) = x^2\) → OLS.</li>
      <li>\(\rho(x) = |x|\) → LAD.</li>
    </ul>
  </li>
</ul>
<p>So LAD is just one member of a family of “M-estimators,” where you choose different functions \(\rho\) to control how much you penalize large errors.</p>

<h2>6. Computation: Why LAD is harder</h2>
<ul>
  <li>For OLS: we can take derivatives and get closed-form solutions.</li>
  <li>For LAD: the absolute value function is not differentiable at zero, so we can’t use simple calculus.</li>
  <li>Instead, LAD is solved using:
    <ol>
      <li><b>Linear programming</b> methods.</li>
      <li><b>Iteratively Reweighted Least Squares (IRLS)</b>.</li>
      <li><b>Geometric median ideas</b> (median instead of mean).</li>
    </ol>
  </li>
</ul>

<h2>7. Related concepts</h2>
<ul>
  <li><b>Median vs Mean:</b> OLS → mean, LAD → median. This is the one-dimensional analogue.</li>
  <li><b>Robustness:</b> LAD is less sensitive to extreme values.</li>
  <li><b>Quantile regression:</b> LAD regression is a special case (median regression). More generally, quantile regression fits other quantiles (like the 25th or 75th percentile).</li>
  <li><b>M-estimators:</b> General framework of estimators defined by minimizing different \(\rho\)-functions.</li>
</ul>

<h2>Quick descriptive summary</h2>
<ul>
  <li>OLS and LAD differ only in how they measure error.</li>
  <li>Squared error (OLS) exaggerates the importance of outliers.</li>
  <li>Absolute error (LAD) keeps all points on a more equal footing.</li>
  <li>The geometry of their loss functions explains this difference.</li>
  <li>LAD is computationally harder but conceptually simple: it finds a regression line that “balances” residuals in a median-like fashion.</li>
  <li>This makes LAD a robust alternative to OLS and a gateway to broader robust regression methods.</li>
</ul>

</section>

  </div>












    <div class="glow-box" id="5.2">
      <section>

<h2>1. The Big Question</h2>
<p>Huber asked: <i>when we fit a regression line, should we prioritize fitting the majority of the data well, or should we also give weight to the few outliers?</i></p>
<ul>
  <li>OLS (least squares) gives a <b>lot of importance to outliers</b> because squaring residuals makes big errors dominate.</li>
  <li>LAD (least absolute deviation) ignores outliers more, but may sometimes be too blunt.</li>
  <li>Huber’s idea: design a <b>compromise</b> — treat small errors like OLS (quadratic penalty, smooth), but treat large errors more gently (linear penalty).</li>
</ul>
<p>Think of it like having a <b>shock absorber</b>: normal bumps are treated smoothly (like OLS), but when the road gets too rough (big outliers), the absorber prevents the car (our model) from jumping violently.</p>

<h2>2. General M-estimation setup</h2>
<p>We write the objective function as:</p>
<p>\[
C(\beta_0, \beta_1) = \sum_{i=1}^n \rho(y_i - \beta_0 - \beta_1 x_i)
\]</p>
<ul>
  <li>Here \(\rho\) is a <b>loss function</b> that measures how bad a residual is.</li>
  <li>Choice of \(\rho\):
    <ul>
      <li>\(\rho(u) = u^2\) → OLS.</li>
      <li>\(\rho(u) = |u|\) → LAD.</li>
      <li>\(\rho(u) =\) Huber’s function (quadratic for small \(u\), linear for large \(u\)) → Robust regression.</li>
    </ul>
  </li>
</ul>
<p>So the framework is: <b>pick a \(\rho\), minimize the sum, get your regression line.</b></p>

<h2>3. First-order conditions (derivatives)</h2>
<p>To minimize, take derivatives with respect to the coefficients:</p>
<ul>
  <li>For \(\beta_0\):
    <p>\[
    \frac{\partial C}{\partial \beta_0} = -\sum_{i=1}^n \rho'(y_i - \beta_0 - \beta_1 x_i)
    \]</p>
  </li>
  <li>For \(\beta_1\):
    <p>\[
    \frac{\partial C}{\partial \beta_1} = -\sum_{i=1}^n \rho'(y_i - \beta_0 - \beta_1 x_i) \cdot x_i
    \]</p>
  </li>
</ul>
<p>Here \(\rho'(u)\) is called the <b>influence function</b> or <b>ψ-function</b>. It tells us how strongly each residual \(u\) “pulls” on the fit.</p>
<ul>
  <li>For OLS: \(\rho(u) = u^2\), so \(\rho'(u) = 2u\). Large residuals exert a very strong pull.</li>
  <li>For LAD: \(\rho(u) = |u|\), so \(\rho'(u) = \text{sign}(u)\). Every point, no matter how large, pulls with the same unit force.</li>
  <li>For Huber: \(\rho'(u)\) grows linearly for small \(u\), but is capped for large \(u\). Outliers can’t pull infinitely.</li>
</ul>
<p><b>Analogy:</b> Imagine each data point attached to the regression line with a spring. In OLS, the spring force grows stronger the more it’s stretched (quadratic). In LAD, every spring pulls with the same constant tension. In Huber’s, springs behave normally at first, but once stretched beyond a threshold, they lock at a maximum pull.</p>

<h2>4. Defining weights</h2>
<p>We introduce weights:</p>
<p>\[
w_i = \frac{\rho'(y_i - \beta_0 - \beta_1 x_i)}{y_i - \beta_0 - \beta_1 x_i}
\]</p>
<p>This formula is clever:</p>
<ul>
  <li>The numerator is the “influence” of a point.</li>
  <li>The denominator is the residual itself.</li>
  <li>So \(w_i\) tells us how much importance the point gets relative to its size.</li>
</ul>
<p>Check special cases:</p>
<ul>
  <li><b>OLS:</b> \(\rho'(u) = 2u\). So \(w_i = \frac{2u}{u} = 2\). Constant weight → all points are equally trusted.</li>
  <li><b>LAD:</b> \(\rho'(u) = \text{sign}(u)\). So \(w_i = \frac{\pm1}{u}\). Big residuals → tiny weights.</li>
  <li><b>Huber:</b> \(w_i\) depends on whether \(u\) is small (weight ≈ 2) or big (weight shrinks).</li>
</ul>
<p>So OLS = equal trust, LAD = diminishing trust, Huber = adaptive trust.</p>

<h2>5. Why the equations become nonlinear</h2>
<ul>
  <li>For OLS, weights are constant, so the equations reduce to neat linear formulas.</li>
  <li>For Huber/LAD, weights depend on the residuals, which themselves depend on the unknown parameters \(\beta_0, \beta_1\). That makes the system <b>nonlinear</b> and hard to solve directly.</li>
</ul>
<p>So we cheat: we <b>linearize</b> the problem by temporarily fixing the weights at some guess of \(\beta\), then updating \(\beta\), then recalculating weights, and so on.</p>

<h2>6. Iteratively Reweighted Least Squares (IRLS)</h2>
<p>The procedure is:</p>
<ol>
  <li>Start with an initial guess of \(\beta_0, \beta_1\) (often OLS estimates).</li>
  <li>Compute residuals.</li>
  <li>Compute weights \(w_i\) based on those residuals.</li>
  <li>Solve a <b>weighted least squares problem</b>:
    <p>\[
    \hat{\beta}^{(n+1)} = \arg\min_{\beta_0,\beta_1} \sum_{i=1}^n w_i^{(n)} (y_i - \beta_0 - \beta_1 x_i)^2
    \]</p>
    <p>This is just like OLS, but each point gets a different importance.</p>
  </li>
  <li>Repeat until the estimates stop changing.</li>
</ol>
<p>This is why it’s called <b>Iteratively Reweighted Least Squares</b>: each step is OLS with weights, but the weights are updated iteratively.</p>

<h2>7. Normal equations in IRLS</h2>
<p>If we differentiate the weighted sum with respect to the coefficients, we get:</p>
<ul>
  <li>For \(\beta_0\):
    <p>\[
    -2 \sum w_i (y_i - \beta_0 - \beta_1 x_i) = 0
    \]</p>
  </li>
  <li>For \(\beta_1\):
    <p>\[
    -2 \sum w_i (y_i - \beta_0 - \beta_1 x_i)(-x_i) = 0
    \]</p>
  </li>
</ul>
<p>These are just like OLS normal equations, but each term is multiplied by a weight.</p>

<h2>8. Related Concepts</h2>
<ul>
  <li><b>Huber’s ψ-function:</b> the derivative of the loss. Defines influence of residuals.</li>
  <li><b>Breakdown point:</b> the proportion of contaminated data the estimator can tolerate. OLS has breakdown point 0% (one bad outlier can ruin it). LAD has 50%. Huber is in between.</li>
  <li><b>Robust statistics:</b> field focused on methods that resist outliers and model misspecification.</li>
  <li><b>M-estimation:</b> generalization of maximum likelihood, where we minimize a sum of some function of residuals. Huber pioneered this.</li>
</ul>

<h2>Summary</h2>
<p>Huber’s work showed that we don’t have to choose between being “too harsh on outliers” (OLS) and “ignoring them completely” (LAD). His M-estimator is a <b>middle path</b>: quadratic penalty for small residuals (like OLS), linear for big ones (like LAD). This leads naturally to <b>IRLS</b>, where we keep updating weights until the regression line stabilizes.</p>

</section>

    </div>








































   <div class="glow-box" id="5.2">
      
    </div>
































   <div class="glow-box" id="5.2">
      
    </div>





























   <div class="glow-box" id="5.2">
      
    </div>






























   <div class="glow-box" id="5.2">
      
    </div>






























   <div class="glow-box" id="5.2">
      
    </div>































   <div class="glow-box" id="5.2">
      
    </div>





























  














    

  









<div class="glow-box" id="4.3">
  <h2>Report on the EM Algorithm in Genetics, Genomics, and Public Health</h2>
<p>
These notes are based on the report by <strong>Nan M. Laird</strong>.  
The full document can be accessed here:
</p>
<ul>
  <li><a href="wk4.pdf" target="_blank">Download: EM Algorithm in Genetics, Genomics, and Public Health</a></li>
</ul>

</div>


















































  
</section>
<button class="prev-btn" onclick="location.href='wk4.html'"> ← Previous Section </button>

<button class="next-btn" onclick="location.href='wk6.html'">Next Section →</button>

<footer>
  <p>&copy; 2025 Shaan | Built with purpose</p>
</footer>
<script>
MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]']]
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

</body>
</html>
