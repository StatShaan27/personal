<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Assignment: Week-3 | Shaan</title>
  <link rel="stylesheet" href="../style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display&family=Inter:wght@300;500&display=swap" rel="stylesheet">
  <style>
    body {
      background-color: #121212;
      color: #e0e0e0;
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
    }

    h1, h2, h3, h4 {
      font-family: 'Playfair Display', serif;
      color: #6effe0;
    }

    header {
      text-align: center;
      padding: 2rem 1rem 1rem;
      background: #1e1e1e;
      border-bottom: 2px solid #333;
    }

    header nav ul {
      list-style: none;
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 1rem;
      padding: 0;
    }

    header nav ul li {
      display: inline;
    }

    header nav ul li a {
      text-decoration: none;
      color: #6effe0;
      font-weight: 500;
      border: 1px solid transparent;
      padding: 0.4rem 0.8rem;
      border-radius: 6px;
      transition: all 0.3s ease;
    }

    header nav ul li a:hover {
      border-color: #6effe0;
      background-color: #6effe0;
      color: #000;
    }

    .notes-wrapper {
      max-width: 900px;
      margin: 3rem auto;
      padding: 2rem;
      animation: fadeIn 1.2s ease;
    }

    .info-block {
      margin-bottom: 2rem;
      text-align: left;
      line-height: 1.6;
    }

    .info-block strong {
      color: #6effe0;
    }

    .toc {
      background-color: #1e1e1e;
      border-left: 4px solid #6effe0;
      padding: 1rem 1.5rem;
      margin-bottom: 2rem;
      border-radius: 8px;
    }

    .toc ul {
      padding-left: 1rem;
    }

    .toc ul li {
      padding: 0.3rem 0;
    }

    .toc a {
      color: #c9d1d9;
      text-decoration: none;
      border-bottom: 1px dashed #6effe0;
    }

    .toc a:hover {
      color: #6effe0;
    }

    .glow-box {
      border: 2px solid #6effe0;
      box-shadow: 0 0 20px rgba(110, 255, 224, 0.2);
      border-radius: 16px;
      padding: 2rem;
      margin-bottom: 2rem;
      background-color: #1e1e1e;
      animation: pulse 3s infinite ease-in-out;
    }

    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 20px rgba(110, 255, 224, 0.2); }
      50% { box-shadow: 0 0 35px rgba(110, 255, 224, 0.4); }
    }

    ul {
      line-height: 1.6;
      padding-left: 1.2rem;
    }

    code, pre {
      background-color: #222;
      padding: 0.5rem 1rem;
      border-radius: 8px;
      font-family: monospace;
      color: #6effe0;
      display: block;
      overflow-x: auto;
    }

    .next-btn {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background-color: #6effe0;
      color: #000;
      padding: 0.6rem 1.2rem;
      border: none;
      border-radius: 8px;
      font-weight: bold;
      cursor: pointer;
      box-shadow: 0 0 10px #6effe0;
      transition: background-color 0.3s ease;
      z-index: 100;
    }

    .next-btn:hover {
      background-color: #00ffcc;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 2rem 1rem;
    }
  </style>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</head>
<body>

<header>
  <h1>Statistical Methods III: Assignment Week-3</h1>
  <p>AY 2025–26</p>
  <nav>
    <ul>
      <li><a href="../../index.html">Home</a></li>
      <li><a href="../../assets/Shaan_CV.pdf">CV</a></li>
      <li><a href="../../sections/projects.html">Projects</a></li>
      <li><a href="../../sections/education.html">Education</a></li>
      <li><a href="../../sections/misc.html">Misc</a></li>
    </ul>
  </nav>
</header>

<section class="notes-wrapper">

  <div class="info-block">
    <p><strong>Instructor:</strong> Debasis Sengupta </p>
    <p><strong>Office / Department:</strong> ASU</p>
    <p><strong>Email:</strong> sdebasis@isical.ac.in</p>
    <p><strong>Marking Scheme:</strong><br>
      Assignments: 20% | Midterm Test: 30% | End Semester: 50%
    </p>
  </div>

  <div class="toc">
    <h3>Contents</h3>
   <ul>
  <li><a href="#1">Question 1</a></li>
  <li><a href="#2">Question 2</a></li>
  <li><a href="#3">Question 3</a></li>
  <li><a href="#4">Question 4</a></li>
  <li><a href="#5">Question 5</a></li>
  <li><a href="#6">Question 6</a></li>
</ul>
  </div>

  <!-- Notes section boxes -->
  <div class="glow-box" id="1">
    <section>
  <h2>Question 1</h2>
  <p>
    We have counts \(Y_1,\dots,Y_4\) (multinomial given total \(n=\sum_i Y_i\)) with cell probabilities
  </p>
  <p>
    \[
      \pi_1=\tfrac12+\tfrac{\pi}{4},\qquad \pi_2=\pi_3=\tfrac{1-\pi}{4},\qquad \pi_4=\tfrac{\pi}{4},
    \]
    where \(\pi\in(0,1)\) is unknown. Reparametrise \(\pi=(1-\theta)^2\) and derive the Fisher–scoring iterative update for the MLE of \(\pi\) (i.e. the update for \(\theta\)).
  </p>

  <h2>Solution — Detailed Answer</h2>

  <h2>Step 0 — substitute the reparametrisation</h2>
  <p>
    Put \(\pi=(1-\theta)^2\) and define
  </p>
  <p>\(a:=\dfrac{1-\theta}{2}\quad(\Rightarrow a^2=\dfrac{(1-\theta)^2}{4}).\)</p>
  <p>
    Then the cell probabilities as functions of \(\theta\) become (useful algebraic forms)
  </p>
  <ul>
    <li>\(p_1(\theta)=\pi_1=\tfrac12+\tfrac{(1-\theta)^2}{4}=\tfrac12 + a^2,\)</li>
    <li>\(p_2(\theta)=p_3(\theta)=\tfrac{1-(1-\theta)^2}{4}=\tfrac14 - a^2,\)</li>
    <li>\(p_4(\theta)=\tfrac{(1-\theta)^2}{4}=a^2.\)</li>
  </ul>

  <h2>Step 1 — log-likelihood</h2>
  <p>
    For total \(n=\sum_i Y_i\),
  </p>
  <p>\(\ell(\theta)=\sum_{i=1}^4 Y_i\log p_i(\theta)+\text{const}.\)</p>

  <h2>Step 2 — first derivative (score)</h2>
  <p>
    Differentiate \(p_i\) w.r.t. \(\theta\). Using \(a=(1-\theta)/2\) and \(da/d\theta=-1/2\),
  </p>
  <ul>
    <li>\(\dfrac{dp_1}{d\theta}=\dfrac{dp_4}{d\theta}=-a,\)</li>
    <li>\(\dfrac{dp_2}{d\theta}=\dfrac{dp_3}{d\theta}=+a.\)</li>
  </ul>
  <p>
    Hence the score \(S(\theta)=\dfrac{d\ell}{d\theta}\) is
  </p>
  <p>
    \[
      \begin{aligned}
        S(\theta)
        &=\sum_{i=1}^4 Y_i\frac{p_i'(\theta)}{p_i(\theta)}
        = -a\frac{Y_1}{p_1}+a\frac{Y_2}{p_2}+a\frac{Y_3}{p_3}-a\frac{Y_4}{p_4}\\
        &= a\Big(\frac{Y_2+Y_3}{p_2}-\frac{Y_1}{p_1}-\frac{Y_4}{p_4}\Big).
      \end{aligned}
    \]
  </p>

  <h2>Step 3 — Fisher (expected) information</h2>
  <p>
    For a multinomial with probabilities \(p_i(\theta)\),
  </p>
  <p>\(I(\theta)=n\sum_{i=1}^4\frac{(p_i'(\theta))^2}{p_i(\theta)}.\)</p>
  <p>
    Because \((p_i')^2=a^2\) for every \(i\),
  </p>
  <p>\(I(\theta)=n\,a^2\Big(\frac{1}{p_1}+\frac{2}{p_2}+\frac{1}{p_4}\Big).\)</p>

  <h2>Step 4 — Fisher scoring update (final iterative step)</h2>
  <p>
    Fisher scoring updates by
  </p>
  <p>\(\displaystyle \theta^{(t+1)}=\theta^{(t)}+\frac{S(\theta^{(t)})}{I(\theta^{(t)})}.\)</p>
  <p>
    Substitute \(S\) and \(I\). Cancelling common factors gives a convenient expression in terms of proportions \(r_i=Y_i/n\):
  </p>
  <p style="margin-left:0.5em; margin-right:0.5em;">
    \[
      \boxed{\,\theta^{(t+1)}=\theta^{(t)}+\frac{1}{a}\cdot
      \frac{\dfrac{r_2+r_3}{p_2}-\dfrac{r_1}{p_1}-\dfrac{r_4}{p_4}}
      {\dfrac{1}{p_1}+\dfrac{2}{p_2}+\dfrac{1}{p_4}}\,,}
    \]
  </p>
  <p>
    where all \(p_j\) and \(a\) are evaluated at \(\theta^{(t)}\):
  </p>
  <ul>
    <li>\(a=\dfrac{1-\theta^{(t)}}{2},\)</li>
    <li>\(p_1=\tfrac12+a^2,\quad p_2=\tfrac14-a^2,\quad p_4=a^2,\)</li>
    <li>\(r_i=Y_i/n\). (Equivalently you may write the update using counts \(Y_i\); the \(n\) cancels.)</li>
  </ul>
  <p>
    This boxed formula is the Fisher–scoring iterative step for \(\theta\). After convergence \(\hat\theta\) gives \(\hat\pi=(1-\hat\theta)^2\).
  </p>

  <h2>Related Concepts / Remarks</h2>
  <p>
    Short remarks and important connections.
  </p>
  <ul>
    <li><strong>Why Fisher scoring?</strong> Fisher scoring replaces the observed negative second derivative by its expectation (the Fisher information). For generalized multinomial models, it often gives a stable and efficient algorithm; it is Newton–Raphson using expected information.</li>
    <li><strong>Score / information simplification:</strong> The algebra simplified nicely because all \((p_i')^2\) equal the same value \(a^2\). That yields compact expressions for \(S\) and \(I\).</li>
    <li><strong>Parameter domain & numerical caution:</strong> For valid probabilities we need \(p_2=\tfrac14-a^2>0\), i.e. \(a^2<1/4\) ⇔ \(|1-\theta|<1\) ⇔ \(\theta\in(0,2)\). Practically choose an initial \(\theta^{(0)}\) inside \((0,2)\) to keep all \(p_i>0\) (and avoid division by zero). If an iteration proposes \(\theta\) outside the admissible range, project it back (or use a damped update).</li>
    <li><strong>Initial guess (practical):</strong> A reasonable start is to match \(p_4\) with the observed proportion \(r_4=Y_4/n\): \(\pi\approx 4r_4\), so \(\theta^{(0)}=1-\sqrt{\max(0,\min(1,4r_4))}\). If \(4r_4>1\) use a safer interior starting value (e.g. \(\theta^{(0)}=1\) ± small).</li>
    <li><strong>Stopping rule:</strong> Use relative change \(|\theta^{(t+1)}-\theta^{(t)}|<\varepsilon\) (e.g. \(10^{-8}\)) or change in log-likelihood, or max iterations.</li>
    <li><strong>Observed vs expected information:</strong> If you prefer Newton–Raphson (observed information), compute the second derivative \(d^2\ell/d\theta^2\) and use \(\theta^{(t+1)}=\theta^{(t)}- \big(\ell'(\theta^{(t)})/\ell''(\theta^{(t)})\big)\). Fisher scoring often yields more stable steps in multinomial problems.</li>
    <li><strong>Asymptotic variance:</strong> At convergence \(\hat\theta\) the asymptotic variance is approximately \(I(\hat\theta)^{-1}\), with \(I(\hat\theta)=n a^2\big(1/p_1+2/p_2+1/p_4\big)\) evaluated at \(\hat\theta\). Delta method gives variance for \(\hat\pi=(1-\theta)^2\).</li>
  </ul>

  <h2>Viz (Visualizations)</h2>
  <p>
    Practical plots to inspect and debug the algorithm numerically.
  </p>
  <ul>
    <li>Plot the log-likelihood \(\ell(\theta)\) over a grid in the admissible \(\theta\)-interval (say \([0.01,1.99]\)) to see where the peak lies and whether it's sharp or flat.</li>
    <li>Plot the iteration path \(\theta^{(t)}\) vs iteration \(t\) to check monotone convergence and whether damping is needed.</li>
    <li>Optionally plot the score \(S(\theta)\) and expected information \(I(\theta)\) over the grid to visualise step sizes implied by Fisher scoring.</li>
  </ul>

  <h2>Other Relevant Points</h2>
  <p>
    Additional practical advice, edge-cases, and numerical tips.
  </p>
  <ul>
    <li>If any observed cell count is zero, avoid dividing by zero; start from interior values and/or add a tiny pseudo-count (e.g. +0.5) if needed for numerical stability.</li>
    <li>Fisher scoring will usually converge fast for this one-parameter problem; if oscillation occurs, use step-halving/damping (e.g. \(\theta^{(t+1)}\leftarrow \theta^{(t)}+\gamma\cdot\)update with \(\gamma\in(0,1)\)).</li>
    <li>After obtaining \(\hat\theta\), transform back to \(\hat\pi=(1-\hat\theta)^2\) and report standard errors via the observed Fisher information or the delta method.</li>
    <li>Implementation note: compute \(p_2=\tfrac14-a^2\) carefully to avoid catastrophic cancellation when \(a^2\) is close to \(\tfrac14\).</li>
  </ul>
</section>

  </div>












    <div class="glow-box" id="2">
      <section>
  <h2>Question 2</h2>
  <p>
    A complete-data formulation for the problem in Q1 was given in class so that the EM algorithm can be used. Starting from that, obtain <strong>a single iterative step</strong> (merge the E-step and M-step) which updates the estimate of \(\pi\).
  </p>

  <h2>Solution — Detailed Answer</h2>

  <h2>Setup / notation</h2>
  <p>
    Write the cell probabilities as linear functions of \(\pi\):
  </p>
  <p>\(p_i(\pi)=a_i+b_i\pi,\qquad i=1,\dots,4,\)</p>
  <ul>
    <li>\(a_1=\tfrac12,\; b_1=\tfrac14\)</li>
    <li>\(a_2=\tfrac14,\; b_2=-\tfrac14\)</li>
    <li>\(a_3=\tfrac14,\; b_3=-\tfrac14\)</li>
    <li>\(a_4=0,\; b_4=\tfrac14\)</li>
  </ul>
  <p>
    Let \(\widetilde Y_i^{(t)}\) denote the expected complete counts computed at the E-step under \(\pi^{(t)}\). Write \(\tilde r_i^{(t)}=\widetilde Y_i^{(t)}/n\) if you prefer proportions.
  </p>

  <h2>Step 1 — Q-function (expected complete log-likelihood)</h2>
  <p>
    The EM M-step maximises
  </p>
  <p>\(Q(\pi\mid\pi^{(t)})=\sum_{i=1}^4 \widetilde Y_i^{(t)}\log\big(a_i+b_i\pi\big) + \text{const (w.r.t. }\pi).\)</p>

  <h2>Step 2 — derivative of Q (score for Q)</h2>
  <p>
    Differentiate to get
  </p>
  <p>\(\dfrac{\partial Q}{\partial\pi}(\pi\mid\pi^{(t)})=\sum_{i=1}^4 \widetilde Y_i^{(t)}\dfrac{b_i}{a_i+b_i\pi}.\)</p>
  <p>
    Solving \(\partial Q/\partial\pi=0\) for \(\pi\) exactly gives the M-step but is implicit. A practical and standard merged one-line update is obtained by taking a single Newton step for this equation using \(\widetilde Y_i^{(t)}\).
  </p>

  <h2>Step 3 — Newton (one-step) update on Q (single merged iteration)</h2>
  <p>
    The second derivative is
  </p>
  <p>\(\dfrac{\partial^2 Q}{\partial\pi^2}(\pi\mid\pi^{(t)})=-\sum_{i=1}^4 \widetilde Y_i^{(t)}\dfrac{b_i^2}{(a_i+b_i\pi)^2}.\)</p>
  <p>
    Therefore a Newton update applied to \(Q\) gives the single-step merged iteration:
  </p>
  <p style="margin-left:0.5em; margin-right:0.5em;">
    \[
      \boxed{\;
      \pi^{(t+1)} \;=\; \pi^{(t)} \;+\; 
      \frac{\displaystyle \sum_{i=1}^4 \widetilde Y_i^{(t)}\dfrac{b_i}{a_i+b_i\pi^{(t)}}}
      {\displaystyle \sum_{i=1}^4 \widetilde Y_i^{(t)}\dfrac{b_i^2}{(a_i+b_i\pi^{(t)})^2}}
      \; }.
    \]
  </p>

  <h2>Step 4 — explicit specialised form (using \(b_i=\pm\tfrac14\))</h2>
  <p>
    Plugging the \(a_i,b_i\) values and factoring constants yields the equivalent explicit form
  </p>
  <p style="margin-left:0.5em; margin-right:0.5em;">
    \[
      \boxed{\;
      \pi^{(t+1)}=\pi^{(t)} + \dfrac{\displaystyle 
      \frac{\widetilde Y_1^{(t)}}{p_1(\pi^{(t)})}-\frac{\widetilde Y_2^{(t)}}{p_2(\pi^{(t)})}-\frac{\widetilde Y_3^{(t)}}{p_3(\pi^{(t)})}+\frac{\widetilde Y_4^{(t)}}{p_4(\pi^{(t)})}
      }
      {\displaystyle 
      \frac{\widetilde Y_1^{(t)}}{p_1(\pi^{(t)})^2}+\frac{\widetilde Y_2^{(t)}}{p_2(\pi^{(t)})^2}
      +\frac{\widetilde Y_3^{(t)}}{p_3(\pi^{(t)})^2}+\frac{\widetilde Y_4^{(t)}}{p_4(\pi^{(t)})^2}
      }\; },
    \]
  </p>
  <p>
    where \(p_i(\pi^{(t)})=a_i+b_i\pi^{(t)}\). If data are complete then \(\widetilde Y_i^{(t)}=Y_i\).
  </p>

  <h2>How this merges E- and M-steps (practical summary)</h2>
  <ul>
    <li><strong>E-step:</strong> compute \(\widetilde Y_i^{(t)}=\mathbb{E}[Y_i^{\text{(complete)}}\mid\text{observed},\pi^{(t)}]\).</li>
    <li><strong>M-step (merged):</strong> perform the single Newton update for \(\pi\) given above (one explicit line).</li>
    <li>If you insist on a pure EM M-step you would need to solve \(\sum_i \widetilde Y_i^{(t)}\dfrac{b_i}{a_i+b_i\pi^{(t+1)}}=0\) exactly each iteration, which is implicit and inconvenient; the one-step Newton is the common practical remedy.</li>
  </ul>

  <h2>Related Concepts</h2>
  <ul>
    <li><strong>Newton-within-EM:</strong> using one Newton (or Fisher) step in the M-step when a closed-form maximiser is unavailable; often accelerates convergence while keeping the EM structure.</li>
    <li><strong>Fisher-scoring variant:</strong> replace the Hessian in the denominator by its expectation (with \(\widetilde Y_i^{(t)}\) weights) to obtain a Fisher-scoring merged iteration.</li>
    <li><strong>Feasible region:</strong> ensure \(\pi^{(t+1)}\in(0,1)\) and \(p_i(\pi^{(t+1)})>0\). If the update leaves the feasible set, apply damping (step-halving) or project back into \((0,1)\).</li>
  </ul>

  <h2>Viz (Diagnostics)</h2>
  <ul>
    <li>Plot \(Q(\pi\mid\pi^{(t)})\) vs \(\pi\) for several \(\pi^{(t)}\) to check whether the Newton step moves uphill.</li>
    <li>Trace \(\pi^{(t)}\) vs iteration \(t\) and optionally plot the observed log-likelihood to verify monotone behaviour (or detect the need for damping).</li>
    <li>Plot the evolution of \(\widetilde Y_i^{(t)}\) if there is missingness — these typically stabilise quickly and reveal which components drive updates.</li>
  </ul>

  <h2>Other Relevant Points / Implementation tips</h2>
  <ul>
    <li>Use interior starting values for \(\pi^{(0)}\) (e.g. method-of-moments or \(\pi^{(0)}=\min(0.99,\max(0.01,4r_4))\)).</li>
    <li>If any \(\widetilde Y_i^{(t)}\) or observed \(Y_i\) is zero, avoid division-by-zero by adding a tiny pseudo-count (e.g. +0.5) or using safe-guards in code.</li>
    <li>Control step length (damping) when necessary: set \(\pi^{(t+1)}=\pi^{(t)}+\gamma\Delta\pi\) with \(\gamma\in(0,1]\) chosen to keep \(Q\) (or the observed log-likelihood) non-decreasing.</li>
    <li>Check convergence by \(|\pi^{(t+1)}-\pi^{(t)}|<\varepsilon\) or small change in observed log-likelihood; report standard errors using the (observed or expected) information at the final iterate and the delta method if transforming parameters.</li>
  </ul>
</section>

    </div>






















  <div class="glow-box" id="3">
      <section>
  <h2>Question 3</h2>
  <p>
    Let \(Y_1,Y_2,Y_3,Y_4\) be counts in four genetic categories. Conditionally on the total
    \(N=Y_1+Y_2+Y_3+Y_4\) the vector has a multinomial distribution with cell probabilities
  </p>
  <p>\[
    p_1(\pi)=\tfrac12+\tfrac{\pi}{4},\qquad
    p_2(\pi)=p_3(\pi)=\tfrac{1-\pi}{4},\qquad
    p_4(\pi)=\tfrac{\pi}{4},
  \]</p>
  <p>where the unknown parameter \(\pi\in(0,1)\).  Viewing this as an incomplete-data / latent-allocation problem, the EM algorithm applies. The question: <strong>obtain a closed form expression for the value of \(\pi\) attained at the convergence of the EM algorithm</strong>.</p>

  <h2>Detailed solution — overview</h2>
  <ul>
    <li>Introduce notation \(n_1=y_1,\; n_2=y_2+y_3,\; n_4=y_4\) and \(N=n_1+n_2+n_4\).</li>
    <li>Write the (observed) log-likelihood \(\ell(\pi)\), differentiate (score), clear denominators to obtain a quadratic, and solve it.</li>
    <li>Choose the root lying in \((0,1)\) — that is the EM fixed-point (the stationary solution of the observed-data likelihood).</li>
  </ul>

  <h2>Step 1 — write the (observed) log-likelihood</h2>
  <ul>
    <li>Up to an additive constant, the log-likelihood is
      <p>\[
        \ell(\pi)=y_1\log\!\Big(\tfrac12+\tfrac{\pi}{4}\Big)+(y_2+y_3)\log\!\Big(\tfrac{1-\pi}{4}\Big)+y_4\log\!\Big(\tfrac{\pi}{4}\Big).
      \]</p>
    </li>
    <li>With \(n_1=y_1,\; n_2=y_2+y_3,\; n_4=y_4\) and \(N=n_1+n_2+n_4\), substitute to simplify notation.</li>
  </ul>

  <h2>Step 2 — stationary equation (score = 0)</h2>
  <ul>
    <li>Differentiate \(\ell(\pi)\) w.r.t. \(\pi\). Useful derivatives:
      <ul>
        <li>\(\dfrac{d}{d\pi}\log\!\Big(\tfrac12+\tfrac{\pi}{4}\Big)=\dfrac{1}{2+\pi}\).</li>
        <li>\(\dfrac{d}{d\pi}\log\!\Big(\tfrac{1-\pi}{4}\Big)=-\dfrac{1}{1-\pi}\).</li>
        <li>\(\dfrac{d}{d\pi}\log\!\Big(\tfrac{\pi}{4}\Big)=\dfrac{1}{\pi}\).</li>
      </ul>
    </li>
    <li>Thus the score equation is
      <p>\[
        \frac{n_1}{2+\pi}-\frac{n_2}{1-\pi}+\frac{n_4}{\pi}=0.
      \]</p>
    </li>
  </ul>

  <h2>Step 3 — clear denominators to obtain a polynomial</h2>
  <ul>
    <li>Multiply the score equation by \(\pi(1-\pi)(2+\pi)\) to remove denominators:
      <p>\[
        n_1\pi(1-\pi)+n_4(1-\pi)(2+\pi)-n_2\pi(2+\pi)=0.
      \]</p>
    </li>
    <li>Expand and collect like powers:
      <ul>
        <li>\(n_1\pi(1-\pi)=n_1(\pi-\pi^2)\).</li>
        <li>\((1-\pi)(2+\pi)=2-\pi-\pi^2\), so \(n_4(1-\pi)(2+\pi)=n_4(2-\pi-\pi^2)\).</li>
        <li>\(n_2\pi(2+\pi)=n_2(2\pi+\pi^2)\).</li>
      </ul>
    </li>
    <li>Collecting coefficients gives the quadratic:
      <p>\[
        - N \pi^2 + (n_1 - n_4 - 2 n_2)\pi + 2 n_4 = 0,
      \]</p>
      and multiplying by \(-1\) yields the standard form
      <p>\[
        N\pi^2 + (-n_1 + n_4 + 2 n_2)\pi - 2 n_4 = 0.
      \]</p>
    </li>
  </ul>

  <h2>Step 4 — solve the quadratic</h2>
  <ul>
    <li>Apply the quadratic formula. The solutions are
      <p>\[
        \boxed{ \displaystyle
        \pi \;=\; \frac{\,n_1 - n_4 - 2 n_2 \;\pm\; \sqrt{( -n_1 + n_4 + 2 n_2)^2 + 8 N n_4}\,}{2N} }.
      \]</p>
      <p>(The discriminant simplifies as \(( -n_1 + n_4 + 2 n_2)^2 - 4\cdot N\cdot(-2 n_4) = ( -n_1 + n_4 + 2 n_2)^2 + 8 N n_4\).)</p>
    </li>
  </ul>

  <h2>Step 5 — choose the correct root (EM convergence)</h2>
  <ul>
    <li>The EM algorithm converges to a stationary point of the observed-data likelihood; the valid parameter must lie in \((0,1)\).</li>
    <li>Thus \(\pi_{\text{EM}}\) is the root above that lies in \((0,1)\). In practice, when \(n_4>0\) the “\(+\)” branch typically yields the admissible root; choose the sign that gives \(0<\pi<1\).</li>
  </ul>

  <p>Conclusion / boxed result (explicit):</p>
  <p>\[
    \pi_{\star} \;=\; \frac{n_1 - n_4 - 2 n_2 \;\pm\; \sqrt{( -n_1 + n_4 + 2 n_2)^2 + 8 N n_4}}{2N},
  \]</p>
  <p>with the sign chosen so that \(0<\pi_\star<1\). This closed-form gives the stationary value that the EM fixed-point must satisfy (hence the EM limit).</p>

  <h2>Related concepts</h2>
  <ul>
    <li><strong>EM and stationary equations:</strong> An EM fixed-point satisfies the observed-data score equations whenever the M-step produces a full maximizer; thus the EM limit is a stationary point of the observed log-likelihood.</li>
    <li><strong>Score equation / MLE:</strong> Here the score reduces to a quadratic in \(\pi\), so the MLE (and EM limit) can be obtained in closed form.</li>
    <li><strong>Root selection / boundaries:</strong> If no root lies in \((0,1)\) check boundary MLEs at \(\pi=0\) or \(\pi=1\) and inspect the log-likelihood for maxima vs minima.</li>
    <li><strong>Numerical stability:</strong> Reparametrizations (e.g., \(\pi=(1-\theta)^2\)) can help numerics, though algebra gave an explicit solution here.</li>
  </ul>

  <h2>Viz</h2>
  <ul>
    <li>Plot the observed-data log-likelihood \(\ell(\pi)\) on \(\pi\in[0,1]\) to visualize the stationary point and to confirm the chosen root is a global maximum.</li>
    <li>Plot the quadratic polynomial \(f(\pi)=N\pi^2 +(-n_1 + n_4 + 2 n_2)\pi -2 n_4\) to see the two roots and where they lie relative to \([0,1]\).</li>
  </ul>

  <h2>Other points worth mentioning</h2>
  <ul>
    <li>If you run EM from different starting values and it always converges to the same root in \((0,1)\), that gives empirical confirmation the root is the global maximizer (for regular data).</li>
    <li>When \(n_4=0\) the formula simplifies (the discriminant reduces) and one must check boundaries carefully — often the MLE may sit at \(\pi=0\).</li>
    <li>Everything above preserves the full derivation and the final closed-form expression; substitute \(n_1=y_1,\; n_2=y_2+y_3,\; n_4=y_4\) when applying to data.</li>
  </ul>
</section>

    </div>























  <div class="glow-box" id="4">
   <section>
  <h2>Question 4</h2>
  <p>
    Consider paired data \((X_i,Y_i),\ i=1,\dots,n\), where the pairs are independent, the marginal distribution of the \(X_i\) is unspecified, and the conditional distribution of \(Y_i\) given \(X_i=x_i\) is
  </p>
  <p>\[
    Y_i\mid X_i=x_i \sim N(\alpha+\beta x_i,\ \sigma^2).
  \]</p>
  <p>Show that the maximum likelihood estimators (MLEs) of \(\alpha\) and \(\beta\) coincide with the ordinary least squares estimators for the linear regression model \(Y=\alpha+\beta X+\varepsilon\).</p>

  <h2>Detailed solution</h2>
  <ul>
    <li><strong>Idea.</strong> Because only the conditional distribution of \(Y_i\) given \(X_i\) is specified, the likelihood for \((\alpha,\beta,\sigma^2)\) is the conditional likelihood
      <p>\[
        L(\alpha,\beta,\sigma^2)=\prod_{i=1}^n f_{Y\mid X}(y_i\mid x_i)
        =\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\!\Big(-\frac{(y_i-(\alpha+\beta x_i))^2}{2\sigma^2}\Big).
      \]</p>
      <p>Maximizing this over \(\alpha,\beta\) (for fixed \(\sigma^2\)) is equivalent to minimizing the sum of squared residuals</p>
      <p>\[
        Q(\alpha,\beta)=\sum_{i=1}^n\big(y_i-(\alpha+\beta x_i)\big)^2.
      \]</p>
      <p>Hence the MLEs of \(\alpha,\beta\) are the least-squares estimators.</p>
    </li>

```
<li><h2>Step 1 — log-likelihood and equivalence to least squares</h2>
  <ul>
    <li>The log-likelihood (conditional on the observed \(x_i\)) is
      <p>\[
        \ell(\alpha,\beta,\sigma^2)
        = -\frac{n}{2}\log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n\big(y_i-(\alpha+\beta x_i)\big)^2.
      \]</p>
    </li>
    <li>For fixed \(\sigma^2\), maximizing \(\ell\) w.r.t. \(\alpha,\beta\) is identical to minimizing \(Q(\alpha,\beta)\).</li>
  </ul>
</li>

<li><h2>Step 2 — normal equations</h2>
  <ul>
    <li>Set partial derivatives of \(Q\) to zero:
      <p>\[
        \frac{\partial Q}{\partial \alpha} = -2\sum_{i=1}^n\big(y_i-(\alpha+\beta x_i)\big)=0,
      \]</p>
      <p>\[
        \frac{\partial Q}{\partial \beta} = -2\sum_{i=1}^n x_i\big(y_i-(\alpha+\beta x_i)\big)=0.
      \]</p>
    </li>
    <li>Divide by \(-2\) and rewrite to obtain the normal equations:
      <p>\[
        n\alpha + \beta\sum_{i=1}^n x_i = \sum_{i=1}^n y_i,
      \]</p>
      <p>\[
        \alpha\sum_{i=1}^n x_i + \beta\sum_{i=1}^n x_i^2 = \sum_{i=1}^n x_i y_i.
      \]</p>
    </li>
  </ul>
</li>

<li><h2>Step 3 — solve for \(\hat\alpha,\hat\beta\)</h2>
  <ul>
    <li>Let \(\bar x=\dfrac{1}{n}\sum_i x_i\), \(\bar y=\dfrac{1}{n}\sum_i y_i\). Define
      <p>\[
        S_{xx}=\sum_{i=1}^n (x_i-\bar x)^2=\sum_i x_i^2 - n\bar x^2,\qquad
        S_{xy}=\sum_{i=1}^n (x_i-\bar x)(y_i-\bar y)=\sum_i x_i y_i - n\bar x\bar y.
      \]</p>
    </li>
    <li>Solving the normal equations yields the OLS formulas (and hence the MLEs):
      <p>\[
        \boxed{\displaystyle
        \hat\beta \;=\; \frac{S_{xy}}{S_{xx}}, \qquad
        \hat\alpha \;=\; \bar y - \hat\beta\,\bar x.}
      \]</p>
    </li>
  </ul>
</li>

<li><h2>Step 4 — MLE for \(\sigma^2\)</h2>
  <ul>
    <li>Plugging \(\hat\alpha,\hat\beta\) back into the log-likelihood, the MLE for \(\sigma^2\) is
      <p>\[
        \boxed{\displaystyle
        \hat\sigma^2_{\text{MLE}} \;=\; \frac{1}{n}\sum_{i=1}^n \big(y_i - \hat\alpha - \hat\beta x_i\big)^2.}
      \]</p>
      <p>Note: this ML estimator uses division by \(n\); the unbiased estimator uses division by \(n-2\).</p>
    </li>
  </ul>
</li>

<li><h2>Step 5 — second-derivative check (concavity)</h2>
  <ul>
    <li>The conditional log-likelihood is quadratic (negative-definite in \(\alpha,\beta\) for fixed \(\sigma^2\)), so the solution above is a global maximizer for \(\alpha,\beta\).</li>
  </ul>
</li>
```

  </ul>

  <p>Thus the MLEs of \(\alpha\) and \(\beta\) coincide exactly with the least-squares estimators.</p>

  <h2>Related concepts</h2>
  <ul>
    <li><strong>Conditional likelihood vs joint likelihood.</strong> When the marginal distribution of \(X\) is unspecified, inference about regression parameters is done via the conditional likelihood \(f(y\mid x)\); this is equivalent to treating \(x_i\) as fixed regressors for likelihood purposes.</li>
    <li><strong>Gauss–Markov theorem.</strong> Under mean-zero, homoscedastic, uncorrelated errors, OLS estimators are the Best Linear Unbiased Estimators (BLUE). With normality they are also MLEs (hence efficient).</li>
    <li><strong>MLE vs unbiased estimation of \(\sigma^2\).</strong> ML gives \(\hat\sigma^2 = \mathrm{RSS}/n\); the unbiased estimator is \(\mathrm{RSS}/(n-2)\).</li>
    <li><strong>Fisher information / asymptotic variance.</strong> For known \(\sigma^2\), the exact variances are
      <p>\[
        \operatorname{Var}(\hat\beta)=\frac{\sigma^2}{S_{xx}},\qquad
        \operatorname{Var}(\hat\alpha)=\sigma^2\Big(\frac{1}{n}+\frac{\bar x^2}{S_{xx}}\Big).
      \]</p>
    </li>
    <li><strong>Interpretation when \(X\) random.</strong> If \(X_i\) are random but independent of the error and their distribution does not depend on \(\alpha,\beta\), the conditional-likelihood approach still yields the same estimators as the joint likelihood (which factors as marginal of \(X\) times conditional of \(Y\mid X\)).</li>
  </ul>

  <h2>Viz</h2>
  <ul>
    <li>Scatter plot \((x_i,y_i)\) with fitted regression line \(y=\hat\alpha+\hat\beta x\) and residuals drawn down to the line — checks fit visually.</li>
    <li>Residual diagnostics: plot residuals vs fitted values and a Q–Q plot of residuals to check homoscedasticity and normality assumptions.</li>
    <li>Confidence bands: show pointwise confidence intervals for the regression line using the variance formulas above.</li>
    <li>Profile of log-likelihood: plot \(\ell(\alpha,\beta)\) (or profile \(\ell(\beta)\) fixing \(\alpha=\bar y-\beta\bar x\)) to visualize the quadratic shape and the unique maximum.</li>
  </ul>

     
</section>


  </div>

























  <div class="glow-box" id="5">
      <section>
  <h2>Question 5</h2>
  <p>
    In the regression model
  </p>
  <p>\[
    Y_i \mid X_i = x_i \;\sim\; N(\alpha + \beta x_i,\ \sigma^2), \quad i=1,\dots,n,
  \]</p>
  <p>we already showed that the MLEs of \(\alpha\) and \(\beta\) are the least-squares estimators. <strong>Now:</strong> What is the MLE of \(\sigma^2\)? Is this MLE unbiased?</p>

  <h2>Detailed solution</h2>
  <ul>
    <li>
      <h2>Step 1 — log-likelihood (conditional on \(x_i\))</h2>
      <ul>
        <li>The conditional likelihood is
          <p>\[
            L(\alpha,\beta,\sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}
            \exp\!\left(-\frac{(y_i-(\alpha+\beta x_i))^2}{2\sigma^2}\right).
          \]</p>
        </li>
        <li>The log-likelihood is
          <p>\[
            \ell(\alpha,\beta,\sigma^2) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log\sigma^2
            - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \alpha - \beta x_i)^2.
          \]</p>
        </li>
      </ul>
    </li>

```
<li>
  <h2>Step 2 — profile likelihood for \(\sigma^2\)</h2>
  <ul>
    <li>Let \(\displaystyle \mathrm{RSS}=\sum_{i=1}^n (y_i-\hat\alpha-\hat\beta x_i)^2\) denote the residual sum of squares at the MLEs \(\hat\alpha,\hat\beta\).</li>
    <li>Plugging \(\hat\alpha,\hat\beta\) into \(\ell\) and differentiating w.r.t. \(\sigma^2\) gives
      <p>\[
        \frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{\mathrm{RSS}}{2\sigma^4}.
      \]</p>
    </li>
    <li>Setting this to zero and solving yields
      <p>\[
        -\frac{n}{2\sigma^2} + \frac{\mathrm{RSS}}{2\sigma^4} = 0
        \ \Longrightarrow\ -n\sigma^2 + \mathrm{RSS}=0,
      \]</p>
      <p>hence</p>
      <p>\[
        \boxed{\displaystyle \hat\sigma^2_{\mathrm{MLE}} = \frac{\mathrm{RSS}}{n}.}
      \]</p>
    </li>
  </ul>
</li>

<li>
  <h2>Step 3 — unbiasedness check</h2>
  <ul>
    <li>From standard linear-regression distributional results,
      \(\dfrac{\mathrm{RSS}}{\sigma^2}\sim\chi^2_{\,n-2}\), so \(E[\mathrm{RSS}]=(n-2)\sigma^2\).</li>
    <li>Therefore
      <p>\[
        E[\hat\sigma^2_{\mathrm{MLE}}] = E\!\left[\frac{\mathrm{RSS}}{n}\right] = \frac{n-2}{n}\,\sigma^2.
      \]</p>
    </li>
    <li>Conclusion: \(\hat\sigma^2_{\mathrm{MLE}}=\mathrm{RSS}/n\) is biased downward. It underestimates \(\sigma^2\) by the factor \(\dfrac{n-2}{n}\).</li>
  </ul>
</li>

<li>
  <h2>Step 4 — unbiased alternative</h2>
  <ul>
    <li>The usual unbiased estimator is
      <p>\[
        \boxed{\displaystyle \tilde\sigma^2 = \frac{\mathrm{RSS}}{n-2},}
      \]</p>
      which satisfies \(E[\tilde\sigma^2]=\sigma^2\).</li>
    <li>Note: for a general linear model with \(p\) parameters (including intercept), the unbiased estimator is \(\mathrm{RSS}/(n-p)\) while the MLE is \(\mathrm{RSS}/n\).</li>
  </ul>
</li>
```

  </ul>

  <h2>Related concepts</h2>
  <ul>
    <li><strong>Bias of variance MLE.</strong> In normal models with unknown mean parameters, the ML estimator of variance is downward-biased because degrees of freedom are used to estimate the mean(s).</li>
    <li><strong>Degrees of freedom.</strong> In simple linear regression with intercept and slope, \(p=2\) parameters are estimated, leaving \(n-2\) residual degrees of freedom.</li>
    <li><strong>Distributional result.</strong> \(\dfrac{\mathrm{RSS}}{\sigma^2}\sim\chi^2_{n-2}\), which yields both expectation and confidence-interval constructions for \(\sigma^2\).</li>
    <li><strong>Asymptotics.</strong> \(\hat\sigma^2_{\mathrm{MLE}}\) is consistent and asymptotically unbiased; the finite-sample bias vanishes as \(n\to\infty\).</li>
  </ul>

  <h2>Viz</h2>
  <ul>
    <li>Plot \(\hat\sigma^2_{\mathrm{MLE}}=\mathrm{RSS}/n\) vs \(\tilde\sigma^2=\mathrm{RSS}/(n-2)\) as \(n\) varies: for small \(n\) the ML estimator is noticeably smaller; for large \(n\) they converge.</li>
    <li>Plot the sampling distribution of \(\dfrac{n\hat\sigma^2}{\sigma^2}=\dfrac{\mathrm{RSS}}{\sigma^2}\) which follows \(\chi^2_{n-2}\); compare to the scaled unbiased version.\</li>
  </ul>

  <h2>Other worthwhile points</h2>
  <ul>
    <li>Statistical software typically reports the unbiased estimator \(\mathrm{RSS}/(n-p)\) for inference (t-tests, CI), because those procedures rely on the exact finite-sample \(\chi^2/\)Student-\(t\) calibrations.</li>
    <li>For likelihood-based comparisons (likelihood-ratio tests, AIC), the ML estimator \(\mathrm{RSS}/n\) often appears naturally because it maximizes the likelihood.</li>
    <li>Both estimators are consistent; difference matters mainly for small samples and for reporting unbiased variance estimates.</li>
  </ul>
</section>

  </div>
























<div class="glow-box" id="6">
  <section>
  <h2>Question 6</h2>
  <p>
    Consider paired data \((X_i,Y_i),\ i=1,\dots,n\), independent, the marginal distribution of the \(X_i\) unspecified, and the conditional density of \(Y_i\) given \(X_i\) is
  </p>
  <p>\[
    f(y\mid X_i,\lambda,\alpha,\beta)=\frac{\lambda}{2}\exp\!\big(-\lambda|y-\alpha-\beta X_i|\big),\qquad -\infty<y<\infty,
  \]</p>
  <p>(i.e. a Laplace / double-exponential conditional distribution with scale parameter \(\lambda^{-1}\)). Show that the maximum likelihood estimators (MLEs) of \(\alpha\) and \(\beta\) minimize</p>
  <p>\[
    \sum_{i=1}^n |Y_i-\alpha-\beta X_i|.
  \]</p>

  <h2>Detailed solution</h2>
  <ul>
    <li>
      <h2>Step 1 — conditional likelihood (treat \(x_i\) as given)</h2>
      <p>
        The conditional likelihood for \((\lambda,\alpha,\beta)\) is
      </p>
      <p>\[
        L(\lambda,\alpha,\beta)
        =\prod_{i=1}^n \frac{\lambda}{2}\exp\!\big(-\lambda|y_i-\alpha-\beta x_i|\big)
        =\Big(\frac{\lambda}{2}\Big)^n \exp\!\Big(-\lambda\sum_{i=1}^n|y_i-\alpha-\beta x_i|\Big).
      \]</p>
    </li>

```
<li>
  <h2>Step 2 — log-likelihood</h2>
  <p>
    The log-likelihood is
  </p>
  <p>\[
    \ell(\lambda,\alpha,\beta)
    = n\log\lambda - n\log 2 \;-\; \lambda\sum_{i=1}^n|y_i-\alpha-\beta x_i|.
  \]</p>
</li>

<li>
  <h2>Step 3 — maximize w.r.t. \(\alpha,\beta\)</h2>
  <ul>
    <li>
      For fixed \((\alpha,\beta)\) the log-likelihood as a function of \(\lambda\) is
      <p>\[
        \ell(\lambda\mid\alpha,\beta)=n\log\lambda - \lambda S(\alpha,\beta) + \text{constant},
        \qquad S(\alpha,\beta)=\sum_{i=1}^n|y_i-\alpha-\beta x_i|.
      \]</p>
    </li>
    <li>
      Differentiating w.r.t. \(\lambda\) and equating to zero gives
      <p>\[
        \frac{n}{\hat\lambda}-S(\alpha,\beta)=0\quad\Rightarrow\quad \hat\lambda(\alpha,\beta)=\frac{n}{S(\alpha,\beta)}.
      \]</p>
    </li>
    <li>
      Plugging \(\hat\lambda\) back yields the profile log-likelihood (up to constants)
      <p>\[
        \ell_{\text{prof}}(\alpha,\beta) = n\log\!\Big(\frac{n}{S(\alpha,\beta)}\Big) - n\log 2 - n
        = -\,n\log S(\alpha,\beta) + \text{constant}.
      \]</p>
    </li>
    <li>
      Since \(-n\log S(\alpha,\beta)\) is a strictly monotone transformation of \(S(\alpha,\beta)>0\), maximizing \(\ell_{\text{prof}}\) over \((\alpha,\beta)\) is equivalent to minimizing
      <p>\[
        S(\alpha,\beta)=\sum_{i=1}^n |y_i-\alpha-\beta x_i|.
      \]</p>
    </li>
  </ul>
</li>

<li>
  <h2>Step 4 — conclusion</h2>
  <p>
    Therefore the MLEs \((\hat\alpha,\hat\beta)\) are exactly the minimizers of the sum of absolute residuals:
  </p>
  <p>\[
    (\hat\alpha,\hat\beta)
    = \arg\min_{(\alpha,\beta)}\sum_{i=1}^n |Y_i-\alpha-\beta X_i|.
  \]</p>
  <p>These are the least absolute deviations (LAD) or L1 regression estimators (equivalently median regression for the conditional median).</p>
</li>

<li>
  <h2>Optional — subgradient (first-order) conditions</h2>
  <p>
    Let \(r_i(\alpha,\beta)=y_i-\alpha-\beta x_i\) and \(\operatorname{sgn}(u)\) be the sign function with \(\operatorname{sgn}(0)\in[-1,1]\). A minimizer \((\hat\alpha,\hat\beta)\) satisfies the subgradient equations
  </p>
  <p>\[
    \sum_{i=1}^n \operatorname{sgn}\big(r_i(\hat\alpha,\hat\beta)\big)=0,\qquad
    \sum_{i=1}^n x_i\,\operatorname{sgn}\big(r_i(\hat\alpha,\hat\beta)\big)=0,
  \]</p>
  <p>
    where equalities are interpreted in the subgradient sense if some residuals equal zero. These are the LAD analogues of the normal equations.
  </p>
</li>
```

  </ul>

  <h2>Related concepts</h2>
  <ul>
    <li><strong>Least absolute deviations (LAD) / L1 regression:</strong> Minimizes \(\sum|r_i|\); robust to outliers in \(Y\).</li>
    <li><strong>Median / quantile regression:</strong> LAD is the special case of quantile regression at the 0.5 quantile (conditional median).</li>
    <li><strong>MLE equivalence:</strong> The Laplace conditional density makes the negative log-likelihood proportional to the L1 loss, hence MLE = LAD.</li>
    <li><strong>Subgradient vs differentiability:</strong> Because \(|\cdot|\) is nondifferentiable at 0, optimality uses subgradients rather than ordinary gradients.</li>
    <li><strong>Uniqueness:</strong> The LAD minimizer may not be unique in degenerate data configurations; with \(x_i\) in general position uniqueness typically holds.</li>
  </ul>

  <h2>Viz</h2>
  <ul>
    <li>Scatter plot \((x_i,y_i)\) with both OLS and LAD fit lines to demonstrate robustness: LAD resists vertical outliers.</li>
    <li>Surface plot of \(S(\alpha,\beta)\) over a grid to visualize kinks where residual signs change; minima occur at intersections of these kinked regions.</li>
    <li>Sign-balance plot at the LAD fit: the residual signs should balance so the sum of signs (and the weighted sum by \(x_i\)) is approximately zero.</li>
    <li>Contamination experiment: simulate outliers and compare OLS vs LAD fits to show trade-offs between efficiency and robustness.</li>
  </ul>

  <h2>Other worthwhile points</h2>
  <ul>
    <li><strong>Computation:</strong> LAD can be computed by linear programming (introduce positive/negative parts for residuals), by specialized algorithms (simplex, interior-point), or by IRLS approximations. Many packages implement quantile regression (e.g. <i>rq</i> in R) that computes LAD.</li>
    <li><strong>Asymptotics:</strong> LAD estimators are consistent and asymptotically normal under mild conditions; asymptotic variance depends on the error density at zero (for Laplace errors one gets closed-form expressions).</li>
    <li><strong>MLE for \(\lambda\):</strong> Given \((\hat\alpha,\hat\beta)\), the MLE for \(\lambda\) is
      <p>\[
        \hat\lambda=\frac{n}{\sum_{i=1}^n|y_i-\hat\alpha-\hat\beta x_i|}.
      \]</p>
    </li>
    <li><strong>Robustness trade-off:</strong> LAD has higher breakdown point and bounded influence compared with OLS, but is typically less efficient under Gaussian errors. Choose method with the error distribution and outlier risk in mind.</li>
  </ul>
</section>

</div>


























</section>

<button class="next-btn" onclick="location.href='wk2.html'">Next Section →</button>

<footer>
  <p>&copy; 2025 Shaan | Built with purpose</p>
</footer>
<script>
MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]']]
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

</body>
</html>
