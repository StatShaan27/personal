<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Assignment: Week-1 | Shaan</title>
  <link rel="stylesheet" href="../style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display&family=Inter:wght@300;500&display=swap" rel="stylesheet">
  <style>
    body {
      background-color: #121212;
      color: #e0e0e0;
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
    }

    h1, h2, h3, h4 {
      font-family: 'Playfair Display', serif;
      color: #6effe0;
    }

    header {
      text-align: center;
      padding: 2rem 1rem 1rem;
      background: #1e1e1e;
      border-bottom: 2px solid #333;
    }

    header nav ul {
      list-style: none;
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 1rem;
      padding: 0;
    }

    header nav ul li {
      display: inline;
    }

    header nav ul li a {
      text-decoration: none;
      color: #6effe0;
      font-weight: 500;
      border: 1px solid transparent;
      padding: 0.4rem 0.8rem;
      border-radius: 6px;
      transition: all 0.3s ease;
    }

    header nav ul li a:hover {
      border-color: #6effe0;
      background-color: #6effe0;
      color: #000;
    }

    .notes-wrapper {
      max-width: 900px;
      margin: 3rem auto;
      padding: 2rem;
      animation: fadeIn 1.2s ease;
    }

    .info-block {
      margin-bottom: 2rem;
      text-align: left;
      line-height: 1.6;
    }

    .info-block strong {
      color: #6effe0;
    }

    .toc {
      background-color: #1e1e1e;
      border-left: 4px solid #6effe0;
      padding: 1rem 1.5rem;
      margin-bottom: 2rem;
      border-radius: 8px;
    }

    .toc ul {
      padding-left: 1rem;
    }

    .toc ul li {
      padding: 0.3rem 0;
    }

    .toc a {
      color: #c9d1d9;
      text-decoration: none;
      border-bottom: 1px dashed #6effe0;
    }

    .toc a:hover {
      color: #6effe0;
    }

    .glow-box {
      border: 2px solid #6effe0;
      box-shadow: 0 0 20px rgba(110, 255, 224, 0.2);
      border-radius: 16px;
      padding: 2rem;
      margin-bottom: 2rem;
      background-color: #1e1e1e;
      animation: pulse 3s infinite ease-in-out;
    }

    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 20px rgba(110, 255, 224, 0.2); }
      50% { box-shadow: 0 0 35px rgba(110, 255, 224, 0.4); }
    }

    ul {
      line-height: 1.6;
      padding-left: 1.2rem;
    }

    code, pre {
      background-color: #222;
      padding: 0.5rem 1rem;
      border-radius: 8px;
      font-family: monospace;
      color: #6effe0;
      display: block;
      overflow-x: auto;
    }

    .next-btn {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background-color: #6effe0;
      color: #000;
      padding: 0.6rem 1.2rem;
      border: none;
      border-radius: 8px;
      font-weight: bold;
      cursor: pointer;
      box-shadow: 0 0 10px #6effe0;
      transition: background-color 0.3s ease;
      z-index: 100;
    }

    .next-btn:hover {
      background-color: #00ffcc;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 2rem 1rem;
    }
  </style>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</head>
<body>

<header>
  <h1>Statistical Methods III: Assignment Week-1</h1>
  <p>AY 2025‚Äì26</p>
  <nav>
    <ul>
      <li><a href="../../index.html">Home</a></li>
      <li><a href="../../assets/Shaan_CV.pdf">CV</a></li>
      <li><a href="../../sections/projects.html">Projects</a></li>
      <li><a href="../../sections/education.html">Education</a></li>
      <li><a href="../../sections/misc.html">Misc</a></li>
    </ul>
  </nav>
</header>

<section class="notes-wrapper">

  <div class="info-block">
    <p><strong>Instructor:</strong> Debasis Sengupta </p>
    <p><strong>Office / Department:</strong> ASU</p>
    <p><strong>Email:</strong> sdebasis@isical.ac.in</p>
    <p><strong>Marking Scheme:</strong><br>
      Assignments: 20% | Midterm Test: 30% | End Semester: 50%
    </p>
  </div>

  <div class="toc">
    <h3>Contents</h3>
   <ul>
  <li><a href="#1">Question 1</a></li>
  <li><a href="#2">Question 2</a></li>
  <li><a href="#3">Question 3</a></li>
  <li><a href="#4">Question 4</a></li>
  <li><a href="#5">Question 5</a></li>
  <li><a href="#6">Question 6</a></li>
  <li><a href="#7">Question 7</a></li>
  <li><a href="#8">Question 8</a></li>
  <li><a href="#9">Question 9</a></li>
  <li><a href="#10">Question 10</a></li>
  <li><a href="#11">Question 11</a></li>
</ul>
  </div>

  <!-- Notes section boxes -->
  <div class="glow-box" id="1">
    <h4>Question:</h4>
<p>
Let \( X_1, X_2, \ldots, X_n \) be i.i.d. random variables with pdf
\[
f(x \mid \theta) = \frac{x^{1/\theta - 1}}{\theta}, \quad 0 \le x \le 1,\ \theta > 0.
\]
Is the <strong>maximum likelihood estimator (MLE)</strong> of \( \theta \) <strong>unbiased</strong>?
</p>

<h4>Solution:</h4>
<p>
The likelihood is:
\[
L(\theta) = \frac{1}{\theta^n} \left( \prod_{i=1}^n X_i \right)^{1/\theta - 1}
\quad \Rightarrow \quad
\ell(\theta) = -n \log \theta + \left( \frac{1}{\theta} - 1 \right) \sum \log X_i
\]

Differentiate:
\[
\frac{d\ell}{d\theta} = -\frac{n}{\theta} - \frac{S}{\theta^2}
\quad \Rightarrow \quad
\hat{\theta}_{\text{MLE}} = -\frac{1}{n} \sum \log X_i
\]

To check unbiasedness:
\[
\mathbb{E}[\hat{\theta}_{\text{MLE}}] = -\mathbb{E}[\log X]
= - \frac{1}{\theta} \int_0^1 x^{1/\theta - 1} \log x\, dx
= - \frac{1}{\theta} \cdot (-\theta^2) = \theta
\]

‚úîÔ∏è <strong>The MLE is unbiased.</strong>
</p>

<h4>Related concepts / Other stuff:</h4>
<ul>
  <li><strong>MLE (Maximum Likelihood Estimation):</strong> Estimator derived by maximizing the likelihood function.</li>
  <li><strong>Unbiased Estimator:</strong> Satisfies \( \mathbb{E}[\hat{\theta}] = \theta \).</li>
  <li><strong>Useful identity:</strong> \( \int_0^1 x^{a - 1} \log x\, dx = -1/a^2 \).</li>
  <li><strong>Transformation Insight:</strong> \( -\log X \sim \text{Exp}(1/\theta) \) is key to understanding the unbiasedness.</li>
  <li><strong>Rare Case:</strong> MLE here is unbiased for all \( n \), not just asymptotically.</li>
</ul>

  </div>












    <div class="glow-box" id="2">
      <h4>Question:</h4>
<p>
In the previous problem, we found that the MLE of \( \theta \) is:
</p>
<p>
\[
\hat{\theta}_{\text{MLE}} = -\frac{1}{n} \sum_{i=1}^{n} \log X_i.
\]
</p>
<p>
<strong>What is the variance of \( \hat{\theta}_{\text{MLE}} \)?</strong><br>
<strong>Does the variance go to zero as \( n \to \infty \)?</strong>
</p>

<h4>Solution:</h4>

<p><strong>Step 1: Recall the MLE expression</strong></p>
<p>
\[
\hat{\theta}_{\text{MLE}} = -\frac{1}{n} \sum_{i=1}^{n} \log X_i.
\]
Define \( Y_i = -\log X_i \). Then:
\[
\hat{\theta}_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^n Y_i.
\]
So,
\[
\text{Var}(\hat{\theta}_{\text{MLE}}) = \frac{1}{n^2} \sum_{i=1}^n \text{Var}(Y_i) = \frac{1}{n} \text{Var}(Y_1).
\]
</p>

<p><strong>Step 2: Distribution of \( Y = -\log X \)</strong></p>
<p>
Given: \( X \sim f(x|\theta) = \frac{x^{1/\theta - 1}}{\theta},\ 0 \le x \le 1 \)<br>
Let \( X = e^{-Y} \), then \( dx = -e^{-Y} dY \)

\[
f_Y(y) = f_X(x(y)) \cdot \left| \frac{dx}{dy} \right| = \frac{e^{-(1/\theta - 1)y}}{\theta} \cdot e^{-y} = \frac{e^{-y/\theta}}{\theta}, \quad y \ge 0.
\]

Thus, \( Y = -\log X \sim \text{Exponential}\left( \frac{1}{\theta} \right) \)
</p>

<p><strong>Step 3: Mean and Variance of Exponential Distribution</strong></p>
<p>
If \( Y \sim \text{Exp}(\lambda) \), then:

<ul>
  <li>\( \mathbb{E}[Y] = \frac{1}{\lambda} \)</li>
  <li>\( \text{Var}(Y) = \frac{1}{\lambda^2} \)</li>
</ul>

Here \( \lambda = 1/\theta \), so:
<ul>
  <li>\( \mathbb{E}[Y] = \theta \)</li>
  <li>\( \text{Var}(Y) = \theta^2 \)</li>
</ul>

Thus,
\[
\text{Var}(\hat{\theta}_{\text{MLE}}) = \frac{1}{n} \theta^2
\]
</p>

<p><strong>Final Answer:</strong></p>
<p>
\[
\boxed{ \text{Var}(\hat{\theta}_{\text{MLE}}) = \frac{\theta^2}{n} }
\]
</p>

<p><strong>Does variance ‚Üí 0 as \( n \to \infty \)?</strong></p>
<p>
Yes ‚Äî clearly:
\[
\lim_{n \to \infty} \frac{\theta^2}{n} = 0
\]
So, the MLE is <strong>consistent</strong>.
</p>

<h4>Related concepts / Other stuff:</h4>
<ul>
  <li><strong>Variance of Sample Mean:</strong> If \( Y_1, \ldots, Y_n \) are i.i.d. with variance \( \sigma^2 \), then \( \text{Var}(\bar{Y}) = \frac{\sigma^2}{n} \)</li>
  <li><strong>Consistency of Estimators:</strong> An estimator \( \hat{\theta}_n \) is consistent if \( \hat{\theta}_n \xrightarrow{P} \theta \). True if unbiased and variance ‚Üí 0.</li>
</ul>

<h4>üß† Visual Intuition:</h4>
<p>
As \( n \) increases, the sample mean of i.i.d. exponential variables concentrates around the true mean \( \theta \). The variance shrinks as \( 1/n \), leading to a tighter distribution around \( \theta \).

Imagine a histogram of \( \hat{\theta}_{\text{MLE}} \) values from repeated sampling ‚Äî it gets narrower around \( \theta \) as \( n \) grows.
</p>

<h4>üí° Worth Mentioning:</h4>
<ul>
  <li>This is a rare case where the MLE is unbiased, has finite variance, and is consistent ‚Äî all in closed form.</li>
  <li>Transformation via \( -\log X \) is a classic estimation technique.</li>
  <li>This MLE is also asymptotically efficient (achieves CRLB).</li>
</ul>

    </div>






















  <div class="glow-box" id="3">
      <h4>Question:</h4>
<p>
Let \( X_1, X_2, \ldots, X_n \) be i.i.d. exponential random variables with mean \( \theta \), i.e.,
</p>
<p>
\[
f(x \mid \theta) = \frac{1}{\theta} e^{-x/\theta}, \quad x \ge 0.
\]
</p>
<p>
Find an <strong>unbiased estimator of \( \theta \)</strong> that depends <strong>only on \( \min\{X_1, X_2, \ldots, X_n\} \)</strong>.
</p>

<h4>Solution:</h4>

<p><strong>Step 1: Let‚Äôs define the statistic</strong></p>
<p>
Let:
\[
Y = \min\{X_1, X_2, \ldots, X_n\}
\]
We aim to find a constant \( c_n \) such that:
\[
\mathbb{E}[c_n Y] = \theta
\quad \Rightarrow \quad \text{then } c_n Y \text{ is unbiased.}
\]
So we must find \( c_n = \frac{1}{\mathbb{E}[Y]} \cdot \theta \).
</p>

<p><strong>Step 2: Distribution of the Minimum</strong></p>
<p>
Let \( Y = X_{(1)} = \min\{X_1, \ldots, X_n\} \).<br>
Then for exponential \( \text{Exp}(\theta) \), the minimum has the distribution:
\[
Y \sim \text{Exponential}\left(\frac{1}{\theta} \cdot n\right)
\]
That is:
\[
f_Y(y) = \frac{n}{\theta} e^{-ny/\theta}, \quad y \ge 0.
\]
So:
\[
\mathbb{E}[Y] = \frac{\theta}{n}
\]
</p>

<p><strong>Step 3: Construct Unbiased Estimator</strong></p>
<p>
We want:
\[
\mathbb{E}[c_n Y] = \theta \Rightarrow c_n \cdot \frac{\theta}{n} = \theta \Rightarrow c_n = n
\]
</p>

<p><strong>Final Answer:</strong></p>
<p>
\[
\boxed{ \hat{\theta} = n \cdot \min\{X_1, \ldots, X_n\} }
\]
This is an <strong>unbiased estimator</strong> of \( \theta \) using only the <strong>minimum</strong>.
</p>

<h4>Related concepts / Other stuff:</h4>

<ul>
  <li><strong>Exponential Distribution and Order Statistics:</strong>
    <ul>
      <li>The minimum of \( n \) i.i.d. \( \text{Exp}(\theta) \) is again exponential, with rate \( \frac{n}{\theta} \).</li>
      <li>This is because the exponential distribution has the <em>memoryless</em> property.</li>
    </ul>
  </li>
  <li><strong>Sufficiency and Completeness:</strong>
    <ul>
      <li>The full data \( X_1, \ldots, X_n \) gives the sufficient statistic \( \sum X_i \), but the minimum alone is not sufficient.</li>
      <li>Still, one can construct an unbiased estimator from a <em>function of a statistic</em>, even if it's not sufficient.</li>
    </ul>
  </li>
</ul>

<h4>üß† Visual Intuition:</h4>
<p>
Imagine sampling 100 exponential lifetimes, and just recording the <strong>first failure</strong> time. Since failures happen randomly, and exponentially, the <strong>first failure</strong> tends to occur <strong>faster</strong> than the average lifetime.
</p>
<p>
Hence, the minimum is <strong>biased low</strong>, but scaling it up by \( n \) compensates for this ‚Äî making it an <strong>unbiased estimate</strong>.
</p>
<blockquote>
Think of \( \min(X_1, \dots, X_n) \approx \frac{\theta}{n} \), so multiplying by \( n \) gets you back to \( \theta \).
</blockquote>

<h4>üí° Worth Mentioning:</h4>
<ul>
  <li>This estimator is not the most <strong>efficient</strong> (in terms of variance). The MLE \( \hat{\theta}_{\text{MLE}} = \bar{X} \) has <strong>lower variance</strong>.</li>
  <li>But this question is important in contexts where only <strong>partial information</strong> is available ‚Äî like censoring or online streaming.</li>
  <li>The estimator \( n \cdot X_{(1)} \) is <strong>minimal sufficient</strong> in some practical settings, like <strong>reliability theory</strong>.</li>
</ul>

    </div>























  <div class="glow-box" id="4">
      Excellent ‚Äî now we‚Äôre entering the core of **comparing estimators** using principles of **mean squared error (MSE)**, **variance**, and **efficiency**.

Let‚Äôs proceed systematically:

---

## ‚ùì Question

For $X_1, \dots, X_n \overset{iid}{\sim} \text{Exp}(\theta)$,
you‚Äôve already identified the estimator:

$$
\hat{\theta}_1 = n \cdot \min\{X_1, \dots, X_n\}
$$

which is unbiased.

> **Find a better estimator for $\theta$** than $\hat{\theta}_1$, and **prove** that it is better.

---

## ‚úÖ Detailed Solution

---

### üîπ Step 1: Known Facts

* The exponential distribution has:

  $$
  \mathbb{E}[X_i] = \theta, \quad \text{Var}(X_i) = \theta^2
  $$

* The **sample mean** is:

  $$
  \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
  $$

* Since $X_i \sim \text{Exp}(\theta)$, the sum $T = \sum X_i \sim \text{Gamma}(n, \theta)$.

Thus:

* $\hat{\theta}_2 := \bar{X} = \frac{1}{n} \sum X_i$ is:

  * **Unbiased**, since $\mathbb{E}[\bar{X}] = \theta$
  * **MLE**, because the exponential family is regular
  * **Minimum variance unbiased estimator (MVUE)**, because it's based on a **complete sufficient statistic**

---

### üîπ Step 2: Variance Comparison

#### Estimator 1 (Based on Minimum):

$$
\hat{\theta}_1 = n X_{(1)}, \quad \text{where } X_{(1)} = \min(X_i)
$$

We know:

$$
X_{(1)} \sim \text{Exp}\left(\frac{n}{\theta}\right)
\Rightarrow \mathbb{E}[X_{(1)}] = \frac{\theta}{n},\quad \text{Var}(X_{(1)}) = \frac{\theta^2}{n^2}
$$

So:

$$
\text{Var}(\hat{\theta}_1) = \text{Var}(n X_{(1)}) = n^2 \cdot \frac{\theta^2}{n^2} = \theta^2
$$

#### Estimator 2 (Sample Mean):

$$
\hat{\theta}_2 = \bar{X},\quad \text{Var}(\bar{X}) = \frac{\theta^2}{n}
$$

---

### üîé Comparison

* $\text{Var}(\hat{\theta}_1) = \theta^2$
* $\text{Var}(\hat{\theta}_2) = \frac{\theta^2}{n}$

Since $n > 1$, we have:

$$
\text{Var}(\hat{\theta}_2) < \text{Var}(\hat{\theta}_1)
\Rightarrow \hat{\theta}_2 \text{ is strictly better in variance}
$$

Both estimators are **unbiased**, so **lower variance ‚áí lower MSE**.

---

### ‚úÖ Final Answer

$$
\boxed{
\text{The sample mean } \hat{\theta}_2 = \bar{X} \text{ is a better estimator than } \hat{\theta}_1 = n X_{(1)}.
}
$$

‚úî It is **unbiased**,
‚úî Has **lower variance**,
‚úî And is the **MLE and MVUE**.

---

## üìå Related Concepts

### 1. **Sufficient and Complete Statistic**

* $T = \sum X_i$ is sufficient and complete for $\theta$ in exponential families.
* Any unbiased function of $T$ (like $\bar{X}$) is the **unique MVUE** by the **Lehmann‚ÄìScheff√© theorem**.

### 2. **Cram√©r‚ÄìRao Lower Bound (CRLB)**

For exponential:

$$
I(\theta) = \frac{n}{\theta^2}, \quad \Rightarrow \text{CRLB} = \frac{1}{I(\theta)} = \frac{\theta^2}{n}
$$

Since:

$$
\text{Var}(\bar{X}) = \frac{\theta^2}{n} = \text{CRLB}
$$

‚Üí So $\bar{X}$ is **efficient**.

---

## üß† Visual Intuition

> Imagine observing only the first failure in a system of $n$ light bulbs ‚Äî this is $\hat{\theta}_1$.
> Now imagine observing **all failure times** and averaging them ‚Äî this is $\hat{\theta}_2$.

Clearly, the second gives more information ‚Äî so naturally, it leads to a better estimator.

---

## üí° Worth Mentioning

* $\hat{\theta}_1 = n X_{(1)}$ is **not sufficient**, so by the **Rao-Blackwell theorem**, we can improve it by conditioning on the sufficient statistic $\sum X_i$\*\* ‚Äî and the result is $\bar{X}$.
* This is an example of **using partial information vs full information**.
* In data-limited scenarios (like censored data), $X_{(1)}$ may be all you have ‚Äî so even if suboptimal, it‚Äôs **practically useful**.

---

Would you like me to show the **Rao-Blackwell improvement** of $\hat{\theta}_1$ to derive $\hat{\theta}_2$ as the MVUE?

    </div>

























  <div class="glow-box" id="12">
      
    </div>
























  <div class="glow-box" id="12">
      
    </div>






















  <div class="glow-box" id="12">
      
    </div>

































  <div class="glow-box" id="12">
      
    </div>






































  <div class="glow-box" id="12">
      
    </div>































  <div class="glow-box" id="12">
      
    </div>

































  <div class="glow-box" id="12">
      
    </div>






























  <div class="glow-box" id="12">
      
    </div>
































  
</section>

<button class="next-btn" onclick="location.href='wk2.html'">Next Section ‚Üí</button>

<footer>
  <p>&copy; 2025 Shaan | Built with purpose</p>
</footer>

</body>
</html>
