<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Assignment: Week-1 | Shaan</title>
  <link rel="stylesheet" href="../style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display&family=Inter:wght@300;500&display=swap" rel="stylesheet">
  <style>
    body {
      background-color: #121212;
      color: #e0e0e0;
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
    }

    h1, h2, h3, h4 {
      font-family: 'Playfair Display', serif;
      color: #6effe0;
    }

    header {
      text-align: center;
      padding: 2rem 1rem 1rem;
      background: #1e1e1e;
      border-bottom: 2px solid #333;
    }

    header nav ul {
      list-style: none;
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 1rem;
      padding: 0;
    }

    header nav ul li {
      display: inline;
    }

    header nav ul li a {
      text-decoration: none;
      color: #6effe0;
      font-weight: 500;
      border: 1px solid transparent;
      padding: 0.4rem 0.8rem;
      border-radius: 6px;
      transition: all 0.3s ease;
    }

    header nav ul li a:hover {
      border-color: #6effe0;
      background-color: #6effe0;
      color: #000;
    }

    .notes-wrapper {
      max-width: 900px;
      margin: 3rem auto;
      padding: 2rem;
      animation: fadeIn 1.2s ease;
    }

    .info-block {
      margin-bottom: 2rem;
      text-align: left;
      line-height: 1.6;
    }

    .info-block strong {
      color: #6effe0;
    }

    .toc {
      background-color: #1e1e1e;
      border-left: 4px solid #6effe0;
      padding: 1rem 1.5rem;
      margin-bottom: 2rem;
      border-radius: 8px;
    }

    .toc ul {
      padding-left: 1rem;
    }

    .toc ul li {
      padding: 0.3rem 0;
    }

    .toc a {
      color: #c9d1d9;
      text-decoration: none;
      border-bottom: 1px dashed #6effe0;
    }

    .toc a:hover {
      color: #6effe0;
    }

    .glow-box {
      border: 2px solid #6effe0;
      box-shadow: 0 0 20px rgba(110, 255, 224, 0.2);
      border-radius: 16px;
      padding: 2rem;
      margin-bottom: 2rem;
      background-color: #1e1e1e;
      animation: pulse 3s infinite ease-in-out;
    }

    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 20px rgba(110, 255, 224, 0.2); }
      50% { box-shadow: 0 0 35px rgba(110, 255, 224, 0.4); }
    }

    ul {
      line-height: 1.6;
      padding-left: 1.2rem;
    }

    code, pre {
      background-color: #222;
      padding: 0.5rem 1rem;
      border-radius: 8px;
      font-family: monospace;
      color: #6effe0;
      display: block;
      overflow-x: auto;
    }

    .next-btn {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background-color: #6effe0;
      color: #000;
      padding: 0.6rem 1.2rem;
      border: none;
      border-radius: 8px;
      font-weight: bold;
      cursor: pointer;
      box-shadow: 0 0 10px #6effe0;
      transition: background-color 0.3s ease;
      z-index: 100;
    }

    .next-btn:hover {
      background-color: #00ffcc;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 2rem 1rem;
    }
  </style>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</head>
<body>

<header>
  <h1>Statistical Methods III: Assignment Week-1</h1>
  <p>AY 2025‚Äì26</p>
  <nav>
    <ul>
      <li><a href="../../index.html">Home</a></li>
      <li><a href="../../assets/Shaan_CV.pdf">CV</a></li>
      <li><a href="../../sections/projects.html">Projects</a></li>
      <li><a href="../../sections/education.html">Education</a></li>
      <li><a href="../../sections/misc.html">Misc</a></li>
    </ul>
  </nav>
</header>

<section class="notes-wrapper">

  <div class="info-block">
    <p><strong>Instructor:</strong> Debasis Sengupta </p>
    <p><strong>Office / Department:</strong> ASU</p>
    <p><strong>Email:</strong> sdebasis@isical.ac.in</p>
    <p><strong>Marking Scheme:</strong><br>
      Assignments: 20% | Midterm Test: 30% | End Semester: 50%
    </p>
  </div>

  <div class="toc">
    <h3>Contents</h3>
   <ul>
  <li><a href="#1">Question 1</a></li>
  <li><a href="#2">Question 2</a></li>
  <li><a href="#3">Question 3</a></li>
  <li><a href="#4">Question 4</a></li>
  <li><a href="#5">Question 5</a></li>
  <li><a href="#6">Question 6</a></li>
  <li><a href="#7">Question 7</a></li>
  <li><a href="#8">Question 8</a></li>
  <li><a href="#9">Question 9</a></li>
  <li><a href="#10">Question 10</a></li>
  <li><a href="#11">Question 11</a></li>
</ul>
  </div>

  <!-- Notes section boxes -->
  <div class="glow-box" id="1">
    <h4>Question:</h4>
<p>
Let \( X_1, X_2, \ldots, X_n \) be i.i.d. random variables with pdf
\[
f(x \mid \theta) = \frac{x^{1/\theta - 1}}{\theta}, \quad 0 \le x \le 1,\ \theta > 0.
\]
Is the <strong>maximum likelihood estimator (MLE)</strong> of \( \theta \) <strong>unbiased</strong>?
</p>

<h4>Solution:</h4>
<p>
The likelihood is:
\[
L(\theta) = \frac{1}{\theta^n} \left( \prod_{i=1}^n X_i \right)^{1/\theta - 1}
\quad \Rightarrow \quad
\ell(\theta) = -n \log \theta + \left( \frac{1}{\theta} - 1 \right) \sum \log X_i
\]

Differentiate:
\[
\frac{d\ell}{d\theta} = -\frac{n}{\theta} - \frac{S}{\theta^2}
\quad \Rightarrow \quad
\hat{\theta}_{\text{MLE}} = -\frac{1}{n} \sum \log X_i
\]

To check unbiasedness:
\[
\mathbb{E}[\hat{\theta}_{\text{MLE}}] = -\mathbb{E}[\log X]
= - \frac{1}{\theta} \int_0^1 x^{1/\theta - 1} \log x\, dx
= - \frac{1}{\theta} \cdot (-\theta^2) = \theta
\]

‚úîÔ∏è <strong>The MLE is unbiased.</strong>
</p>

<h4>Related concepts / Other stuff:</h4>
<ul>
  <li><strong>MLE (Maximum Likelihood Estimation):</strong> Estimator derived by maximizing the likelihood function.</li>
  <li><strong>Unbiased Estimator:</strong> Satisfies \( \mathbb{E}[\hat{\theta}] = \theta \).</li>
  <li><strong>Useful identity:</strong> \( \int_0^1 x^{a - 1} \log x\, dx = -1/a^2 \).</li>
  <li><strong>Transformation Insight:</strong> \( -\log X \sim \text{Exp}(1/\theta) \) is key to understanding the unbiasedness.</li>
  <li><strong>Rare Case:</strong> MLE here is unbiased for all \( n \), not just asymptotically.</li>
</ul>

  </div>












    <div class="glow-box" id="2">
      <h4>Question:</h4>
<p>
In the previous problem, we found that the MLE of \( \theta \) is:
</p>
<p>
\[
\hat{\theta}_{\text{MLE}} = -\frac{1}{n} \sum_{i=1}^{n} \log X_i.
\]
</p>
<p>
<strong>What is the variance of \( \hat{\theta}_{\text{MLE}} \)?</strong><br>
<strong>Does the variance go to zero as \( n \to \infty \)?</strong>
</p>

<h4>Solution:</h4>

<p><strong>Step 1: Recall the MLE expression</strong></p>
<p>
\[
\hat{\theta}_{\text{MLE}} = -\frac{1}{n} \sum_{i=1}^{n} \log X_i.
\]
Define \( Y_i = -\log X_i \). Then:
\[
\hat{\theta}_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^n Y_i.
\]
So,
\[
\text{Var}(\hat{\theta}_{\text{MLE}}) = \frac{1}{n^2} \sum_{i=1}^n \text{Var}(Y_i) = \frac{1}{n} \text{Var}(Y_1).
\]
</p>

<p><strong>Step 2: Distribution of \( Y = -\log X \)</strong></p>
<p>
Given: \( X \sim f(x|\theta) = \frac{x^{1/\theta - 1}}{\theta},\ 0 \le x \le 1 \)<br>
Let \( X = e^{-Y} \), then \( dx = -e^{-Y} dY \)

\[
f_Y(y) = f_X(x(y)) \cdot \left| \frac{dx}{dy} \right| = \frac{e^{-(1/\theta - 1)y}}{\theta} \cdot e^{-y} = \frac{e^{-y/\theta}}{\theta}, \quad y \ge 0.
\]

Thus, \( Y = -\log X \sim \text{Exponential}\left( \frac{1}{\theta} \right) \)
</p>

<p><strong>Step 3: Mean and Variance of Exponential Distribution</strong></p>
<p>
If \( Y \sim \text{Exp}(\lambda) \), then:

<ul>
  <li>\( \mathbb{E}[Y] = \frac{1}{\lambda} \)</li>
  <li>\( \text{Var}(Y) = \frac{1}{\lambda^2} \)</li>
</ul>

Here \( \lambda = 1/\theta \), so:
<ul>
  <li>\( \mathbb{E}[Y] = \theta \)</li>
  <li>\( \text{Var}(Y) = \theta^2 \)</li>
</ul>

Thus,
\[
\text{Var}(\hat{\theta}_{\text{MLE}}) = \frac{1}{n} \theta^2
\]
</p>

<p><strong>Final Answer:</strong></p>
<p>
\[
\boxed{ \text{Var}(\hat{\theta}_{\text{MLE}}) = \frac{\theta^2}{n} }
\]
</p>

<p><strong>Does variance ‚Üí 0 as \( n \to \infty \)?</strong></p>
<p>
Yes ‚Äî clearly:
\[
\lim_{n \to \infty} \frac{\theta^2}{n} = 0
\]
So, the MLE is <strong>consistent</strong>.
</p>

<h4>Related concepts / Other stuff:</h4>
<ul>
  <li><strong>Variance of Sample Mean:</strong> If \( Y_1, \ldots, Y_n \) are i.i.d. with variance \( \sigma^2 \), then \( \text{Var}(\bar{Y}) = \frac{\sigma^2}{n} \)</li>
  <li><strong>Consistency of Estimators:</strong> An estimator \( \hat{\theta}_n \) is consistent if \( \hat{\theta}_n \xrightarrow{P} \theta \). True if unbiased and variance ‚Üí 0.</li>
</ul>

<h4>üß† Visual Intuition:</h4>
<p>
As \( n \) increases, the sample mean of i.i.d. exponential variables concentrates around the true mean \( \theta \). The variance shrinks as \( 1/n \), leading to a tighter distribution around \( \theta \).

Imagine a histogram of \( \hat{\theta}_{\text{MLE}} \) values from repeated sampling ‚Äî it gets narrower around \( \theta \) as \( n \) grows.
</p>

<h4>üí° Worth Mentioning:</h4>
<ul>
  <li>This is a rare case where the MLE is unbiased, has finite variance, and is consistent ‚Äî all in closed form.</li>
  <li>Transformation via \( -\log X \) is a classic estimation technique.</li>
  <li>This MLE is also asymptotically efficient (achieves CRLB).</li>
</ul>

    </div>






















  <div class="glow-box" id="3">
      <h4>Question:</h4>
<p>
Let \( X_1, X_2, \ldots, X_n \) be i.i.d. exponential random variables with mean \( \theta \), i.e.,
</p>
<p>
\[
f(x \mid \theta) = \frac{1}{\theta} e^{-x/\theta}, \quad x \ge 0.
\]
</p>
<p>
Find an <strong>unbiased estimator of \( \theta \)</strong> that depends <strong>only on \( \min\{X_1, X_2, \ldots, X_n\} \)</strong>.
</p>

<h4>Solution:</h4>

<p><strong>Step 1: Let‚Äôs define the statistic</strong></p>
<p>
Let:
\[
Y = \min\{X_1, X_2, \ldots, X_n\}
\]
We aim to find a constant \( c_n \) such that:
\[
\mathbb{E}[c_n Y] = \theta
\quad \Rightarrow \quad \text{then } c_n Y \text{ is unbiased.}
\]
So we must find \( c_n = \frac{1}{\mathbb{E}[Y]} \cdot \theta \).
</p>

<p><strong>Step 2: Distribution of the Minimum</strong></p>
<p>
Let \( Y = X_{(1)} = \min\{X_1, \ldots, X_n\} \).<br>
Then for exponential \( \text{Exp}(\theta) \), the minimum has the distribution:
\[
Y \sim \text{Exponential}\left(\frac{1}{\theta} \cdot n\right)
\]
That is:
\[
f_Y(y) = \frac{n}{\theta} e^{-ny/\theta}, \quad y \ge 0.
\]
So:
\[
\mathbb{E}[Y] = \frac{\theta}{n}
\]
</p>

<p><strong>Step 3: Construct Unbiased Estimator</strong></p>
<p>
We want:
\[
\mathbb{E}[c_n Y] = \theta \Rightarrow c_n \cdot \frac{\theta}{n} = \theta \Rightarrow c_n = n
\]
</p>

<p><strong>Final Answer:</strong></p>
<p>
\[
\boxed{ \hat{\theta} = n \cdot \min\{X_1, \ldots, X_n\} }
\]
This is an <strong>unbiased estimator</strong> of \( \theta \) using only the <strong>minimum</strong>.
</p>

<h4>Related concepts / Other stuff:</h4>

<ul>
  <li><strong>Exponential Distribution and Order Statistics:</strong>
    <ul>
      <li>The minimum of \( n \) i.i.d. \( \text{Exp}(\theta) \) is again exponential, with rate \( \frac{n}{\theta} \).</li>
      <li>This is because the exponential distribution has the <em>memoryless</em> property.</li>
    </ul>
  </li>
  <li><strong>Sufficiency and Completeness:</strong>
    <ul>
      <li>The full data \( X_1, \ldots, X_n \) gives the sufficient statistic \( \sum X_i \), but the minimum alone is not sufficient.</li>
      <li>Still, one can construct an unbiased estimator from a <em>function of a statistic</em>, even if it's not sufficient.</li>
    </ul>
  </li>
</ul>

<h4>üß† Visual Intuition:</h4>
<p>
Imagine sampling 100 exponential lifetimes, and just recording the <strong>first failure</strong> time. Since failures happen randomly, and exponentially, the <strong>first failure</strong> tends to occur <strong>faster</strong> than the average lifetime.
</p>
<p>
Hence, the minimum is <strong>biased low</strong>, but scaling it up by \( n \) compensates for this ‚Äî making it an <strong>unbiased estimate</strong>.
</p>
<blockquote>
Think of \( \min(X_1, \dots, X_n) \approx \frac{\theta}{n} \), so multiplying by \( n \) gets you back to \( \theta \).
</blockquote>

<h4>üí° Worth Mentioning:</h4>
<ul>
  <li>This estimator is not the most <strong>efficient</strong> (in terms of variance). The MLE \( \hat{\theta}_{\text{MLE}} = \bar{X} \) has <strong>lower variance</strong>.</li>
  <li>But this question is important in contexts where only <strong>partial information</strong> is available ‚Äî like censoring or online streaming.</li>
  <li>The estimator \( n \cdot X_{(1)} \) is <strong>minimal sufficient</strong> in some practical settings, like <strong>reliability theory</strong>.</li>
</ul>

    </div>























  <div class="glow-box" id="4">
      Excellent ‚Äî now we‚Äôre entering the core of **comparing estimators** using principles of **mean squared error (MSE)**, **variance**, and **efficiency**.

Let‚Äôs proceed systematically:

---

## ‚ùì Question

For $X_1, \dots, X_n \overset{iid}{\sim} \text{Exp}(\theta)$,
you‚Äôve already identified the estimator:

$$
\hat{\theta}_1 = n \cdot \min\{X_1, \dots, X_n\}
$$

which is unbiased.

> **Find a better estimator for $\theta$** than $\hat{\theta}_1$, and **prove** that it is better.

---

## ‚úÖ Detailed Solution

---

### üîπ Step 1: Known Facts

* The exponential distribution has:

  $$
  \mathbb{E}[X_i] = \theta, \quad \text{Var}(X_i) = \theta^2
  $$

* The **sample mean** is:

  $$
  \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
  $$

* Since $X_i \sim \text{Exp}(\theta)$, the sum $T = \sum X_i \sim \text{Gamma}(n, \theta)$.

Thus:

* $\hat{\theta}_2 := \bar{X} = \frac{1}{n} \sum X_i$ is:

  * **Unbiased**, since $\mathbb{E}[\bar{X}] = \theta$
  * **MLE**, because the exponential family is regular
  * **Minimum variance unbiased estimator (MVUE)**, because it's based on a **complete sufficient statistic**

---

### üîπ Step 2: Variance Comparison

#### Estimator 1 (Based on Minimum):

$$
\hat{\theta}_1 = n X_{(1)}, \quad \text{where } X_{(1)} = \min(X_i)
$$

We know:

$$
X_{(1)} \sim \text{Exp}\left(\frac{n}{\theta}\right)
\Rightarrow \mathbb{E}[X_{(1)}] = \frac{\theta}{n},\quad \text{Var}(X_{(1)}) = \frac{\theta^2}{n^2}
$$

So:

$$
\text{Var}(\hat{\theta}_1) = \text{Var}(n X_{(1)}) = n^2 \cdot \frac{\theta^2}{n^2} = \theta^2
$$

#### Estimator 2 (Sample Mean):

$$
\hat{\theta}_2 = \bar{X},\quad \text{Var}(\bar{X}) = \frac{\theta^2}{n}
$$

---

### üîé Comparison

* $\text{Var}(\hat{\theta}_1) = \theta^2$
* $\text{Var}(\hat{\theta}_2) = \frac{\theta^2}{n}$

Since $n > 1$, we have:

$$
\text{Var}(\hat{\theta}_2) < \text{Var}(\hat{\theta}_1)
\Rightarrow \hat{\theta}_2 \text{ is strictly better in variance}
$$

Both estimators are **unbiased**, so **lower variance ‚áí lower MSE**.

---

### ‚úÖ Final Answer

$$
\boxed{
\text{The sample mean } \hat{\theta}_2 = \bar{X} \text{ is a better estimator than } \hat{\theta}_1 = n X_{(1)}.
}
$$

‚úî It is **unbiased**,
‚úî Has **lower variance**,
‚úî And is the **MLE and MVUE**.

---

## üìå Related Concepts

### 1. **Sufficient and Complete Statistic**

* $T = \sum X_i$ is sufficient and complete for $\theta$ in exponential families.
* Any unbiased function of $T$ (like $\bar{X}$) is the **unique MVUE** by the **Lehmann‚ÄìScheff√© theorem**.

### 2. **Cram√©r‚ÄìRao Lower Bound (CRLB)**

For exponential:

$$
I(\theta) = \frac{n}{\theta^2}, \quad \Rightarrow \text{CRLB} = \frac{1}{I(\theta)} = \frac{\theta^2}{n}
$$

Since:

$$
\text{Var}(\bar{X}) = \frac{\theta^2}{n} = \text{CRLB}
$$

‚Üí So $\bar{X}$ is **efficient**.

---

## üß† Visual Intuition

> Imagine observing only the first failure in a system of $n$ light bulbs ‚Äî this is $\hat{\theta}_1$.
> Now imagine observing **all failure times** and averaging them ‚Äî this is $\hat{\theta}_2$.

Clearly, the second gives more information ‚Äî so naturally, it leads to a better estimator.

---

## üí° Worth Mentioning

* $\hat{\theta}_1 = n X_{(1)}$ is **not sufficient**, so by the **Rao-Blackwell theorem**, we can improve it by conditioning on the sufficient statistic $\sum X_i$\*\* ‚Äî and the result is $\bar{X}$.
* This is an example of **using partial information vs full information**.
* In data-limited scenarios (like censored data), $X_{(1)}$ may be all you have ‚Äî so even if suboptimal, it‚Äôs **practically useful**.

---

Would you like me to show the **Rao-Blackwell improvement** of $\hat{\theta}_1$ to derive $\hat{\theta}_2$ as the MVUE?

    </div>

























  <div class="glow-box" id="5">
      <h4>Question:</h4>
<p>
Let \( X_1, X_2, \ldots, X_n \) be i.i.d. with distribution \( \mathcal{N}(\theta, \theta^2) \), where \( \theta > 0 \).
</p>
<p>Define:</p>
<ul>
  <li>\( \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i \) (sample mean)</li>
  <li>\( S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2 \) (sample variance)</li>
</ul>
<blockquote>
For which value of \( c \) is \( cS \) an <strong>unbiased estimator of \( \theta \)</strong>?
</blockquote>

<h4>Solution:</h4>

<p>We are to find \( c \) such that:</p>
<p>\[
\mathbb{E}[cS] = \theta.
\]</p>

<p>So, we need:</p>
<p>\[
c = \frac{\theta}{\mathbb{E}[S]}
\]</p>

<p>We know:</p>
<ul>
  <li>\( X_i \sim \mathcal{N}(\theta, \theta^2) \)</li>
  <li>\( \text{Var}(X_i) = \theta^2 \)</li>
</ul>

<p>The sample variance:</p>
<p>\[
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2
\]</p>
<p>is an <strong>unbiased estimator of \( \theta^2 \)</strong>:</p>
<p>\[
\Rightarrow \mathbb{E}[S^2] = \theta^2
\]</p>

<p>But:</p>
<p>\[
S = \sqrt{S^2} \Rightarrow \mathbb{E}[S] \ne \sqrt{\mathbb{E}[S^2]} \Rightarrow \mathbb{E}[S] < \theta
\]</p>

<p>So we must <strong>explicitly calculate</strong> \( \mathbb{E}[S] \) when \( X_i \sim \mathcal{N}(\theta, \theta^2) \).</p>

<h4>üîπ Step 1: Use Chi-Square Distribution</h4>

<p>We know that for \( X_i \sim \mathcal{N}(\mu, \sigma^2) \), the statistic:</p>
<p>\[
\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}
\]</p>

<p>This holds <em>even if \( \mu \ne 0 \)</em>.</p>

<p>Here, \( \sigma^2 = \theta^2 \), so:</p>
<p>\[
\frac{(n-1)S^2}{\theta^2} \sim \chi^2_{n-1}
\Rightarrow S = \theta \cdot \sqrt{\frac{1}{n-1} \cdot \chi^2_{n-1}}
\]</p>

<p>Let:</p>
<p>\[
Z = \chi^2_{n-1}, \quad \Rightarrow S = \theta \cdot \sqrt{\frac{Z}{n-1}}
\]</p>

<p>So:</p>
<p>\[
\mathbb{E}[S] = \theta \cdot \mathbb{E}\left[\sqrt{\frac{Z}{n-1}}\right]
= \theta \cdot \frac{1}{\sqrt{n-1}} \cdot \mathbb{E}\left[\sqrt{Z}\right]
\]</p>

<p>We now need the expected value of \( \sqrt{Z} \), where \( Z \sim \chi^2_{n-1} \).</p>

<h4>üîπ Step 2: Known Result: Expected value of square root of chi-square</h4>

<p>For \( Z \sim \chi^2_k \), there‚Äôs a known result:</p>
<p>\[
\mathbb{E}[\sqrt{Z}] = \sqrt{2} \cdot \frac{\Gamma\left(\frac{k+1}{2}\right)}{\Gamma\left(\frac{k}{2}\right)}
\]</p>

<p>Apply this to \( Z \sim \chi^2_{n-1} \):</p>
<p>\[
\mathbb{E}[S] = \theta \cdot \frac{1}{\sqrt{n-1}} \cdot \mathbb{E}[\sqrt{Z}]
= \theta \cdot \frac{\sqrt{2}}{\sqrt{n-1}} \cdot \frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)}
\]</p>

<h4>üîπ Step 3: Solve for \( c \)</h4>

<p>We want:</p>
<p>\[
\mathbb{E}[cS] = \theta \Rightarrow c \cdot \mathbb{E}[S] = \theta
\Rightarrow c = \frac{\theta}{\mathbb{E}[S]}
\]</p>

<p>Substitute:</p>
<p>\[
\mathbb{E}[S] = \theta \cdot \frac{\sqrt{2}}{\sqrt{n-1}} \cdot \frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)}
\]</p>

<p>Cancel \( \theta \), we get:</p>
<p>\[
c = \boxed{
\frac{\sqrt{n-1}}{\sqrt{2}} \cdot \frac{\Gamma\left(\frac{n-1}{2}\right)}{\Gamma\left(\frac{n}{2}\right)}
}
\]</p>

<h4>Final Answer:</h4>
<p>\[
\boxed{
c = \frac{\sqrt{n - 1}}{\sqrt{2}} \cdot \frac{\Gamma\left( \frac{n - 1}{2} \right)}{\Gamma\left( \frac{n}{2} \right)}
}
\quad \text{makes } cS \text{ an unbiased estimator of } \theta.
\]</p>

<h4>Related concepts / Other stuff:</h4>

<ul>
  <li><strong>Chi-square and Gamma functions:</strong>
    <ul>
      <li>If \( Z \sim \chi^2_k \), then it‚Äôs a special case of \( \text{Gamma}(k/2, 2) \)</li>
      <li>\( \mathbb{E}[\sqrt{Z}] \) involves non-linear expectations ‚Üí handled with gamma functions.</li>
    </ul>
  </li>
  <li><strong>Bias of the Sample Standard Deviation:</strong>
    <ul>
      <li>\( \mathbb{E}[S] < \sqrt{\text{Var}(X)} \)</li>
      <li>This is because square root is concave ‚Üí Jensen‚Äôs inequality implies bias downward.</li>
    </ul>
  </li>
</ul>

<h4>üß† Visual Intuition:</h4>
<ul>
  <li>The sample variance \( S^2 \) is <strong>unbiased</strong> for \( \theta^2 \)</li>
  <li>But taking the square root <strong>pulls the expectation down</strong>, because of concavity.</li>
  <li>To compensate, you scale up by \( c > 1 \), computed exactly via gamma ratios.</li>
</ul>

<h4>üí° Worth Mentioning:</h4>
<ul>
  <li>This is a <strong>rare exact correction</strong> for bias ‚Äî many times people use approximate corrections (e.g., in small-sample t-tests).</li>
  <li>For large \( n \), \( c \to 1 \), so \( S \approx \theta \), and bias becomes negligible.</li>
  <li>This \( cS \) is <strong>not MVUE</strong> ‚Äî \( \bar{X} \) is a better candidate due to sufficiency and completeness, but it doesn‚Äôt depend solely on \( S \).</li>
</ul>

    </div>
























  <div class="glow-box" id="6">
      <section class="border border-gray-300 p-6 rounded-2xl shadow-md mb-8">
  <h2 class="text-2xl font-semibold mb-4 text-black">üî∑ QUESTION</h2>
  <p class="mb-4">
    Suppose, in the above problem, the values of <span class="math">c</span> is chosen as the value that makes <span class="math">cS</span> an unbiased estimator of <span class="math">\(\theta\)</span>. Then
  </p>
  <div class="text-center mb-4">
    <span class="math">\[
    \lambda \bar{X} + (1 - \lambda)cS
    \]</span>
  </div>
  <p class="mb-4">
    is also an unbiased estimator of <span class="math">\(\theta\)</span> for every value of <span class="math">\(\lambda \in [0, 1]\)</span>.
    <br>
    <strong>For which value of <span class="math">\(\lambda\)</span> in this interval does the unbiased estimator have the minimum variance?</strong>
  </p>

  <h2 class="text-2xl font-semibold mt-8 mb-4 text-black">üìò DETAILED SOLUTION</h2>
  <p class="mb-4">
    We are given:
  </p>
  <ul class="list-disc list-inside mb-4">
    <li><span class="math">\(X_1, \dots, X_n \overset{iid}{\sim} \mathcal{N}(\theta, \theta^2)\)</span> where <span class="math">\(\theta > 0\)</span></li>
    <li><span class="math">\(\bar{X} = \frac{1}{n} \sum X_i\)</span></li>
    <li><span class="math">\(S^2 = \frac{1}{n-1} \sum (X_i - \bar{X})^2\)</span></li>
    <li><span class="math">\(S = \sqrt{S^2}\)</span></li>
    <li>Let <span class="math">c</span> be such that <span class="math">cS</span> is unbiased for <span class="math">\(\theta\)</span></li>
    <li>Consider <span class="math">\(\delta_\lambda = \lambda \bar{X} + (1 - \lambda)cS\)</span></li>
  </ul>

  <h3 class="text-xl font-semibold mb-2 text-black">‚úÖ Step 1: Check Unbiasedness</h3>
  <p class="mb-4">
    Both <span class="math">\(\bar{X}\)</span> and <span class="math">\(cS\)</span> are unbiased estimators of <span class="math">\(\theta\)</span>. So:
  </p>
  <div class="text-center mb-4">
    <span class="math">\[
    \mathbb{E}[\delta_\lambda] = \lambda \theta + (1 - \lambda)\theta = \theta
    \]</span>
  </div>

  <h3 class="text-xl font-semibold mb-2 text-black">‚úÖ Step 2: Compute Variance</h3>
  <p class="mb-4">
    Since <span class="math">\(\bar{X}\)</span> and <span class="math">\(S\)</span> are independent:
  </p>
  <div class="text-center mb-4">
    <span class="math">\[
    \operatorname{Var}(\delta_\lambda) = \lambda^2 \operatorname{Var}(\bar{X}) + (1 - \lambda)^2 \operatorname{Var}(cS)
    \]</span>
  </div>
  <p class="mb-4">
    We know:
  </p>
  <ul class="list-disc list-inside mb-4">
    <li><span class="math">\(\operatorname{Var}(\bar{X}) = \frac{\theta^2}{n}\)</span></li>
    <li><span class="math">\(\mathbb{E}[S] = b_n \theta \Rightarrow c = \frac{1}{b_n}\)</span></li>
    <li><span class="math">\(\operatorname{Var}(S) \approx \frac{\theta^2}{2(n-1)}\)</span></li>
  </ul>
  <p class="mb-4">
    So:
  </p>
  <div class="text-center mb-4">
    <span class="math">\[
    \operatorname{Var}(cS) = \frac{1}{b_n^2} \cdot \frac{\theta^2}{2(n-1)}
    \]</span>
  </div>
  <p class="mb-4">
    Putting all together:
  </p>
  <div class="text-center mb-4">
    <span class="math">\[
    \operatorname{Var}(\delta_\lambda) = \theta^2 \left[\lambda^2 \cdot \frac{1}{n} + (1 - \lambda)^2 \cdot \frac{1}{b_n^2 \cdot 2(n - 1)}\right]
    \]</span>
  </div>

  <h3 class="text-xl font-semibold mb-2 text-black">‚úÖ Step 3: Minimize Variance w.r.t. \(\lambda\)</h3>
  <p class="mb-4">
    Let:
  </p>
  <div class="text-center mb-4">
    <span class="math">\[
    V(\lambda) = A\lambda^2 + B(1 - \lambda)^2
    \]</span>
  </div>
  <p class="mb-4">
    where:
    <span class="math">\(A = \frac{1}{n}, \quad B = \frac{1}{b_n^2 \cdot 2(n - 1)}\)</span>
  </p>
  <p class="mb-4">
    Then:
  </p>
  <div class="text-center mb-4">
    <span class="math">\[
    V(\lambda) = (A + B)\lambda^2 - 2B\lambda + B
    \]</span>
  </div>
  <p class="mb-4">
    Minimize by setting derivative to 0:
  </p>
  <div class="text-center mb-4">
    <span class="math">\[
    \lambda^* = \frac{B}{A + B}
    \]</span>
  </div>

  <h2 class="text-2xl font-semibold mt-8 mb-4 text-black">‚úÖ Final Answer</h2>
  <div class="text-center mb-4 text-xl font-bold">
    <span class="math">\[
    \boxed{\lambda^* = \frac{\displaystyle \frac{1}{b_n^2 \cdot 2(n - 1)}}{\displaystyle \frac{1}{n} + \frac{1}{b_n^2 \cdot 2(n - 1)}}}
    \]</span>
  </div>

  <h2 class="text-2xl font-semibold mt-8 mb-4 text-black">üìå Related Concepts</h2>
  <ul class="list-disc list-inside mb-4">
    <li>Linear combinations of unbiased estimators stay unbiased.</li>
    <li>To minimize variance among unbiased estimators, exploit independence and variance structure.</li>
    <li>Convex combinations are common when fusing noisy signals.</li>
    <li>Chi-square variance and Gamma correction methods appear in finite-sample theory.</li>
  </ul>

  <h2 class="text-2xl font-semibold mt-8 mb-4 text-black">üß† Visual Intuition</h2>
  <p class="mb-4">
    Picture a slider between <span class="math">\(\bar{X}\)</span> and <span class="math">\(cS\)</span>. Moving it shifts weight from one to another. The optimal point balances their variance contributions.
  </p>

  <h2 class="text-2xl font-semibold mt-8 mb-4 text-black">üí° Worth Mentioning</h2>
  <ul class="list-disc list-inside mb-4">
    <li>As <span class="math">n \to \infty</span>, <span class="math">\(\lambda^* \to 1\)</span>.</li>
    <li>This derivation is symbolic ‚Äî no need for data values.</li>
    <li>You can evaluate <span class="math">b_n</span> numerically for practical usage.</li>
  </ul>
</section>

    </div>






















    <div class="glow-box" id="12">
        
    </div>

































  <div class="glow-box" id="12">
      
    </div>






































  <div class="glow-box" id="12">
      
    </div>































  <div class="glow-box" id="12">
      
    </div>

































  <div class="glow-box" id="12">
      
    </div>






























  <div class="glow-box" id="12">
      
    </div>
































  
</section>

<button class="next-btn" onclick="location.href='wk2.html'">Next Section ‚Üí</button>

<footer>
  <p>&copy; 2025 Shaan | Built with purpose</p>
</footer>

</body>
</html>
